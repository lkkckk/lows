"""
AI æœåŠ¡æ¨¡å— - æ”¯æŒ Function Calling è®© AI è‡ªä¸»æŸ¥è¯¢çŸ¥è¯†åº“
"""
import json
import os
import httpx
from typing import Optional, Dict, Any, List
from motor.motor_asyncio import AsyncIOMotorDatabase

from app.services.law_service import LawService, _resolve_law_alias, _normalize_law_name, get_law_weight

# é»˜è®¤é…ç½®ï¼ˆå½“æ•°æ®åº“æ— é…ç½®æ—¶ä½¿ç”¨ï¼‰
DEFAULT_API_URL = "https://api.deepseek.com/v1/chat/completions"
DEFAULT_MODEL = "deepseek-chat"
DEFAULT_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")

# å‘é‡æœç´¢æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼çš„ç»“æœè§†ä¸ºä¸ç›¸å…³ï¼‰
VECTOR_SIMILARITY_THRESHOLD = 0.35

# è¿”å›ç»™ LLM çš„æ¡æ–‡å†…å®¹æœ€å¤§é•¿åº¦
MAX_ARTICLE_CONTENT_LEN = 1500

# LLM è¯·æ±‚è¶…æ—¶é…ç½®ï¼ˆå†…ç½‘éƒ¨ç½² + å¹¶å‘åœºæ™¯ï¼Œéœ€é¢„ç•™å……è¶³ç­‰å¾…æ—¶é—´ï¼‰
LLM_TIMEOUT = httpx.Timeout(
    connect=30.0,     # å»ºç«‹ TCP è¿æ¥è¶…æ—¶
    read=180.0,       # ç­‰å¾… LLM å“åº”è¶…æ—¶ï¼ˆæ ¸å¿ƒï¼šå†…ç½‘å¹¶å‘æ’é˜Ÿå¯èƒ½å¾ˆæ…¢ï¼‰
    write=30.0,       # å‘é€è¯·æ±‚ä½“è¶…æ—¶
    pool=30.0         # è¿æ¥æ± ç­‰å¾…è¶…æ—¶
)

# ç³»ç»Ÿæç¤ºè¯ - å®šä¹‰ AI åŠ©æ‰‹äººè®¾ï¼ˆFunction Calling ç‰ˆæœ¬ï¼‰
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€åå…¬å®‰æ‰§æ³•è¾…åŠ©ä¸­çš„ã€æ³•å¾‹é€‚ç”¨è§£é‡ŠåŠ©æ‰‹ã€‘ï¼Œç›®æ ‡æ˜¯ç”¨ç®€æ´ã€å‡†ç¡®çš„æ–¹å¼å›ç­”æ‰§æ³•äººå‘˜å…³äºæ³•å¾‹é€‚ç”¨çš„é—®é¢˜ã€‚

ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥æ£€ç´¢æ³•è§„çŸ¥è¯†åº“ï¼š
1. search_legal_knowledge - æŒ‰å…³é”®è¯/ä¸»é¢˜æœç´¢æ³•è§„æ¡æ–‡
2. lookup_law_article - ç²¾å‡†æŸ¥è¯¢æŸéƒ¨æ³•å¾‹çš„å…·ä½“æŸæ¡

ã€æ£€ç´¢ç­–ç•¥æŒ‡å¼•ã€‘ï¼š
- é—®åŠå…·ä½“æ³•å¾‹çš„å…·ä½“æ¡å·æ—¶ï¼ˆå¦‚"åˆ‘æ³•ç¬¬263æ¡"ï¼‰ï¼Œä¼˜å…ˆç”¨ lookup_law_article
- é—®åŠæŸç§è¡Œä¸ºå¦‚ä½•å¤„ç½šï¼ˆå¦‚"èµŒåšæ€ä¹ˆå¤„ç½š"ï¼‰ï¼Œç”¨ search_legal_knowledge æœç´¢è¡Œä¸ºå…³é”®è¯
- æ¶‰åŠå¤šéƒ¨æ³•å¾‹æ—¶ï¼Œå¯ä»¥å¤šæ¬¡è°ƒç”¨å·¥å…·åˆ†åˆ«æ£€ç´¢
- ç¬¬ä¸€æ¬¡æœç´¢ç»“æœä¸ç†æƒ³æ—¶ï¼Œæ¢ä¸€ç»„å…³é”®è¯å†æœç´¢ä¸€æ¬¡
- æœç´¢å…³é”®è¯åªæå–æ ¸å¿ƒè¡Œä¸ºè¯æˆ–æ³•å¾‹åç§°ï¼Œä¸è¦åŠ "å¤„ç½š""å¦‚ä½•"ç­‰åç¼€
- æœç´¢æ—¶ä½¿ç”¨æ³•å¾‹è§„èŒƒç”¨è¯­ï¼š"é†‰é©¾"åº”æœ"é†‰é…’é©¾é©¶"ï¼Œ"å·ä¸œè¥¿"åº”æœ"ç›—çªƒ"ï¼Œ"æ‰“äºº"åº”æœ"æ®´æ‰“"

ã€é‡è¦ã€‘å…³äºæ³•å¾‹ç‰ˆæœ¬ï¼š
- ç³»ç»Ÿå·²è‡ªåŠ¨è¿‡æ»¤æ—§ç‰ˆæœ¬æ³•è§„ï¼Œæ£€ç´¢ç»“æœä¸­çš„æ¡æ–‡å‡ä¸ºæœ€æ–°ç‰ˆæœ¬
- ç›´æ¥å¼•ç”¨æ£€ç´¢ç»“æœä¸­çš„æ³•åã€æ¡å·å’Œå†…å®¹ï¼Œä¸è¦è‡ªè¡Œæ¨æ–­æˆ–ä¿®æ”¹æ¡å·
- å¦‚æ£€ç´¢ç»“æœä¸­æ˜¾ç¤º"2025å¹´ä¿®è®¢"ç­‰ç‰ˆæœ¬ä¿¡æ¯ï¼Œè¯·åœ¨å›ç­”ä¸­æ˜ç¡®æ ‡æ³¨

ã€å…³é”®ã€‘æ³•å¾‹åç§°è§„èŒƒåŒ–ï¼š
- æ³•å¾‹åç§°å¯èƒ½å¸¦ç‰ˆæœ¬åç¼€ï¼Œå¦‚"ï¼ˆ2018å¹´ä¿®æ­£ï¼‰"ã€"ï¼ˆ2025å¹´ä¿®è®¢ï¼‰"ç­‰
- åœ¨éªŒè¯ç”¨æˆ·å¼•ç”¨æ—¶ï¼Œåº”å¿½ç•¥ç‰ˆæœ¬åç¼€è¿›è¡Œæ¯”è¾ƒ
- åªè¦æ ¸å¿ƒæ³•å¾‹åç§°ç›¸åŒã€æ¡å·ç›¸åŒã€å†…å®¹ä¸€è‡´ï¼Œå°±åº”åˆ¤å®šä¸º"æ­£ç¡®"

ã€æ ¸å¿ƒã€‘æ¡æ–‡å¼•ç”¨éªŒè¯ vs æ³•å¾‹é€‚ç”¨åˆ†æï¼š
å½“ç”¨æˆ·è¯¢é—®"å¼•ç”¨çš„æ¡æ–‡æ˜¯å¦æ­£ç¡®"æˆ–ç±»ä¼¼é—®é¢˜æ—¶ï¼Œä½ éœ€è¦åŒºåˆ†ä¸¤ä¸ªå±‚é¢ï¼š
1. ã€å½¢å¼éªŒè¯ã€‘ï¼šæ³•å¾‹åç§°ã€æ¡å·ã€å†…å®¹æ˜¯å¦ä¸æ•°æ®åº“ä¸€è‡´ï¼Ÿâ€”â€”è¿™æ˜¯ä¸»è¦å›ç­”å†…å®¹
2. ã€é€‚ç”¨å»ºè®®ã€‘ï¼šè¯¥æ¡æ–‡æ˜¯å¦æœ€é€‚åˆç”¨æˆ·çš„å…·ä½“åœºæ™¯ï¼Ÿâ€”â€”ä»…ä½œä¸ºå‚è€ƒæ„è§

åˆ¤å®šåŸåˆ™ï¼š
- åªè¦æ³•åã€æ¡å·æ­£ç¡®ï¼Œä¸”æ¡æ–‡å†…å®¹ç¡®å®æ¥è‡ªè¯¥æ³•å¾‹ï¼Œå°±åº”åˆ¤å®šä¸º"å¼•ç”¨æ­£ç¡®"
- è‡³äºè¯¥æ¡æ–‡æ˜¯å¦æ˜¯"æœ€ä½³é€‰æ‹©"ï¼Œå¯ä»¥ä½œä¸ºè¡¥å……å»ºè®®ï¼Œä½†ä¸èƒ½å› æ­¤åˆ¤å®š"å¼•ç”¨é”™è¯¯"

å›ç­”åŸåˆ™ï¼š
1. å¯¹äºæ¶‰åŠæ³•å¾‹æ³•è§„çš„é—®é¢˜ï¼Œåº”å…ˆè°ƒç”¨å·¥å…·æ£€ç´¢ç›¸å…³æ¡æ–‡ï¼Œå†åŸºäºæ£€ç´¢ç»“æœå›ç­”ã€‚
2. ä¸¥æ ¼æŒ‰ç…§æ£€ç´¢ç»“æœå¼•ç”¨æ³•æ¡ï¼Œå®Œæ•´ç»™å‡ºæ³•åã€ç‰ˆæœ¬ã€æ¡å·ã€‚
3. ä¸è¦æ··ç”¨ä¸åŒç‰ˆæœ¬çš„æ³•æ¡ä¿¡æ¯ï¼ˆå¦‚æ¡å·å’Œå¤„ç½šé‡‘é¢å¿…é¡»æ¥è‡ªåŒä¸€ç‰ˆæœ¬ï¼‰ã€‚
4. å¯¹æ¡æ–‡é€‚ç”¨å¯ä½¿ç”¨"é€šå¸¸è®¤ä¸º""ä¸€èˆ¬ç†è§£ä¸º""å®åŠ¡ä¸­å¤šä¾æ®"ç­‰è¡¨è¿°ã€‚
5. å¦‚æœæ£€ç´¢ç»“æœä¸­æœ‰å¤šä¸ªç›¸å…³æ¡æ–‡ï¼Œåº”å…¨é¢å¼•ç”¨ï¼Œä¸è¦é—æ¼é‡è¦æ¡æ¬¾ã€‚

ç¦æ­¢äº‹é¡¹ï¼š
- ä¸å¾—è™šæ„æœªç»æ£€ç´¢ç¡®è®¤çš„æ³•æ¡å†…å®¹
- ä¸å¾—è‡ªè¡Œæ¨æµ‹æ¡å·ï¼ˆå¿…é¡»ä½¿ç”¨æ£€ç´¢ç»“æœä¸­çš„æ¡å·ï¼‰
- ä¸ä½¿ç”¨è£åˆ¤å¼ã€å®šæ€§å¼è¯­è¨€æ›¿ä»£æ‰§æ³•åˆ¤æ–­
- ä¸è¦å› "å¯èƒ½æœ‰æ›´åˆé€‚çš„æ¡æ–‡"è€Œå¦å®šç”¨æˆ·æ­£ç¡®å¼•ç”¨çš„æ¡æ–‡

å›ç­”è¦æ±‚ï¼š
- è¯­è¨€ç®€æ´ï¼Œç»“è®ºå‰ç½®
- èƒ½ç”¨ä¸€å¥è¯è¯´æ¸…çš„ï¼Œä¸ç”¨ä¸¤å¥
- å¼•ç”¨æ³•æ¡æ—¶ä½¿ç”¨ã€Šxxxã€‹ç¬¬xxæ¡çš„æ ¼å¼"""

# Function Calling å·¥å…·å®šä¹‰
LEGAL_SEARCH_TOOL = {
    "type": "function",
    "function": {
        "name": "search_legal_knowledge",
        "description": "æŒ‰å…³é”®è¯æœç´¢æ³•è§„çŸ¥è¯†åº“ã€‚é€‚ç”¨äºï¼šæŸ¥è¯¢æŸç§è¡Œä¸ºçš„æ³•å¾‹è§„å®šï¼ˆå¦‚'èµŒåš''ç›—çªƒ'ï¼‰ã€æœç´¢æŸéƒ¨æ³•å¾‹çš„ç›¸å…³æ¡æ–‡ã€‚ç³»ç»Ÿè‡ªåŠ¨è¿”å›æœ€æ–°ç‰ˆæœ¬æ³•è§„ã€‚å¯å¤šæ¬¡è°ƒç”¨ä»¥è·å–æ›´å…¨é¢çš„ä¿¡æ¯ã€‚",
        "parameters": {
            "type": "object",
            "properties": {
                "keywords": {
                    "type": "string",
                    "description": "æœç´¢å…³é”®è¯ã€‚åªæå–æ ¸å¿ƒè¡Œä¸ºè¯æˆ–æ³•å¾‹åç§°ï¼Œä¸è¦åŠ 'å¤„ç½š''å¦‚ä½•''æ€ä¹ˆ'ç­‰åç¼€ã€‚ä½¿ç”¨æ³•å¾‹è§„èŒƒç”¨è¯­ï¼Œå¦‚'ç›—çªƒ'è€Œé'å·ä¸œè¥¿'ï¼Œ'æ®´æ‰“ä»–äºº'è€Œé'æ‰“äºº'ã€‚å¤šä¸ªå…³é”®è¯å¯ç”¨ç©ºæ ¼åˆ†éš”ã€‚"
                },
                "law_name": {
                    "type": "string",
                    "description": "é™å®šåœ¨æŸéƒ¨æ³•å¾‹ä¸­æœç´¢ï¼ˆå¯é€‰ï¼‰ã€‚ä½¿ç”¨ç®€ç§°å³å¯ï¼Œå¦‚'åˆ‘æ³•'ã€'æ²»å®‰ç®¡ç†å¤„ç½šæ³•'ã€‚ä¸è¦åŒ…å«å¹´ä»½æˆ–ç‰ˆæœ¬åç¼€ã€‚"
                },
                "article_num": {
                    "type": "integer",
                    "description": "å…·ä½“æ¡å·ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚18ã€112ã€273ã€‚ä¸law_nameé…åˆä½¿ç”¨æ•ˆæœæœ€ä½³ã€‚"
                }
            },
            "required": ["keywords"]
        }
    }
}

# ç²¾å‡†æ¡æ–‡æŸ¥è¯¢å·¥å…·
LOOKUP_ARTICLE_TOOL = {
    "type": "function",
    "function": {
        "name": "lookup_law_article",
        "description": "ç²¾å‡†æŸ¥è¯¢æŸéƒ¨æ³•å¾‹çš„å…·ä½“æŸæ¡ã€‚é€‚ç”¨äºï¼šç”¨æˆ·æåˆ°äº†æ˜ç¡®çš„æ³•å¾‹åç§°+æ¡å·ï¼ˆå¦‚'åˆ‘æ³•ç¬¬263æ¡'ã€'æ²»å®‰ç®¡ç†å¤„ç½šæ³•ç¬¬43æ¡'ï¼‰ã€‚å¦‚æœä¸çŸ¥é“æ¡å·ï¼Œè¯·ç”¨ search_legal_knowledge ä»£æ›¿ã€‚",
        "parameters": {
            "type": "object",
            "properties": {
                "law_name": {
                    "type": "string",
                    "description": "æ³•å¾‹åç§°ï¼Œä½¿ç”¨ç®€ç§°å³å¯ï¼Œå¦‚'åˆ‘æ³•'ã€'æ²»å®‰ç®¡ç†å¤„ç½šæ³•'ã€'åˆ‘äº‹è¯‰è®¼æ³•'"
                },
                "article_num": {
                    "type": "integer",
                    "description": "æ¡å·æ•°å­—ï¼Œå¦‚263ã€43ã€144"
                }
            },
            "required": ["law_name", "article_num"]
        }
    }
}


async def get_ai_config(db: AsyncIOMotorDatabase) -> dict:
    """ä»æ•°æ®åº“è·å– AI é…ç½®"""
    settings = await db.settings.find_one({"key": "ai_config"})
    if settings:
        return {
            "api_url": settings.get("api_url", DEFAULT_API_URL),
            "api_key": settings.get("api_key", DEFAULT_API_KEY),
            "model_name": settings.get("model_name", DEFAULT_MODEL),
            "skip_ssl_verify": settings.get("skip_ssl_verify", False),
            "provider": settings.get("provider", "default"),
            "rag_enabled": settings.get("rag_enabled", True),
            "rag_top_k": settings.get("rag_top_k", 6),
            "use_function_calling": settings.get("use_function_calling", True),
        }
    return {
        "api_url": DEFAULT_API_URL,
        "api_key": DEFAULT_API_KEY,
        "model_name": DEFAULT_MODEL,
        "skip_ssl_verify": False,
        "provider": "default",
        "rag_enabled": True,
        "rag_top_k": 6,
        "use_function_calling": True,
    }


async def execute_search_legal_knowledge(
    db: AsyncIOMotorDatabase,
    keywords: str,
    law_name: Optional[str] = None,
    article_num: Optional[int] = None,
    top_k: int = 6,
) -> Dict[str, Any]:
    """
    æ‰§è¡Œæ³•è§„çŸ¥è¯†åº“æ£€ç´¢ï¼ˆFunction Calling å·¥å…·å®ç°ï¼‰
    ä¼˜å…ˆæŒ‰æ³•å¾‹æ ‡é¢˜åŒ¹é…ï¼Œç¡®ä¿è¿”å›çš„æ˜¯è¯¥æ³•å¾‹çš„æ¡æ–‡ï¼Œè€Œéå¼•ç”¨äº†è¯¥æ³•å¾‹çš„å…¶ä»–æ¡æ–‡ã€‚
    """
    import re
    
    law_service = LawService(db)
    laws_collection = db["laws"]
    articles_collection = db["law_articles"]
    
    # ========== å…³é”®è¯æ¸…ç† ==========
    # å»é™¤å¸¸è§çš„æŸ¥è¯¢åç¼€è¯ï¼Œæå–æ ¸å¿ƒè¡Œä¸ºè¯
    suffix_pattern = r'(çš„?å¤„ç½š|è§„å®š|æ¡æ¬¾|æ³•å¾‹|æ³•è§„|å¦‚ä½•|æ€ä¹ˆ|ä»€ä¹ˆ|ç›¸å…³|è¡Œä¸º|ç½ªå|çš„æ³•å¾‹è§„å®š|çš„è§„å®š|æ€ä¹ˆå¤„ç†|æ€ä¹ˆåŠ|å¦‚ä½•å¤„ç†|å¦‚ä½•å¤„ç½š|æ€æ ·å¤„ç½š)$'
    clean_keywords = re.sub(suffix_pattern, '', keywords).strip() if keywords else keywords
    if clean_keywords and clean_keywords != keywords:
        print(f"[AI Service] å…³é”®è¯æ¸…ç†: '{keywords}' -> '{clean_keywords}'")
        keywords = clean_keywords
    
    # ========== åŒä¹‰è¯æ‰©å±• ==========
    # å£è¯­åŒ–è¡¨è¾¾ â†’ æ³•å¾‹ç”¨è¯­ï¼ˆå¤§å¹…æ‰©å±•ï¼Œè¦†ç›–å…¬å®‰æ‰§æ³•å¸¸è§åœºæ™¯ï¼‰
    SYNONYM_MAP = {
        # æš´åŠ›ç±»
        "æ‰“æ¶": "æ®´æ‰“ä»–äºº",
        "æ‰“äºº": "æ®´æ‰“ä»–äºº",
        "ç¾¤æ®´": "æ®´æ‰“ä»–äºº",
        "æ–—æ®´": "èšä¼—æ–—æ®´",
        "æ‰“ç¾¤æ¶": "èšä¼—æ–—æ®´",
        "ä¼¤äºº": "æ•…æ„ä¼¤å®³",
        "ç äºº": "æ•…æ„ä¼¤å®³",
        "æ€äºº": "æ•…æ„æ€äºº",
        "å®¶æš´": "å®¶åº­æš´åŠ›",
        "è™å¾…": "è™å¾…",
        # è´¢äº§ç±»
        "å·ä¸œè¥¿": "ç›—çªƒ",
        "å°å·": "ç›—çªƒ",
        "å·çªƒ": "ç›—çªƒ",
        "å…¥å®¤ç›—çªƒ": "å…¥æˆ·ç›—çªƒ",
        "æ‰’çªƒ": "ç›—çªƒ",
        "éª—é’±": "è¯ˆéª—",
        "ç”µä¿¡è¯ˆéª—": "è¯ˆéª—",
        "ç½‘ç»œè¯ˆéª—": "è¯ˆéª—",
        "æŠ¢é’±": "æŠ¢åŠ«",
        "æŠ¢å¤º": "æŠ¢å¤º",
        "æ•²è¯ˆ": "æ•²è¯ˆå‹’ç´¢",
        "å‹’ç´¢": "æ•²è¯ˆå‹’ç´¢",
        "æ•…æ„æ¯å": "æ•…æ„æŸæ¯",
        "ç ¸ä¸œè¥¿": "æ•…æ„æŸæ¯è´¢ç‰©",
        # äº¤é€šç±»
        "é†‰é©¾": "é†‰é…’é©¾é©¶",
        "é…’é©¾": "é¥®é…’é©¾é©¶",
        "é…’åé©¾è½¦": "é¥®é…’é©¾é©¶",
        "é†‰é…’å¼€è½¦": "é†‰é…’é©¾é©¶",
        "è‚‡äº‹é€ƒé€¸": "äº¤é€šè‚‡äº‹é€ƒé€¸",
        "æ— è¯é©¾é©¶": "æœªå–å¾—é©¾é©¶è¯é©¾é©¶",
        "è¶…é€Ÿ": "è¶…è¿‡è§„å®šæ—¶é€Ÿ",
        "é—¯çº¢ç¯": "è¿åäº¤é€šä¿¡å·",
        # æ¯’å“ç±»
        "å¸æ¯’": "å¸é£Ÿæ¯’å“",
        "å¸ç²‰": "å¸é£Ÿæ¯’å“",
        "è´©æ¯’": "è´©å–æ¯’å“",
        "è´©ç²‰": "è´©å–æ¯’å“",
        "å–æ¯’å“": "è´©å–æ¯’å“",
        "åˆ¶æ¯’": "åˆ¶é€ æ¯’å“",
        "è¿æ¯’": "è¿è¾“æ¯’å“",
        "ç§å¤§éº»": "ç§æ¤æ¯’å“åŸæ¤ç‰©",
        # å–æ·«å«–å¨¼ç±»
        "å«–": "å«–å¨¼",
        "å«–å¨¼": "å–æ·«å«–å¨¼",
        "å–æ·«": "å–æ·«å«–å¨¼",
        "å–èº«": "å–æ·«å«–å¨¼",
        "ç»„ç»‡å–æ·«": "ç»„ç»‡å–æ·«",
        "å®¹ç•™å–æ·«": "å®¹ç•™å–æ·«",
        # èµŒåšç±»
        "èµŒåš": "èµŒåš",
        "èµŒé’±": "èµŒåš",
        "é»„èµŒæ¯’": "èµŒåš",
        "å¼€èµŒåœº": "å¼€è®¾èµŒåœº",
        "ç½‘èµŒ": "èµŒåš",
        "èšä¼—èµŒåš": "èµŒåš",
        # æ²»å®‰ç±»
        "é—¹äº‹": "å¯»è¡…æ»‹äº‹",
        "è€æµæ°“": "å¯»è¡…æ»‹äº‹",
        "æŒ‘è¡…": "å¯»è¡…æ»‹äº‹",
        "æ‹¦è·¯": "å¯»è¡…æ»‹äº‹",
        "éªšæ‰°": "éªšæ‰°",
        "è·Ÿè¸ª": "è·Ÿè¸ªéªšæ‰°",
        "å·æ‹": "å·çª¥å·æ‹",
        "å·çª¥": "å·çª¥å·æ‹",
        "é—¯å…¥åˆ«äººå®¶": "éæ³•ä¾µå…¥ä½å®…",
        "å¼ºè¡Œé—¯å…¥": "éæ³•ä¾µå…¥ä½å®…",
        "éæ³•æ‹˜ç•™": "éæ³•æ‹˜ç¦",
        "éæ³•å…³æŠ¼": "éæ³•æ‹˜ç¦",
        "ç»‘æ¶": "ç»‘æ¶",
        "æ‹å–": "æ‹å–",
        "æ‹å–å¦‡å¥³": "æ‹å–å¦‡å¥³å„¿ç«¥",
        "æ‹å–å„¿ç«¥": "æ‹å–å¦‡å¥³å„¿ç«¥",
        "ä¼ é”€": "ç»„ç»‡é¢†å¯¼ä¼ é”€",
        # å…¬å…±ç§©åºç±»
        "é€ è°£": "æ•£å¸ƒè°£è¨€",
        "è°£è¨€": "æ•£å¸ƒè°£è¨€",
        "ä¼ è°£": "æ•£å¸ƒè°£è¨€",
        "æŠ¥å‡è­¦": "è°æŠ¥è­¦æƒ…",
        "è°æŠ¥": "è°æŠ¥è­¦æƒ…",
        "å‡æŠ¥è­¦": "è°æŠ¥è­¦æƒ…",
        "æ‰°ä¹±ç§©åº": "æ‰°ä¹±å…¬å…±ç§©åº",
        "é—¹äº‹": "æ‰°ä¹±å…¬å…±ç§©åº",
        "é˜»ç¢æ‰§æ³•": "é˜»ç¢æ‰§è¡ŒèŒåŠ¡",
        "å¦¨ç¢å…¬åŠ¡": "é˜»ç¢æ‰§è¡ŒèŒåŠ¡",
        "è¢­è­¦": "è¢­è­¦",
        "æ‰“è­¦å¯Ÿ": "è¢­è­¦",
        "ä¼ªé€ ": "ä¼ªé€ å˜é€ ",
        "å‡è¯": "ä¼ªé€ å˜é€ ",
        "å‡èº«ä»½è¯": "ä¼ªé€ å±…æ°‘èº«ä»½è¯",
        # æªæ”¯ç®¡åˆ¶ç±»
        "ç§è—æªæ”¯": "éæ³•æŒæœ‰æªæ”¯",
        "éæ³•æŒæª": "éæ³•æŒæœ‰æªæ”¯",
        "æºå¸¦ç®¡åˆ¶åˆ€å…·": "éæ³•æºå¸¦ç®¡åˆ¶å™¨å…·",
        "å¸¦åˆ€": "éæ³•æºå¸¦ç®¡åˆ¶å™¨å…·",
        # å…¶ä»–å¸¸è§
        "å¯»è¡…æ»‹äº‹": "å¯»è¡…æ»‹äº‹",
        "çŒ¥äºµ": "çŒ¥äºµ",
        "å¼ºå¥¸": "å¼ºå¥¸",
        "æ€§éªšæ‰°": "çŒ¥äºµ",
        "éæ³•ç»è¥": "éæ³•ç»è¥",
        "ä¾µçŠ¯éšç§": "ä¾µçŠ¯å…¬æ°‘ä¸ªäººä¿¡æ¯",
        "æ³„éœ²ä¸ªäººä¿¡æ¯": "ä¾µçŠ¯å…¬æ°‘ä¸ªäººä¿¡æ¯",
    }
    
    # åŒä¹‰è¯æ˜ å°„ï¼šæ”¯æŒç²¾ç¡®åŒ¹é…å’ŒåŒ…å«åŒ¹é…
    mapped_keyword = SYNONYM_MAP.get(keywords)
    if mapped_keyword:
        print(f"[AI Service] åŒä¹‰è¯æ˜ å°„: '{keywords}' -> '{mapped_keyword}'")
        keywords = mapped_keyword
    else:
        # å°è¯•åŒ…å«åŒ¹é…ï¼ˆå¦‚"é†‰é©¾å¤„ç½š"åŒ…å«"é†‰é©¾"ï¼‰
        for key, val in SYNONYM_MAP.items():
            if key in keywords:
                original = keywords
                keywords = keywords.replace(key, val)
                print(f"[AI Service] åŒä¹‰è¯éƒ¨åˆ†æ›¿æ¢: '{original}' -> '{keywords}'")
                break
    
    # ========== æ³•å¾‹åç§°åˆ«åè§£æ ==========
    # åˆ©ç”¨ law_aliases.json å°†ç®€ç§°è§£æä¸ºå…¨ç§°
    resolved_law_name = law_name
    if law_name:
        alias_resolved = _resolve_law_alias(law_name)
        if alias_resolved and alias_resolved != _normalize_law_name(law_name):
            print(f"[AI Service] æ³•å¾‹åç§°åˆ«åè§£æ: '{law_name}' -> '{alias_resolved}'")
            resolved_law_name = alias_resolved
    
    # ç¡®å®šè¦æœç´¢çš„æ³•å¾‹åç§°
    search_law_name = resolved_law_name or keywords
    
    print(f"[AI Service] æ£€ç´¢æ³•å¾‹: '{search_law_name}', æ¡å·: {article_num}")
    
    # ========== åˆ¤æ–­å…³é”®è¯ç±»å‹ ==========
    # å¦‚æœå…³é”®è¯åƒæ³•å¾‹åç§°ï¼ˆåŒ…å«"æ³•""æ¡ä¾‹""è§„å®š""è§£é‡Š"ç­‰ï¼‰ï¼Œæ‰è¿›è¡Œæ ‡é¢˜åŒ¹é…
    # å¦åˆ™ï¼ˆå¦‚"èµŒåš""å–æ·«"ç­‰è¡Œä¸ºå…³é”®è¯ï¼‰ï¼Œç›´æ¥è¿›è¡Œå†…å®¹æ£€ç´¢
    def looks_like_law_name(name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åƒæ³•å¾‹åç§°"""
        law_indicators = ["æ³•", "æ¡ä¾‹", "è§„å®š", "è§„åˆ™", "åŠæ³•", "è§£é‡Š", "æ„è§", "é€šçŸ¥", "å†³å®š"]
        return any(ind in name for ind in law_indicators)
    
    # å¦‚æœæ˜¾å¼æä¾›äº† law_nameï¼Œæˆ–å…³é”®è¯åƒæ³•å¾‹åç§°ï¼Œæ‰å°è¯•æ ‡é¢˜åŒ¹é…
    should_try_title_match = law_name or looks_like_law_name(search_law_name)
    matching_laws = []
    
    if should_try_title_match:
        # ========== ç¬¬ä¸€æ­¥ï¼šå°è¯•æŒ‰æ³•å¾‹æ ‡é¢˜åŒ¹é… ==========
        # ç”¨æ­£åˆ™åŒ¹é…æ³•å¾‹æ ‡é¢˜ï¼ˆæ”¯æŒæ¨¡ç³ŠåŒ¹é…ï¼‰
        law_name_pattern = search_law_name.replace("ä¸­åäººæ°‘å…±å’Œå›½", "").strip()
        law_regex = {"title": {"$regex": law_name_pattern, "$options": "i"}}
        
        matching_laws = await laws_collection.find(
            law_regex, {"law_id": 1, "title": 1}
        ).to_list(length=10)
    else:
        print(f"[AI Service] '{search_law_name}' ä¸åƒæ³•å¾‹åç§°ï¼Œè·³è¿‡æ ‡é¢˜åŒ¹é…ï¼Œç›´æ¥å†…å®¹æ£€ç´¢")
    
    if matching_laws:
        print(f"[AI Service] åŒ¹é…åˆ° {len(matching_laws)} éƒ¨æ³•å¾‹: {[l['title'] for l in matching_laws]}")
        
        # ========== è¿‡æ»¤æ—§ç‰ˆæœ¬æ³•å¾‹ï¼Œåªä¿ç•™æœ€æ–°ç‰ˆ ==========
        latest_laws = _filter_latest_laws(matching_laws)
        print(f"[AI Service] è¿‡æ»¤åä¿ç•™æœ€æ–°ç‰ˆæœ¬: {[l['title'] for l in latest_laws]}")
        
        # å–æœ€æ–°ç‰ˆæ³•å¾‹çš„ law_id
        law_ids = [law["law_id"] for law in latest_laws]
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦åœ¨åŒ¹é…åˆ°çš„æ³•å¾‹å†…è¿›è¡Œå†…å®¹æœç´¢
        # å¦‚æœ keywords ä¸åŒäº law_nameï¼ˆå¦‚ law_name="æ²»å®‰ç®¡ç†å¤„ç½šæ³•", keywords="å¸æ¯’"ï¼‰
        # åˆ™éœ€è¦è¿›è¡Œå†…å®¹æœç´¢ï¼Œè€Œä¸æ˜¯è¿”å›å‰Næ¡
        needs_content_search = keywords and law_name and keywords != law_name and not looks_like_law_name(keywords)
        
        if needs_content_search:
            print(f"[AI Service] åœ¨åŒ¹é…åˆ°çš„æ³•å¾‹ä¸­æœç´¢å…³é”®è¯: '{keywords}'")
            
            # ä¼˜å…ˆå°è¯•å‘é‡æœç´¢ï¼ˆé™å®šåœ¨åŒ¹é…åˆ°çš„æ³•å¾‹èŒƒå›´å†…ï¼‰
            import os
            vector_enabled = os.getenv("VECTOR_SEARCH_ENABLED", "true").lower() == "true"
            
            if vector_enabled:
                try:
                    # å‘é‡æœç´¢ï¼Œè¿‡æ»¤ç»“æœåªä¿ç•™åŒ¹é…åˆ°çš„æ³•å¾‹
                    vector_items = await law_service.vector_search_for_rag(keywords, top_k=top_k * 3)
                    if vector_items:
                        # è¿‡æ»¤ï¼šåªä¿ç•™åŒ¹é…åˆ°çš„æ³•å¾‹çš„æ¡æ–‡ï¼Œä¸”ç›¸ä¼¼åº¦é«˜äºé˜ˆå€¼
                        filtered_items = [
                            item for item in vector_items
                            if item.get("law_id") in law_ids
                            and item.get("similarity", 0) >= VECTOR_SIMILARITY_THRESHOLD
                        ]
                        if filtered_items:
                            print(f"[AI Service] å‘é‡æœç´¢åœ¨ {latest_laws[0]['title']} ä¸­æ‰¾åˆ° {len(filtered_items)} æ¡ç›¸å…³æ¡æ–‡")
                            articles = []
                            for item in filtered_items[:top_k]:
                                content = item.get("content", "")
                                articles.append({
                                    "law_title": item.get("law_title", ""),
                                    "article_display": item.get("article_display", ""),
                                    "content": content[:MAX_ARTICLE_CONTENT_LEN] if len(content) > MAX_ARTICLE_CONTENT_LEN else content,
                                })
                            return {
                                "found": True,
                                "message": f"åœ¨ã€Š{latest_laws[0]['title']}ã€‹ä¸­æ£€ç´¢åˆ° {len(articles)} æ¡ç›¸å…³æ³•è§„ï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰",
                                "articles": articles
                            }
                except Exception as e:
                    print(f"[AI Service] âš ï¸ å‘é‡æœç´¢å¼‚å¸¸: {e}, å›é€€åˆ°å…³é”®è¯æœç´¢")
            
            # å‘é‡æœç´¢æ— ç»“æœï¼Œå°è¯•å…³é”®è¯å†…å®¹åŒ¹é…
            article_query = {
                "law_id": {"$in": law_ids},
                "content": {"$regex": keywords, "$options": "i"}
            }
            articles = await articles_collection.find(article_query).sort("article_num", 1).limit(top_k).to_list(length=top_k)
            
            if articles:
                law_map = {law["law_id"]: law["title"] for law in latest_laws}
                items = []
                for article in articles:
                    items.append({
                        "law_id": article.get("law_id"),
                        "law_title": law_map.get(article.get("law_id"), ""),
                        "article_num": article.get("article_num"),
                        "article_display": article.get("article_display", ""),
                        "content": article.get("content", ""),
                    })
                print(f"[AI Service] åœ¨æ³•å¾‹ä¸­æŒ‰å†…å®¹åŒ¹é…åˆ° {len(items)} æ¡æ¡æ–‡")
                return await _filter_and_format_results(items, keywords, top_k)
        
        # æŸ¥è¯¢è¿™äº›æ³•å¾‹çš„æ¡æ–‡ï¼ˆä»…å½“æŒ‡å®šäº†æ¡å·ï¼Œæˆ–ä¸éœ€è¦å†…å®¹æœç´¢æ—¶ï¼‰
        article_query = {"law_id": {"$in": law_ids}}
        if article_num:
            # å¦‚æœæŒ‡å®šäº†æ¡å·ï¼Œç”¨ article_display æ­£åˆ™åŒ¹é…
            chinese_num = law_service._arabic_to_chinese(article_num)
            article_query["article_display"] = {"$regex": f"^ç¬¬{chinese_num}æ¡", "$options": "i"}
        
        articles = await articles_collection.find(article_query).sort("article_num", 1).limit(top_k).to_list(length=top_k)
        
        if articles:
            # æ„å»ºæ³•å¾‹IDåˆ°æ ‡é¢˜çš„æ˜ å°„
            law_map = {law["law_id"]: law["title"] for law in latest_laws}
            
            items = []
            for article in articles:
                items.append({
                    "law_id": article.get("law_id"),
                    "law_title": law_map.get(article.get("law_id"), ""),
                    "article_num": article.get("article_num"),
                    "article_display": article.get("article_display", ""),
                    "content": article.get("content", ""),
                })
            
            print(f"[AI Service] æŒ‰æ³•å¾‹æ ‡é¢˜åŒ¹é…åˆ° {len(items)} æ¡æ¡æ–‡")
            # è·³è¿‡åç»­çš„å…¨æ–‡æ£€ç´¢ï¼Œç›´æ¥è¿›å…¥ç‰ˆæœ¬è¿‡æ»¤
            return await _filter_and_format_results(items, keywords, top_k)

    
    # ========== ç¬¬äºŒæ­¥ï¼šå°è¯•å‘é‡è¯­ä¹‰æœç´¢ ==========
    import os
    vector_enabled = os.getenv("VECTOR_SEARCH_ENABLED", "true").lower() == "true"
    
    if vector_enabled:
        print(f"[AI Service] å°è¯•å‘é‡è¯­ä¹‰æœç´¢: '{keywords}'")
        vector_items = await law_service.vector_search_for_rag(keywords, top_k=top_k * 2)
        if vector_items:
            # åº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
            vector_items = [item for item in vector_items if item.get("similarity", 0) >= VECTOR_SIMILARITY_THRESHOLD]
            if vector_items:
                print(f"[AI Service] å‘é‡æœç´¢æˆåŠŸï¼ˆé˜ˆå€¼è¿‡æ»¤åï¼‰ï¼Œè¿”å› {len(vector_items)} æ¡ç»“æœ")
                # å‘é‡æœç´¢ç»“æœæ ¼å¼åŒ–è¿”å›ï¼Œå¹¶æŒ‰æ³•å¾‹æƒé‡äºŒæ¬¡æ’åº
                vector_items.sort(key=lambda x: (-get_law_weight(x.get("law_title", "")), -x.get("similarity", 0)))
                articles = []
                for item in vector_items[:top_k]:
                    content = item.get("content", "")
                    articles.append({
                        "law_title": item.get("law_title", ""),
                        "article_display": item.get("article_display", ""),
                        "content": content[:MAX_ARTICLE_CONTENT_LEN] if len(content) > MAX_ARTICLE_CONTENT_LEN else content,
                    })
                return await _filter_and_format_results_from_articles(articles, keywords, top_k)
            else:
                print(f"[AI Service] å‘é‡æœç´¢æœ‰ç»“æœä½†ç›¸ä¼¼åº¦å‡ä½äºé˜ˆå€¼ {VECTOR_SIMILARITY_THRESHOLD}")
        else:
            print(f"[AI Service] å‘é‡æœç´¢æ— ç»“æœæˆ–æœåŠ¡ä¸å¯ç”¨ï¼Œå›é€€åˆ°å…³é”®è¯æœç´¢")
    
    # ========== ç¬¬ä¸‰æ­¥ï¼šå›é€€åˆ°å…¨æ–‡æ£€ç´¢ ==========
    print(f"[AI Service] ä½¿ç”¨å…³é”®è¯æ£€ç´¢")
    
    # æ„å»ºæœç´¢æŸ¥è¯¢
    if resolved_law_name and article_num:
        query = f"{resolved_law_name}ç¬¬{article_num}æ¡"
    elif resolved_law_name and keywords and resolved_law_name != keywords:
        query = f"{resolved_law_name} {keywords}"
    elif resolved_law_name:
        query = resolved_law_name
    else:
        query = keywords
    
    print(f"[AI Service] å…¨æ–‡æ£€ç´¢æŸ¥è¯¢: '{query}'")
    
    items = await law_service.search_for_rag(query, top_k=top_k * 2)
    
    # ========== ç¬¬å››æ­¥ï¼šå¦‚æœæ— ç»“æœï¼Œå°è¯•å¤šç§å›é€€ç­–ç•¥ ==========
    if not items:
        print(f"[AI Service] å…¨æ–‡æ£€ç´¢æ— ç»“æœï¼Œå°è¯•å›é€€ç­–ç•¥")
        
        # ç­–ç•¥ Aï¼šä»…ç”¨å…³é”®è¯æœç´¢ï¼ˆå»æ‰æ³•å¾‹åç§°é™å®šï¼‰
        if keywords and query != keywords:
            print(f"[AI Service] å›é€€ç­–ç•¥Aï¼šä»…ç”¨å…³é”®è¯ '{keywords}' æœç´¢")
            items = await law_service.search_for_rag(keywords, top_k=top_k * 2)
        
        # ç­–ç•¥ Bï¼šå°è¯•ç”¨æ­£åˆ™ç›´æ¥æœç´¢ articles é›†åˆ
        if not items:
            print(f"[AI Service] å›é€€ç­–ç•¥Bï¼šæ­£åˆ™æœç´¢ '{keywords}'")
            regex_results = await articles_collection.find(
                {"content": {"$regex": re.escape(keywords), "$options": "i"}}
            ).sort("article_num", 1).limit(top_k * 2).to_list(length=top_k * 2)
            
            if regex_results:
                # å…³è”æ³•å¾‹ä¿¡æ¯
                law_ids_set = list({a.get("law_id") for a in regex_results if a.get("law_id")})
                law_docs = await laws_collection.find(
                    {"law_id": {"$in": law_ids_set}},
                    {"law_id": 1, "title": 1, "category": 1}
                ).to_list(length=len(law_ids_set))
                law_map = {l["law_id"]: l for l in law_docs}
                
                items = []
                for a in regex_results:
                    law_info = law_map.get(a.get("law_id"), {})
                    items.append({
                        "law_id": a.get("law_id"),
                        "law_title": law_info.get("title", ""),
                        "article_num": a.get("article_num"),
                        "article_display": a.get("article_display", ""),
                        "content": a.get("content", ""),
                    })
                print(f"[AI Service] æ­£åˆ™æœç´¢æ‰¾åˆ° {len(items)} æ¡ç»“æœ")
        
        # ç­–ç•¥ Cï¼šå°è¯•æ‹†åˆ†å…³é”®è¯åˆ†åˆ«æœç´¢ï¼ˆå¦‚"é†‰é…’é©¾é©¶"â†’"é†‰é…’""é©¾é©¶"ï¼‰
        if not items and len(keywords) > 2:
            # å°è¯•ç”¨æ¯ä¸ª 2-gram æœç´¢
            for i in range(0, len(keywords) - 1, 2):
                sub_kw = keywords[i:i+2]
                sub_items = await law_service.search_for_rag(sub_kw, top_k=top_k)
                if sub_items:
                    print(f"[AI Service] å›é€€ç­–ç•¥Cï¼šå­å…³é”®è¯ '{sub_kw}' æ‰¾åˆ° {len(sub_items)} æ¡")
                    items = sub_items
                    break
    
    if not items:
        return {
            "found": False,
            "message": f"æœªæ£€ç´¢åˆ°ä¸'{keywords}'ç›¸å…³çš„æ³•è§„æ¡æ–‡",
            "articles": []
        }
    
    return await _filter_and_format_results(items, keywords, top_k)


def _get_law_year(title: str) -> int:
    """ä»æ³•å¾‹æ ‡é¢˜ä¸­æå–å¹´ä»½"""
    import re
    # æ”¯æŒå¤šç§æ ¼å¼ï¼šï¼ˆ2018å¹´ä¿®æ­£ï¼‰ã€ï¼ˆ2025å¹´ä¿®è®¢ï¼‰ã€ï¼ˆ2020å¹´ï¼‰
    year_pattern = r'[ï¼ˆ\(](\d{4})å¹´?[ä¿®è®¢æ­£]*[ï¼‰\)]'
    match = re.search(year_pattern, title)
    return int(match.group(1)) if match else 0


def _get_law_base_name(title: str) -> str:
    """æå–æ³•å¾‹åŸºç¡€åç§°ï¼ˆå»æ‰å¹´ä»½éƒ¨åˆ†ï¼‰"""
    import re
    year_pattern = r'[ï¼ˆ\(]\d{4}å¹´?[ä¿®è®¢æ­£]*[ï¼‰\)]'
    return re.sub(year_pattern, '', title).strip()


def _filter_latest_laws(laws: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """æŒ‰åŸºç¡€åç§°åˆ†ç»„ï¼Œä¿ç•™æœ€æ–°ç‰ˆæœ¬çš„æ³•å¾‹"""
    law_by_base = {}
    for law in laws:
        base_name = _get_law_base_name(law["title"])
        year = _get_law_year(law["title"])
        if base_name not in law_by_base or year > law_by_base[base_name]["year"]:
            law_by_base[base_name] = {"law": law, "year": year}
    return [v["law"] for v in law_by_base.values()]


async def execute_lookup_law_article(
    db: AsyncIOMotorDatabase,
    law_name: str,
    article_num: int,
) -> Dict[str, Any]:
    """
    ç²¾å‡†æŸ¥è¯¢æŸéƒ¨æ³•å¾‹çš„å…·ä½“æŸæ¡ï¼ˆæ–°å¢çš„ç²¾å‡†æ£€ç´¢å·¥å…·ï¼‰
    """
    law_service = LawService(db)
    laws_collection = db["laws"]
    articles_collection = db["law_articles"]
    
    # ä½¿ç”¨åˆ«åè§£ææ‰¾åˆ°æ³•å¾‹å…¨ç§°
    resolved = _resolve_law_alias(law_name)
    if resolved:
        search_name = resolved
    else:
        search_name = law_name
    
    # å»æ‰å¸¸è§å‰ç¼€ä»¥è¿›è¡Œæ¨¡ç³ŠåŒ¹é…
    clean_name = search_name.replace("ä¸­åäººæ°‘å…±å’Œå›½", "").strip()
    
    # æŸ¥æ‰¾æ³•å¾‹
    matching_laws = await laws_collection.find(
        {"title": {"$regex": clean_name, "$options": "i"}},
        {"law_id": 1, "title": 1}
    ).to_list(length=10)
    
    if not matching_laws:
        # å°è¯•æ›´æ¨¡ç³Šçš„åŒ¹é…
        matching_laws = await law_service._find_laws_by_keyword(clean_name)
    
    if not matching_laws:
        return {
            "found": False,
            "message": f"æœªæ‰¾åˆ°æ³•å¾‹'{law_name}'",
            "articles": []
        }
    
    # è¿‡æ»¤ä¿ç•™æœ€æ–°ç‰ˆæœ¬
    latest_laws = _filter_latest_laws(matching_laws)
    law_ids = [law["law_id"] for law in latest_laws]
    
    # ä½¿ç”¨ article_display æ­£åˆ™ç²¾å‡†åŒ¹é…æ¡å·
    chinese_num = law_service._arabic_to_chinese(article_num)
    # åŒæ—¶åŒ¹é… "ç¬¬XXæ¡" å’Œ "ç¬¬XXæ¡ä¹‹ä¸€" ç­‰
    display_pattern = f"^ç¬¬{chinese_num}æ¡"
    
    articles = await articles_collection.find({
        "law_id": {"$in": law_ids},
        "article_display": {"$regex": display_pattern}
    }).sort("article_num", 1).to_list(length=5)
    
    if not articles:
        return {
            "found": False,
            "message": f"åœ¨ã€Š{latest_laws[0]['title']}ã€‹ä¸­æœªæ‰¾åˆ°ç¬¬{article_num}æ¡",
            "articles": []
        }
    
    law_map = {law["law_id"]: law["title"] for law in latest_laws}
    result_articles = []
    for article in articles:
        content = article.get("content", "")
        result_articles.append({
            "law_title": law_map.get(article.get("law_id"), ""),
            "article_display": article.get("article_display", ""),
            "content": content[:MAX_ARTICLE_CONTENT_LEN] if len(content) > MAX_ARTICLE_CONTENT_LEN else content,
        })
    
    return {
        "found": True,
        "message": f"åœ¨ã€Š{latest_laws[0]['title']}ã€‹ä¸­æ‰¾åˆ°ç¬¬{article_num}æ¡",
        "articles": result_articles
    }


async def _filter_and_format_results(
    items: List[Dict[str, Any]],
    keywords: str,
    top_k: int,
) -> Dict[str, Any]:
    """
    è¿‡æ»¤æ—§ç‰ˆæœ¬æ³•è§„å¹¶æ ¼å¼åŒ–ç»“æœï¼ŒæŒ‰æ³•å¾‹æƒé‡æ’åº
    """
    import re
    
    def extract_base_name_and_year(title: str) -> tuple:
        """ä»æ ‡é¢˜ä¸­æå–åŸºç¡€æ³•è§„åå’Œå¹´ä»½"""
        return _get_law_base_name(title), _get_law_year(title)
    
    # æŒ‰ (æ³•è§„åŸºç¡€å, æ¡å·) åˆ†ç»„ï¼Œä¿ç•™æœ€æ–°ç‰ˆæœ¬
    grouped = {}
    for item in items:
        law_title = item.get("law_title", "")
        article_display = item.get("article_display", "")
        base_name, year = extract_base_name_and_year(law_title)
        key = (base_name, article_display)
        
        if key not in grouped or year > grouped[key]["year"]:
            grouped[key] = {
                "item": item,
                "year": year
            }
    
    # æŒ‰æ³•å¾‹æƒé‡æ’åºï¼ˆæ ¸å¿ƒæ³•å¾‹ä¼˜å…ˆï¼‰
    sorted_items = sorted(
        grouped.values(),
        key=lambda v: (-get_law_weight(v["item"].get("law_title", "")), v["item"].get("article_num", 0))
    )
    filtered_items = [v["item"] for v in sorted_items][:top_k]
    
    # æ ¼å¼åŒ–ç»“æœ
    articles = []
    for item in filtered_items:
        law_title = item.get("law_title", "")
        article_display = item.get("article_display", "")
        content = item.get("content", "")
        articles.append({
            "law_title": law_title,
            "article_display": article_display,
            "content": content[:MAX_ARTICLE_CONTENT_LEN] if len(content) > MAX_ARTICLE_CONTENT_LEN else content,
        })
    
    return {
        "found": len(articles) > 0,
        "message": f"æ£€ç´¢åˆ° {len(articles)} æ¡ç›¸å…³æ³•è§„" if articles else f"æœªæ£€ç´¢åˆ°ä¸'{keywords}'ç›¸å…³çš„æ³•è§„æ¡æ–‡",
        "articles": articles
    }


async def _filter_and_format_results_from_articles(
    articles: List[Dict[str, Any]],
    keywords: str,
    top_k: int,
) -> Dict[str, Any]:
    """
    å¯¹å·²æ ¼å¼åŒ–çš„ articles åˆ—è¡¨è¿›è¡Œç‰ˆæœ¬è¿‡æ»¤
    """
    # æŒ‰ (æ³•è§„åŸºç¡€å, æ¡å·) å»é‡ï¼Œä¿ç•™æœ€æ–°ç‰ˆæœ¬
    grouped = {}
    for article in articles:
        law_title = article.get("law_title", "")
        article_display = article.get("article_display", "")
        base_name = _get_law_base_name(law_title)
        year = _get_law_year(law_title)
        key = (base_name, article_display)
        
        if key not in grouped or year > grouped[key]["year"]:
            grouped[key] = {"article": article, "year": year}
    
    filtered = [v["article"] for v in grouped.values()][:top_k]
    
    return {
        "found": len(filtered) > 0,
        "message": f"æ£€ç´¢åˆ° {len(filtered)} æ¡ç›¸å…³æ³•è§„" if filtered else f"æœªæ£€ç´¢åˆ°ä¸'{keywords}'ç›¸å…³çš„æ³•è§„æ¡æ–‡",
        "articles": filtered
    }


def _build_messages_with_tools(
    message: str,
    history: Optional[list],
) -> List[Dict[str, str]]:
    """æ„å»ºå¸¦å·¥å…·è°ƒç”¨çš„æ¶ˆæ¯åˆ—è¡¨"""
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    if history:
        messages.extend(history)
    messages.append({"role": "user", "content": message})
    return messages


def _build_messages_with_context(
    message: str,
    history: Optional[list],
    tool_result: str,
    has_results: bool = True,
    related_memory: str = "",
) -> List[Dict[str, str]]:
    """æ„å»ºå¸¦å·¥å…·ç»“æœçš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆç”¨äºç¬¬äºŒè½®è°ƒç”¨ï¼‰"""
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    
    # æ³¨å…¥ç›¸å…³è®°å¿†ä½œä¸º few-shot å‚è€ƒ
    if related_memory:
        messages.append({
            "role": "system",
            "content": f"ä»¥ä¸‹æ˜¯ç»è¿‡éªŒè¯çš„ç›¸ä¼¼é—®ç­”å‚è€ƒï¼ˆä»…ä¾›å‚è€ƒæ ¼å¼å’Œæ€è·¯ï¼Œè¯·ç»“åˆæœ€æ–°æ£€ç´¢ç»“æœå›ç­”ï¼‰ï¼š\n{related_memory}"
        })
    
    if has_results:
        # æœ‰æ£€ç´¢ç»“æœï¼šåŸºäºæ³•è§„æ¡æ–‡å›ç­”
        messages.append({
            "role": "system",
            "content": f"ä»¥ä¸‹ä¸ºä»çŸ¥è¯†åº“æ£€ç´¢åˆ°çš„æ³•è§„æ¡æ–‡ï¼š\n{tool_result}\n\nè¯·ä¸¥æ ¼åŸºäºä¸Šè¿°æ³•è§„å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¼•ç”¨æ—¶æ³¨æ˜æ³•è§„åç§°å’Œæ¡å·ã€‚"
        })
    else:
        # æ— æ£€ç´¢ç»“æœï¼šå…è®¸ LLM ç”¨è‡ªèº«çŸ¥è¯†å…œåº•ï¼Œä½†å¿…é¡»æ ‡æ³¨
        messages.append({
            "role": "system",
            "content": (
                "çŸ¥è¯†åº“ä¸­æœªæ£€ç´¢åˆ°ä¸ç”¨æˆ·é—®é¢˜ç›´æ¥ç›¸å…³çš„æ³•è§„æ¡æ–‡ã€‚\n\n"
                "è¯·æŒ‰ä»¥ä¸‹è§„åˆ™å›ç­”ï¼š\n"
                "1. ä½ å¯ä»¥åŸºäºè‡ªèº«æ³•å¾‹çŸ¥è¯†å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä½†å¿…é¡»åœ¨å›ç­”å¼€å¤´æ˜ç¡®è¯´æ˜ï¼šå½“å‰çŸ¥è¯†åº“ä¸­æœªæ”¶å½•ç›¸å…³æ³•è§„ï¼Œä»¥ä¸‹å†…å®¹åŸºäºAIè‡ªèº«çŸ¥è¯†ï¼Œä»…ä¾›å‚è€ƒ\n"
                "2. å¼•ç”¨å…·ä½“æ³•æ¡æ—¶ï¼Œè¯´æ˜æ¡æ–‡å†…å®¹æ¥è‡ªä½ çš„è®­ç»ƒçŸ¥è¯†ã€æœªç»çŸ¥è¯†åº“éªŒè¯\n"
                "3. å»ºè®®ç”¨æˆ·æŸ¥é˜…æƒå¨æ³•å¾‹æ–‡æœ¬ç¡®è®¤\n"
                "4. å¦‚æœé—®é¢˜å®Œå…¨è¶…å‡ºä½ çš„èƒ½åŠ›èŒƒå›´ï¼Œå¦è¯šè¯´æ˜æ— æ³•å›ç­”"
            )
        })
    
    if history:
        messages.extend(history)
    messages.append({"role": "user", "content": message})
    return messages


async def _call_llm(
    api_url: str,
    api_key: str,
    model: str,
    messages: List[Dict],
    skip_ssl_verify: bool,
    tools: Optional[List[Dict]] = None,
    timeout: httpx.Timeout = LLM_TIMEOUT,
) -> Dict[str, Any]:
    """è°ƒç”¨ LLM APIï¼ˆå†…ç½‘éƒ¨ç½²å¹¶å‘åœºæ™¯ï¼Œé»˜è®¤ read è¶…æ—¶ 180 ç§’ï¼‰"""
    headers = {"Content-Type": "application/json"}
    if api_key:
        headers["Authorization"] = f"Bearer {api_key}"
    
    payload = {
        "model": model,
        "messages": messages,
        "temperature": 0,
        "max_tokens": 2000,
    }
    
    if tools:
        payload["tools"] = tools
        payload["tool_choice"] = "auto"
    
    async with httpx.AsyncClient(timeout=timeout, verify=not skip_ssl_verify) as client:
        response = await client.post(api_url, headers=headers, json=payload)
        response.raise_for_status()
        return response.json()


def _format_tool_result(result: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–å·¥å…·è°ƒç”¨ç»“æœä¸ºäººç±»å¯è¯»çš„æ–‡æœ¬"""
    if not result.get("found"):
        return result.get("message", "æœªæ£€ç´¢åˆ°ç›¸å…³æ³•è§„")
    
    articles = result.get("articles", [])
    if not articles:
        return "æœªæ£€ç´¢åˆ°ç›¸å…³æ³•è§„"
    
    formatted = []
    for i, article in enumerate(articles, 1):
        law_title = article.get("law_title", "")
        article_display = article.get("article_display", "")
        content = article.get("content", "")
        formatted.append(f"[{i}] ã€Š{law_title}ã€‹{article_display}ï¼š{content}")
    
    return "\n\n".join(formatted)


async def chat_with_ai(
    message: str,
    history: Optional[list] = None,
    db: AsyncIOMotorDatabase = None,
    use_rag: bool = True,
    rag_top_k: Optional[int] = None,
) -> Dict[str, Any]:
    """
    ä¸ AI è¿›è¡Œå¯¹è¯ï¼ˆæ”¯æŒ Function Calling + QA è®°å¿†åº“ï¼‰
    
    æµç¨‹ï¼š
    0. è®°å¿†åº“æŸ¥è¯¢ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å·²éªŒè¯çš„ç­”æ¡ˆ
    1. ç¬¬ä¸€è½®ï¼šå‘é€ç”¨æˆ·é—®é¢˜ + å·¥å…·å®šä¹‰ï¼Œè®© AI å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
    2. å¦‚æœ AI è°ƒç”¨å·¥å…·ï¼šæ‰§è¡Œæ£€ç´¢ï¼Œè·å–ç»“æœ
    3. ç¬¬äºŒè½®ï¼šå°†æ£€ç´¢ç»“æœä½œä¸ºä¸Šä¸‹æ–‡ï¼Œè®© AI ç”Ÿæˆæœ€ç»ˆå›ç­”
    """
    # ========== ç¬¬ 0 æ­¥ï¼šæŸ¥è¯¢è®°å¿†åº“ ==========
    if db is not None:
        from app.services.qa_memory_service import QAMemoryService
        memory_service = QAMemoryService(db)
        
        # æŸ¥æ‰¾ç²¾ç¡®åŒ¹é…çš„å·²éªŒè¯ç­”æ¡ˆ
        memory_hit = await memory_service.find_match(message)
        if memory_hit:
            match_type = memory_hit.get("match_type", "exact")
            similarity = memory_hit.get("similarity", 1.0)
            print(f"[AI Service] ğŸ¯ è®°å¿†åº“å‘½ä¸­! type={match_type}, similarity={similarity:.2f}")
            return {
                "reply": memory_hit["answer"],
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                "provider": "qa_memory",
                "sources": memory_hit.get("sources", []),
                "from_memory": True,
            }
    
    # è·å–é…ç½®
    if db is not None:
        config = await get_ai_config(db)
    else:
        config = {
            "api_url": DEFAULT_API_URL,
            "api_key": DEFAULT_API_KEY,
            "model_name": DEFAULT_MODEL,
            "skip_ssl_verify": False,
            "use_function_calling": True,
        }
    
    api_url = config.get("api_url", DEFAULT_API_URL)
    if not api_url:
        raise Exception("AI æœåŠ¡æœªé…ç½® API URLï¼Œè¯·åœ¨åå°ç®¡ç†é¡µé¢é…ç½®")
    
    api_key = config.get("api_key", DEFAULT_API_KEY) or ""
    model = config.get("model_name", DEFAULT_MODEL)
    skip_ssl_verify = config.get("skip_ssl_verify", False)
    provider_id = config.get("provider", "default")
    use_function_calling = config.get("use_function_calling", True)
    top_k = rag_top_k if rag_top_k is not None else config.get("rag_top_k", 6)
    
    total_usage = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
    rag_sources = []
    
    # æŸ¥æ‰¾ç›¸å…³è®°å¿†ä½œä¸º few-shot å‚è€ƒ
    related_memory_context = ""
    if db is not None:
        try:
            related = await memory_service.find_related(message, top_k=2)
            if related:
                examples = []
                for mem in related:
                    q = mem.get("question", "")
                    a = mem.get("answer", "")
                    # æˆªå–ç­”æ¡ˆå‰ 300 å­—ä½œä¸ºå‚è€ƒ
                    a_short = a[:300] + "..." if len(a) > 300 else a
                    examples.append(f"é—®ï¼š{q}\nç­”ï¼š{a_short}")
                related_memory_context = "\n\n".join(examples)
                print(f"[AI Service] æ‰¾åˆ° {len(related)} æ¡ç›¸å…³è®°å¿†ä½œä¸ºå‚è€ƒ")
        except Exception as e:
            print(f"[AI Service] æŸ¥è¯¢ç›¸å…³è®°å¿†å¤±è´¥: {e}")
    
    try:
        if use_rag and use_function_calling and db is not None:
            # ========== Function Calling æ¨¡å¼ ==========
            print(f"[AI Service] Function Calling æ¨¡å¼å¯ç”¨")
            
            # ç¬¬ä¸€è½®ï¼šè®© AI å†³å®šæ˜¯å¦éœ€è¦æ£€ç´¢
            messages = _build_messages_with_tools(message, history)
            
            try:
                data = await _call_llm(
                    api_url, api_key, model, messages, skip_ssl_verify,
                    tools=[LEGAL_SEARCH_TOOL, LOOKUP_ARTICLE_TOOL]
                )
                print(f"[AI Service] LLM å“åº”: tool_calls={data.get('choices', [{}])[0].get('message', {}).get('tool_calls')}")
            except httpx.HTTPStatusError as e:
                # å¦‚æœ API ä¸æ”¯æŒ tools å‚æ•°æˆ–æœåŠ¡ç«¯é”™è¯¯ï¼Œå›é€€åˆ°æ™®é€šæ¨¡å¼
                print(f"[AI Service] HTTP é”™è¯¯ {e.response.status_code}ï¼Œå›é€€åˆ°æ™®é€šæ¨¡å¼")
                if e.response.status_code in (400, 500, 502, 503):
                    return await _fallback_chat(
                        message, history, db, config, top_k
                    )
                raise
            
            # ç´¯è®¡ token ä½¿ç”¨
            usage = data.get("usage", {})
            total_usage["prompt_tokens"] += usage.get("prompt_tokens", 0)
            total_usage["completion_tokens"] += usage.get("completion_tokens", 0)
            total_usage["total_tokens"] += usage.get("total_tokens", 0)
            
            choice = data.get("choices", [{}])[0]
            msg = choice.get("message", {})
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            tool_calls = msg.get("tool_calls")
            
            if tool_calls:
                # AI å†³å®šè°ƒç”¨å·¥å…·ï¼ˆæ”¯æŒå¤šæ¬¡è°ƒç”¨ï¼‰
                print(f"[AI Service] AI è°ƒç”¨äº†å·¥å…·: {len(tool_calls)} ä¸ª")
                all_tool_results = []
                
                for tool_call in tool_calls:
                    func = tool_call.get("function", {})
                    func_name = func.get("name")
                    
                    try:
                        args = json.loads(func.get("arguments", "{}"))
                    except json.JSONDecodeError:
                        args = {}
                    
                    if func_name == "search_legal_knowledge":
                        keywords = args.get("keywords", message)
                        law_name = args.get("law_name")
                        article_num = args.get("article_num")
                        
                        print(f"[AI Service] searchå·¥å…·å‚æ•°: keywords='{keywords}', law_name='{law_name}', article_num={article_num}")
                        
                        result = await execute_search_legal_knowledge(
                            db, keywords, law_name, article_num, top_k
                        )
                        
                    elif func_name == "lookup_law_article":
                        law_name = args.get("law_name", "")
                        article_num = args.get("article_num", 0)
                        
                        print(f"[AI Service] lookupå·¥å…·å‚æ•°: law_name='{law_name}', article_num={article_num}")
                        
                        result = await execute_lookup_law_article(
                            db, law_name, article_num
                        )
                    else:
                        print(f"[AI Service] æœªçŸ¥å·¥å…·: {func_name}")
                        continue
                    
                    print(f"[AI Service] æ£€ç´¢ç»“æœ: found={result.get('found')}, articles_count={len(result.get('articles', []))}")
                    
                    # è®°å½•æ¥æº
                    for article in result.get("articles", []):
                        rag_sources.append({
                            "law_title": article.get("law_title", ""),
                            "article_display": article.get("article_display", ""),
                        })
                    
                    formatted = _format_tool_result(result)
                    if formatted:
                        all_tool_results.append(formatted)
                
                tool_result_text = "\n\n".join(all_tool_results) if all_tool_results else "æœªæ£€ç´¢åˆ°ç›¸å…³æ³•è§„"
                has_db_results = len(rag_sources) > 0
                
                # ç¬¬äºŒè½®ï¼šå¸¦æ£€ç´¢ç»“æœç”Ÿæˆå›ç­”
                print(f"[AI Service] ä¼ ç»™ç¬¬äºŒè½®LLMçš„ä¸Šä¸‹æ–‡(å‰500å­—): {tool_result_text[:500]}...")
                print(f"[AI Service] çŸ¥è¯†åº“æ˜¯å¦æœ‰ç»“æœ: {has_db_results}")
                messages2 = _build_messages_with_context(message, history, tool_result_text, has_results=has_db_results, related_memory=related_memory_context)
                data2 = await _call_llm(
                    api_url, api_key, model, messages2, skip_ssl_verify
                )
                
                # ç´¯è®¡ token ä½¿ç”¨
                usage2 = data2.get("usage", {})
                total_usage["prompt_tokens"] += usage2.get("prompt_tokens", 0)
                total_usage["completion_tokens"] += usage2.get("completion_tokens", 0)
                total_usage["total_tokens"] += usage2.get("total_tokens", 0)
                
                reply = data2.get("choices", [{}])[0].get("message", {}).get("content", "")
            else:
                # AI ç›´æ¥å›ç­”ï¼ˆä¸éœ€è¦æ£€ç´¢ï¼‰
                reply = msg.get("content", "")
        else:
            # ========== æ™®é€šæ¨¡å¼ï¼ˆä¸ä½¿ç”¨ Function Callingï¼‰==========
            return await _fallback_chat(message, history, db, config, top_k)
        
        return {
            "reply": reply or "æŠ±æ­‰ï¼Œæœªèƒ½ç”Ÿæˆå›ç­”ã€‚",
            "usage": total_usage,
            "provider": provider_id,
            "sources": rag_sources,
        }
        
    except httpx.HTTPStatusError as e:
        error_msg = f"AI æœåŠ¡è¯·æ±‚å¤±è´¥: {e.response.status_code}"
        if e.response.status_code == 401:
            error_msg = "AI æœåŠ¡è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API Key é…ç½®"
        elif e.response.status_code == 429:
            error_msg = "AI æœåŠ¡è¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åé‡è¯•"
        raise Exception(error_msg)
    except httpx.TimeoutException:
        raise Exception("AI æœåŠ¡å“åº”è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•")
    except Exception as e:
        raise Exception(f"AI æœåŠ¡å‡ºé”™: {str(e)}")


async def _fallback_chat(
    message: str,
    history: Optional[list],
    db: AsyncIOMotorDatabase,
    config: dict,
    top_k: int,
) -> Dict[str, Any]:
    """
    å›é€€æ¨¡å¼ï¼šä½¿ç”¨ä¼ ç»Ÿ RAG æ–¹å¼ï¼ˆé€‚ç”¨äºä¸æ”¯æŒ Function Calling çš„æ¨¡å‹ï¼‰
    """
    from app.services.knowledge_base_service import KnowledgeBaseService
    
    api_url = config.get("api_url", DEFAULT_API_URL)
    api_key = config.get("api_key", DEFAULT_API_KEY) or ""
    model = config.get("model_name", DEFAULT_MODEL)
    skip_ssl_verify = config.get("skip_ssl_verify", False)
    provider_id = config.get("provider", "default")
    
    rag_context = ""
    rag_sources = []
    
    if db is not None:
        kb_service = KnowledgeBaseService(db)
        rag_data = await kb_service.retrieve(message, top_k=top_k)
        rag_context = rag_data.get("context", "")
        rag_sources = rag_data.get("sources", [])
        direct_answer = rag_data.get("direct_answer", "")
        
        if direct_answer:
            return {
                "reply": direct_answer,
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                "provider": "knowledge_base",
                "sources": rag_sources,
            }
    
    # æ„å»ºæ¶ˆæ¯
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    if rag_context:
        messages.append({
            "role": "system",
            "content": f"ä»¥ä¸‹ä¸ºå¯å¼•ç”¨çš„æ³•è§„æ¡æ–‡æ‘˜è¦ï¼š\n{rag_context}\n\nå›ç­”æ—¶åº”ä¼˜å…ˆå¼•ç”¨ä¸Šè¿°æ¡æ–‡ã€‚"
        })
    if history:
        messages.extend(history)
    messages.append({"role": "user", "content": message})
    
    data = await _call_llm(api_url, api_key, model, messages, skip_ssl_verify)
    
    reply = data.get("choices", [{}])[0].get("message", {}).get("content", "")
    usage = data.get("usage", {})
    
    return {
        "reply": reply or "æŠ±æ­‰ï¼Œæœªèƒ½ç”Ÿæˆå›ç­”ã€‚",
        "usage": {
            "prompt_tokens": usage.get("prompt_tokens", 0),
            "completion_tokens": usage.get("completion_tokens", 0),
            "total_tokens": usage.get("total_tokens", 0),
        },
        "provider": provider_id,
        "sources": rag_sources,
    }
