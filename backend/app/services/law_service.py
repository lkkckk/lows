"""
æ³•è§„ç›¸å…³ä¸šåŠ¡é€»è¾‘
"""
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta
from app.models import LawCreate
from motor.motor_asyncio import AsyncIOMotorDatabase
from pathlib import Path
import json
from app.services.search_engine import get_search_engine
from app.services import embedding_client
import hashlib
import re
import math


# æ³•å¾‹æƒé‡é…ç½®ï¼ˆæƒé‡è¶Šå¤§æ’åºè¶Šé å‰ï¼‰
LAW_WEIGHT_CONFIG = {
    "æ²»å®‰ç®¡ç†å¤„ç½šæ³•": 100,
    "ä¸­åäººæ°‘å…±å’Œå›½æ²»å®‰ç®¡ç†å¤„ç½šæ³•": 100,
    "åˆ‘æ³•": 95,
    "ä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•": 95,
    "åˆ‘äº‹è¯‰è®¼æ³•": 90,
    "ä¸­åäººæ°‘å…±å’Œå›½åˆ‘äº‹è¯‰è®¼æ³•": 90,
    "å…¬å®‰æœºå…³åŠç†è¡Œæ”¿æ¡ˆä»¶ç¨‹åºè§„å®š": 85,
    "å…¬å®‰æœºå…³åŠç†åˆ‘äº‹æ¡ˆä»¶ç¨‹åºè§„å®š": 80,
}
DEFAULT_WEIGHT = 50


COMMON_LAW_PREFIXES = [
    "ä¸­åäººæ°‘å…±å’Œå›½",
    "ä¸­åäººæ°‘",
    "å…¨å›½äººæ°‘ä»£è¡¨å¤§ä¼š",
    "æœ€é«˜äººæ°‘æ³•é™¢",
    "æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢",
]

_LAW_ALIAS_MAP: Optional[Dict[str, str]] = None


def _normalize_law_name(keyword: str) -> str:
    if not keyword:
        return ""

    cleaned = re.sub(r"[^\u4e00-\u9fa5A-Za-z0-9]+", "", keyword)
    if not cleaned:
        return ""

    for prefix in COMMON_LAW_PREFIXES:
        if cleaned.startswith(prefix):
            cleaned = cleaned[len(prefix):]
            break

    return cleaned


def _load_law_alias_map() -> Dict[str, str]:
    global _LAW_ALIAS_MAP
    if _LAW_ALIAS_MAP is not None:
        return _LAW_ALIAS_MAP

    alias_map: Dict[str, str] = {}
    alias_path = Path(__file__).resolve().parents[1] / "data" / "law_aliases.json"
    if alias_path.exists():
        try:
            data = json.loads(alias_path.read_text(encoding="utf-8"))
            if isinstance(data, dict):
                for canonical, aliases in data.items():
                    norm_key = _normalize_law_name(canonical)
                    if norm_key:
                        alias_map[norm_key] = canonical
                    if isinstance(aliases, list):
                        for alias in aliases:
                            norm_alias = _normalize_law_name(alias)
                            if norm_alias:
                                alias_map[norm_alias] = canonical
        except Exception:
            alias_map = {}

    _LAW_ALIAS_MAP = alias_map
    return alias_map


def _resolve_law_alias(keyword: str) -> str:
    normalized = _normalize_law_name(keyword)
    if not normalized:
        return ""
    return _load_law_alias_map().get(normalized, normalized)


def get_law_weight(title: str) -> int:
    """æ ¹æ®æ³•å¾‹æ ‡é¢˜è·å–æƒé‡
    
    æ’åºè§„åˆ™ï¼š
    1. ç²¾ç¡®åŒ¹é…çš„æ ¸å¿ƒæ³•å¾‹æƒé‡æœ€é«˜
    2. åŒ…å«æ ¸å¿ƒæ³•å¾‹å…³é”®è¯çš„æ¬¡ä¹‹
    3. è§£é‡Šã€è§„å®šã€æ„è§ã€é€šçŸ¥ç­‰é…å¥—æ–‡ä»¶é™æƒ
    4. å…¶ä»–é»˜è®¤æƒé‡
    """
    base_weight = DEFAULT_WEIGHT
    
    # ç²¾ç¡®åŒ¹é…
    if title in LAW_WEIGHT_CONFIG:
        base_weight = LAW_WEIGHT_CONFIG[title]
    else:
        # æ¨¡ç³ŠåŒ¹é…ï¼ˆåŒ…å«å…³é”®è¯ï¼‰
        for key, weight in LAW_WEIGHT_CONFIG.items():
            if key in title:
                base_weight = weight
                break
    
    # é…å¥—æ–‡ä»¶é™æƒï¼ˆè§£é‡Šã€è§„å®šã€æ„è§ã€é€šçŸ¥ã€æ‰¹å¤ã€ç­”å¤ã€å†³å®šç­‰ï¼‰
    # è¿™äº›æ–‡ä»¶è™½ç„¶å¯èƒ½åŒ…å«æ ¸å¿ƒæ³•å¾‹åç§°ï¼Œä½†åº”è¯¥æ’åœ¨æ­£æ³•ä¹‹å
    SUPPLEMENTARY_KEYWORDS = [
        "è§£é‡Š", "è§„å®š", "æ„è§", "é€šçŸ¥", "æ‰¹å¤", "ç­”å¤", 
        "å†³å®š", "åŠæ³•", "ç»†åˆ™", "æ¡ä¾‹", "è§„åˆ™", "æŒ‡å¼•",
        "çºªè¦", "å¤å‡½", "å‡½", "å…¬å‘Š"
    ]
    
    for keyword in SUPPLEMENTARY_KEYWORDS:
        if keyword in title:
            # é™ä½æƒé‡ï¼Œä½†ä»ä¿æŒç›¸å¯¹æ’åº
            base_weight = max(base_weight - 20, 1)
            break
    
    return base_weight


class LawService:
    """æ³•è§„æœåŠ¡ç±»"""

    def __init__(self, db: AsyncIOMotorDatabase):
        self.db = db
        self.laws_collection = db.laws
        self.articles_collection = db.law_articles
        self.view_logs_collection = db.view_logs

    async def create_law(self, law_in: LawCreate) -> Dict[str, Any]:
        """åˆ›å»ºæ³•è§„ï¼ˆåŒ…å«æ¡æ–‡ï¼‰"""
        # 1. ç”Ÿæˆ ID
        law_id = hashlib.md5(law_in.title.encode()).hexdigest()[:16]
        
        # 2. å‡†å¤‡æ³•è§„ä¸»æ¡£æ•°æ®
        law_data = law_in.dict(exclude={"articles"})
        law_data["law_id"] = law_id
        # ä¸ºäº†é¿å…å…¨æ–‡è¿‡å¤§ï¼Œè¿™é‡Œå¯ä»¥ç®€å•æ‹¼æ¥å‰å‡ æ¡ä½œä¸º full_text, æˆ–è€…å­˜ç©ºå­—ç¬¦ä¸²
        # æ›´å¥½çš„åšæ³•æ˜¯æŠŠæ‰€æœ‰ content æ‹¼èµ·æ¥
        full_text = "\n".join([a["content"] for a in law_in.articles])
        law_data["full_text"] = full_text
        
        # upsert æ³•è§„
        await self.laws_collection.replace_one(
            {"law_id": law_id}, law_data, upsert=True
        )
        
        # 3. å‡†å¤‡æ¡æ–‡æ•°æ®
        # å…ˆåˆ é™¤æ—§æ¡æ–‡
        await self.articles_collection.delete_many({"law_id": law_id})
        
        article_docs = []
        for art in law_in.articles:
            art_doc = {
                "law_id": law_id,
                "article_num": art["article_num"],
                "article_display": art["article_display"],
                "content": art["content"],
                "chapter": art.get("chapter", ""),
                "section": art.get("section", ""),
                "keywords": [] # TODO: æå–å…³é”®è¯
            }
            article_docs.append(art_doc)
            
        if article_docs:
            # å…ˆå…¥åº“ï¼Œä¸ç­‰å‘é‡åŒ–ï¼ˆå‘é‡åŒ–ç”±åå°ä»»åŠ¡å¼‚æ­¥å®Œæˆï¼‰
            await self.articles_collection.insert_many(article_docs)
            
        return {"law_id": law_id, "article_count": len(article_docs), "message": f"æˆåŠŸå¯¼å…¥ {len(article_docs)} æ¡æ¡æ–‡"}

    async def vectorize_law_articles(self, law_id: str) -> Dict[str, Any]:
        """
        ä¸ºæŒ‡å®šæ³•è§„çš„æ¡æ–‡å¼‚æ­¥ç”Ÿæˆå‘é‡ï¼ˆåå°ä»»åŠ¡è°ƒç”¨ï¼‰
        åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹10æ¡ï¼Œå¸¦é‡è¯•
        """
        BATCH_SIZE = 10
        
        # æ£€æŸ¥å‘é‡æœåŠ¡æ˜¯å¦å¯ç”¨
        if not await embedding_client.check_health():
            print(f"[LawService] âš ï¸ å‘é‡æœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡ {law_id} çš„å‘é‡åŒ–")
            return {"law_id": law_id, "status": "skipped", "reason": "å‘é‡æœåŠ¡ä¸å¯ç”¨"}
        
        # æŸ¥è¯¢è¯¥æ³•è§„ä¸‹æœªå‘é‡åŒ–çš„æ¡æ–‡
        cursor = self.articles_collection.find(
            {"law_id": law_id, "embedding": {"$exists": False}},
            {"_id": 1, "content": 1}
        )
        articles = await cursor.to_list(length=None)
        
        if not articles:
            print(f"[LawService] â„¹ï¸ {law_id} æ— éœ€å‘é‡åŒ–ï¼ˆå·²å…¨éƒ¨å®Œæˆæˆ–æ— æ¡æ–‡ï¼‰")
            return {"law_id": law_id, "status": "done", "vectorized": 0, "failed": 0}
        
        total = len(articles)
        vectorized = 0
        failed = 0
        
        print(f"[LawService] ğŸ”„ å¼€å§‹åå°å‘é‡åŒ– {law_id}: {total} æ¡å¾…å¤„ç†")
        
        for batch_start in range(0, total, BATCH_SIZE):
            batch = articles[batch_start:batch_start + BATCH_SIZE]
            try:
                contents = [a["content"][:2000] for a in batch]
                embeddings = await embedding_client.get_embeddings(contents)
                if embeddings and len(embeddings) == len(batch):
                    for i, emb in enumerate(embeddings):
                        await self.articles_collection.update_one(
                            {"_id": batch[i]["_id"]},
                            {"$set": {"embedding": emb}}
                        )
                    vectorized += len(batch)
                else:
                    failed += len(batch)
                    print(f"[LawService] âš ï¸ æ‰¹æ¬¡ {batch_start // BATCH_SIZE + 1} è¿”å›ä¸åŒ¹é…")
            except Exception as e:
                failed += len(batch)
                print(f"[LawService] âš ï¸ æ‰¹æ¬¡ {batch_start // BATCH_SIZE + 1} å¼‚å¸¸: {e}")
        
        status = "done" if failed == 0 else ("partial" if vectorized > 0 else "failed")
        msg = f"[LawService] {'âœ…' if status == 'done' else 'âš ï¸'} åå°å‘é‡åŒ–å®Œæˆ {law_id}: æˆåŠŸ {vectorized}/{total}"
        if failed > 0:
            msg += f"ï¼Œå¤±è´¥ {failed}"
        print(msg)
        
        return {"law_id": law_id, "status": status, "vectorized": vectorized, "failed": failed, "total": total}

    async def get_laws_list(
        self,
        page: int = 1,
        page_size: int = 20,
        category: Optional[str] = None,
        level: Optional[str] = None,
        status: Optional[str] = None,
        tags: Optional[List[str]] = None,
        title: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        è·å–æ³•è§„åˆ—è¡¨ï¼ˆåˆ†é¡µ + ç­›é€‰ï¼‰
        """
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        query = {}
        if category:
            query["category"] = category
        if level:
            query["level"] = level
        if status:
            query["status"] = status
        if tags:
            query["tags"] = {"$in": tags}
        if title:
            query["title"] = {"$regex": title, "$options": "i"}

        # è®¡ç®—æ€»æ•°
        total = await self.laws_collection.count_documents(query)

        # è®¡ç®—åˆ†é¡µ
        skip = (page - 1) * page_size
        total_pages = math.ceil(total / page_size)

        # æŸ¥è¯¢æ•°æ®ï¼ˆæ’é™¤å…¨æ–‡å­—æ®µä»¥æé«˜æ€§èƒ½ï¼‰
        cursor = self.laws_collection.find(
            query, {"full_text": 0}
        ).sort("effect_date", -1).skip(skip).limit(page_size)

        laws = await cursor.to_list(length=page_size)

        # è½¬æ¢ ObjectId ä¸ºå­—ç¬¦ä¸²
        for law in laws:
            law["_id"] = str(law["_id"])

        return {
            "data": laws,
            "pagination": {
                "total": total,
                "page": page,
                "page_size": page_size,
                "total_pages": total_pages,
            }
        }

    async def get_law_detail(self, law_id: str) -> Optional[Dict[str, Any]]:
        """
        è·å–æ³•è§„è¯¦æƒ…ï¼ˆåŒ…å«å…¨æ–‡ï¼‰
        """
        law = await self.laws_collection.find_one({"law_id": law_id})
        if law:
            law["_id"] = str(law["_id"])
        return law

    async def update_law(self, law_id: str, update_data: Dict[str, Any]) -> bool:
        """
        æ›´æ–°æ³•è§„ä¿¡æ¯
        """
        # åªå…è®¸æ›´æ–°ç‰¹å®šå­—æ®µ
        allowed_fields = {"status", "category", "level", "issue_org", "issue_date", "effect_date", "expire_date", "summary"}
        filtered_data = {k: v for k, v in update_data.items() if k in allowed_fields}
        
        if not filtered_data:
            return False
            
        result = await self.laws_collection.update_one(
            {"law_id": law_id},
            {"$set": filtered_data}
        )
        return result.matched_count > 0

    async def delete_law(self, law_id: str) -> bool:
        """
        åˆ é™¤æ³•è§„åŠå…¶æ‰€æœ‰æ¡æ–‡
        """
        # å…ˆæ£€æŸ¥æ³•è§„æ˜¯å¦å­˜åœ¨
        law = await self.laws_collection.find_one({"law_id": law_id})
        if not law:
            return False
            
        # åˆ é™¤æ‰€æœ‰å…³è”æ¡æ–‡
        await self.articles_collection.delete_many({"law_id": law_id})
        # åˆ é™¤æ³•è§„ä¸»è®°å½•
        await self.laws_collection.delete_one({"law_id": law_id})
        return True

    async def get_law_articles(
        self, law_id: str, chapter: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        è·å–æ³•è§„çš„æ‰€æœ‰æ¡æ–‡
        """
        query = {"law_id": law_id}
        if chapter:
            query["chapter"] = chapter

        cursor = self.articles_collection.find(query).sort("article_num", 1)
        articles = await cursor.to_list(length=None)

        for article in articles:
            article["_id"] = str(article["_id"])

        return articles

    async def get_article_by_number(
        self, law_id: str, article_num: int, sub_index: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®æ¡å·è·å–æ¡æ–‡
        é€šè¿‡å°†é˜¿æ‹‰ä¼¯æ•°å­—è½¬æ¢ä¸ºä¸­æ–‡ï¼ŒåŒ¹é… article_display å­—æ®µ
        """
        # å°†é˜¿æ‹‰ä¼¯æ•°å­—è½¬æ¢ä¸ºä¸­æ–‡æ•°å­—
        chinese_num = self._arabic_to_chinese(article_num)
        # å¦‚æœæœ‰å­æ¡ç›®ï¼ŒåŒ¹é… "ç¬¬XXæ¡ä¹‹Y"ï¼Œå¦åˆ™åŒ¹é… "ç¬¬XXæ¡" (ä¸”åé¢æ²¡æœ‰"ä¹‹")
        if sub_index:
             display_pattern = f"^ç¬¬{chinese_num}æ¡{sub_index}"
        else:
             # Negative lookahead ensuring "ç¬¬XXæ¡" is not followed by "ä¹‹"
             # e.g. match "ç¬¬åå…«æ¡", but not "ç¬¬åå…«æ¡ä¹‹ä¸€"
             # Regex: ^ç¬¬åå…«æ¡(?!ä¹‹)
             display_pattern = f"^ç¬¬{chinese_num}æ¡(?!ä¹‹)"
        
        article = await self.articles_collection.find_one({
            "law_id": law_id,
            "article_display": {"$regex": display_pattern}
        })
        if article:
            article["_id"] = str(article["_id"])
        return article
    
    def _arabic_to_chinese(self, num: int, full_form: bool = False) -> str:
        """
        é˜¿æ‹‰ä¼¯æ•°å­—è½¬ä¸­æ–‡æ•°å­—ï¼ˆæ”¯æŒ1-999ï¼‰
        full_form: å½“ä¸ºTrueæ—¶ï¼Œ10-19ä¼šè½¬æ¢ä¸º"ä¸€å"è€Œä¸æ˜¯"å"
        """
        if num <= 0:
            return "é›¶"
        
        digits = ["é›¶", "ä¸€", "äºŒ", "ä¸‰", "å››", "äº”", "å…­", "ä¸ƒ", "å…«", "ä¹"]
        
        if num < 10:
            return digits[num]
        elif num < 20:
            # ååˆ°åä¹
            # å½“ full_form=True æˆ–å•ç‹¬ä½¿ç”¨æ—¶ï¼Œ10="ä¸€å"ï¼Œ11="ä¸€åä¸€"
            # å½“ full_form=Falseï¼ˆé»˜è®¤ï¼Œç”¨äºæ¡æ–‡é¦–å­—ï¼‰ï¼Œ10="å"ï¼Œ11="åä¸€"
            prefix = "ä¸€" if full_form else ""
            return prefix + "å" + (digits[num % 10] if num % 10 != 0 else "")
        elif num < 100:
            # äºŒååˆ°ä¹åä¹
            tens = num // 10
            ones = num % 10
            result = digits[tens] + "å"
            if ones > 0:
                result += digits[ones]
            return result
        elif num < 1000:
            hundreds = num // 100
            remainder = num % 100
            result = digits[hundreds] + "ç™¾"
            if remainder >= 10:
                # ç™¾ä½åçš„10-19éœ€è¦å®Œæ•´å½¢å¼ï¼Œå¦‚110="ä¸€ç™¾ä¸€å"
                result += self._arabic_to_chinese(remainder, full_form=True)
            elif remainder > 0:
                result += "é›¶" + digits[remainder]
            return result
        else:
            return str(num)  # è¶…è¿‡999ç›´æ¥è¿”å›æ•°å­—

    def parse_article_input(self, input_str: str) -> tuple[Optional[int], Optional[str]]:
        """
        è§£ææ¡å·è¾“å…¥ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼š
        - "ç¬¬å…«åä¸‰æ¡" â†’ (83, None)
        - "ç¬¬å…«åä¸‰æ¡ä¹‹ä¸€" â†’ (83, "ä¹‹ä¸€")
        - "83æ¡" â†’ (83, None)
        - "83" â†’ (83, None)
        è¿”å›: (article_num, sub_index)
        """
        # ä¸­æ–‡æ•°å­—æ˜ å°„
        chinese_num_map = {
            'é›¶': 0, 'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4,
            'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9,
            'å': 10, 'ç™¾': 100, 'åƒ': 1000
        }

        # åŒ¹é…æ¨¡å¼1: ç¬¬XXæ¡[ä¹‹Y]
        pattern1 = r'ç¬¬([é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+)æ¡([ä¹‹][é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)?'
        match = re.search(pattern1, input_str)
        if match:
            chinese_num = match.group(1)
            sub_index = match.group(2) # e.g. "ä¹‹ä¸€"
            return (self._chinese_to_arabic(chinese_num, chinese_num_map), sub_index)

        # åŒ¹é…æ¨¡å¼2: XXæ¡
        pattern2 = r'(\d+)æ¡'
        match = re.search(pattern2, input_str)
        if match:
            return (int(match.group(1)), None)

        # åŒ¹é…æ¨¡å¼3: çº¯æ•°å­—
        pattern3 = r'^\d+$'
        if re.match(pattern3, input_str.strip()):
            return (int(input_str.strip()), None)

        return (None, None)

    def _extract_law_keyword(self, query: str) -> str:
        """
        Extract law name part from a query that includes an article number.
        """
        if not query:
            return ""

        cn_nums = "\u96f6\u4e00\u4e8c\u4e09\u56db\u4e94\u516d\u4e03\u516b\u4e5d\u5341\u767e\u5343"
        # åŒ¹é… "ç¬¬XXæ¡" æˆ– "ç¬¬XXæ¡ä¹‹Y"
        text = re.sub(rf"\u7b2c?[{cn_nums}]+\u6761([ä¹‹][{cn_nums}]+)?", " ", query)
        text = re.sub(r"\u7b2c?\d+\u6761", " ", text) # åŒ¹é… "XXæ¡"
        text = re.sub(r"^(è¯·é—®|é—®ä¸‹|é—®ä¸€ä¸‹|å’¨è¯¢|è¯·æ•™)", "", text)
        # å»é™¤å°¾éƒ¨å¸¸è§é—®å¥ï¼šå†…å®¹æ˜¯ä»€ä¹ˆã€æ˜¯ä»€ä¹ˆå†…å®¹ã€è§„å®šæ˜¯ä»€ä¹ˆã€æ€ä¹ˆè§„å®šçš„ç­‰
        text = re.sub(r"(çš„?å†…å®¹|çš„?è§„å®š|æ¡æ–‡|å«ä¹‰|ä¸»è¦å†…å®¹|å…·ä½“å†…å®¹|è§„å®šå†…å®¹)?(æ˜¯ä»€ä¹ˆ|æ˜¯å•¥|æŒ‡ä»€ä¹ˆ|ä»€ä¹ˆæ„æ€|æŒ‡å•¥|æ€ä¹ˆè§„å®š|æ€ä¹ˆè¯´)?[?ï¼Ÿã€‚ï¼Œï¼›ï¼šâ€¦]*$", "", text)
        text = re.sub(r"[\s\?\uff1f\u3002\uff0c\uff1b\uff1a\u2026]+$", "", text)
        
        # Remove common brackets that might wrap the law name
        text = re.sub(r"[ã€Šã€‹<>ã€ˆã€‰ï¼ˆï¼‰()\[\]ã€ã€‘]", "", text)

        return text.strip()

    def _normalize_law_keyword(self, keyword: str) -> str:
        """
        Normalize law keyword for fuzzy title matching.
        """
        return _normalize_law_name(keyword)

    def _build_title_fuzzy_regex(self, keyword: str) -> Optional[str]:
        """
        Build a fuzzy regex that requires all tokens to appear in the title.
        """
        if not keyword or len(keyword) <= 1:
            return None

        if len(keyword) <= 4:
            tokens = list(keyword)
        else:
            tokens = [keyword[i:i + 2] for i in range(len(keyword) - 1)]

        tokens = [t for t in tokens if t.strip()]
        if not tokens:
            return None

        lookaheads = "".join(f"(?=.*{re.escape(t)})" for t in tokens)
        return f"{lookaheads}.*"

    async def _find_laws_by_keyword(self, keyword: str) -> List[Dict[str, Any]]:
        """
        Find candidate laws by keyword using exact-substring, then fuzzy match.
        """
        if not keyword:
            return []

        exact = await self.laws_collection.find(
            {"title": {"$regex": re.escape(keyword), "$options": "i"}},
            {"full_text": 0},
        ).to_list(length=20)
        if exact:
            return exact

        fuzzy_regex = self._build_title_fuzzy_regex(keyword)
        if not fuzzy_regex:
            return []

        return await self.laws_collection.find(
            {"title": {"$regex": fuzzy_regex, "$options": "i"}},
            {"full_text": 0},
        ).to_list(length=20)

    async def _search_by_law_article(
        self, query: str, article_num: int, page: int, page_size: int, sub_index: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """
        Search by law name + article number pattern.
        ä½¿ç”¨ article_display å­—æ®µçš„æ­£åˆ™åŒ¹é…ï¼Œè€Œé article_num å­—æ®µã€‚
        å› ä¸º article_num å¯èƒ½å› ä¸º"ä¹‹ä¸€"ç±»æ¡æ–‡å¯¼è‡´åºå·é”™ä¹±ã€‚
        """
        raw_keyword = self._extract_law_keyword(query)
        if not raw_keyword:
            return None

        keywords = []
        resolved = _resolve_law_alias(raw_keyword)
        if resolved:
            keywords.append(resolved)
        normalized = _normalize_law_name(raw_keyword)
        if normalized and normalized not in keywords:
            keywords.append(normalized)

        laws = []
        for keyword in keywords:
            laws = await self._find_laws_by_keyword(keyword)
            if laws:
                break
        if not laws:
            return None

        law_map = {law["law_id"]: law for law in laws}
        law_ids = list(law_map.keys())

        # ä½¿ç”¨ article_display æ­£åˆ™åŒ¹é…ï¼Œè€Œé article_num
        # å°†é˜¿æ‹‰ä¼¯æ•°å­—è½¬æ¢ä¸ºä¸­æ–‡æ•°å­—
        chinese_num = self._arabic_to_chinese(article_num)
        
        # æ„å»ºæ­£åˆ™è¡¨è¾¾å¼
        if sub_index:
            # æœç´¢ "ç¬¬XXæ¡ä¹‹Y"
            display_pattern = f"^ç¬¬{chinese_num}æ¡{sub_index}"
        else:
            # æœç´¢ "ç¬¬XXæ¡" ä¸”åé¢ä¸æ˜¯ "ä¹‹"ï¼ˆé¿å…åŒ¹é…åˆ° "ç¬¬XXæ¡ä¹‹ä¸€"ï¼‰
            display_pattern = f"^ç¬¬{chinese_num}æ¡(?!ä¹‹)"

        search_query = {
            "law_id": {"$in": law_ids},
            "article_display": {"$regex": display_pattern}
        }

        total = await self.articles_collection.count_documents(search_query)
        total_pages = math.ceil(total / page_size) if total > 0 else 0
        skip = (page - 1) * page_size

        cursor = self.articles_collection.find(search_query).skip(skip).limit(page_size)
        articles = await cursor.to_list(length=page_size)

        results = []
        for article in articles:
            article["_id"] = str(article["_id"])
            law_info = law_map.get(article["law_id"], {})
            results.append({
                "law_id": article["law_id"],
                "law_title": law_info.get("title", ""),
                "law_category": law_info.get("category", ""),
                "article_num": article.get("article_num"),
                "article_display": article.get("article_display", ""),
                "content": article.get("content", ""),
                "highlight": self._generate_highlight(
                    article.get("content", ""), normalized
                ),
            })

        # æŒ‰æ³•å¾‹æƒé‡æ’åºï¼ˆæƒé‡é«˜çš„æ’å‰é¢ï¼‰
        results.sort(key=lambda x: -get_law_weight(x.get("law_title", "")))

        return {
            "data": results,
            "pagination": {
                "total": total,
                "page": page,
                "page_size": page_size,
                "total_pages": total_pages,
            }
        }

    def _chinese_to_arabic(self, chinese_num: str, num_map: Dict[str, int]) -> int:
        """
        ä¸­æ–‡æ•°å­—è½¬é˜¿æ‹‰ä¼¯æ•°å­—
        æ”¯æŒï¼šä¸€ã€åã€åä¸€ã€äºŒåã€äºŒåä¸‰ã€ä¸€ç™¾ã€ä¸€ç™¾é›¶ä¸‰ã€ä¸€åƒç­‰
        """
        if not chinese_num:
            return 0

        result = 0
        current = 0

        for char in chinese_num:
            num = num_map.get(char, 0)
            if num >= 10:  # å•ä½ï¼ˆåã€ç™¾ã€åƒï¼‰
                if current == 0:
                    current = 1
                result += current * num
                current = 0
            else:
                current = num

        return result + current

    async def search_in_law(
        self, law_id: str, query: str, page: int = 1, page_size: int = 20
    ) -> Dict[str, Any]:
        """
        åœ¨å•ä¸ªæ³•è§„å†…æœç´¢ - æ”¯æŒæ¡å·å’Œå…³é”®å­—
        
        æœç´¢ç­–ç•¥ï¼š
        1. é¦–å…ˆå°è¯•è§£æä¸ºæ¡å·ï¼ˆå¦‚"ç¬¬å…«åä¸‰æ¡"ã€"83"ï¼‰
        2. å¦‚æœä¸æ˜¯æ¡å·ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æœç´¢
        """
        # å…ˆå°è¯•è§£æä¸ºæ¡å·
        article_num, sub_index = self.parse_article_input(query)

        if article_num:
            # æŒ‰æ¡å·ç²¾å‡†æŸ¥è¯¢
            article = await self.get_article_by_number(law_id, article_num, sub_index)
            if article:
                return {
                    "data": [article],
                    "pagination": {
                        "total": 1,
                        "page": 1,
                        "page_size": page_size,
                        "total_pages": 1,
                    }
                }

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æœç´¢ï¼ˆç¡®ä¿å‡†ç¡®æ€§ï¼‰
        search_query = {
            "law_id": law_id,
            "content": {"$regex": query, "$options": "i"}
        }
        total = await self.articles_collection.count_documents(search_query)
        
        # åˆ†é¡µå‚æ•°
        skip = (page - 1) * page_size
        total_pages = math.ceil(total / page_size) if total > 0 else 0
        
        # æŒ‰æ¡å·æ’åº
        cursor = self.articles_collection.find(search_query).sort("article_num", 1).skip(skip).limit(page_size)
        results = await cursor.to_list(length=page_size)

        for result in results:
            result["_id"] = str(result["_id"])
            result["highlight"] = self._generate_highlight(result["content"], query)

        return {
            "data": results,
            "pagination": {
                "total": total,
                "page": page,
                "page_size": page_size,
                "total_pages": total_pages,
            }
        }

    async def search_global(
        self, query: str, page: int = 1, page_size: int = 20
    ) -> Dict[str, Any]:
        """
        å…¨åº“æœç´¢ï¼ˆè·¨æ³•è§„ï¼‰- ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç¡®ä¿å‡†ç¡®æ€§
        
        æ³¨æ„ï¼šMongoDB æ–‡æœ¬ç´¢å¼•å¯¹ä¸­æ–‡æ”¯æŒä¸å®Œå–„ï¼Œä¼šæ¼æ‰å¤§é‡ç»“æœã€‚
        ä¸ºç¡®ä¿æœç´¢å‡†ç¡®æ€§ï¼ˆ100%ä¸æ¼ï¼‰ï¼Œç›´æ¥ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æœç´¢ã€‚
        
        æ’åºç­–ç•¥ï¼šæŒ‰æ³•å¾‹æƒé‡é™åº + æ¡å·å‡åº
        """
        import time
        start_time = time.time()

        article_num, sub_index = self.parse_article_input(query)
        if article_num:
            # å½“ç”¨æˆ·æ˜ç¡®æœç´¢ç‰¹å®šæ¡å·æ—¶ï¼ˆå¦‚"åˆ‘æ³•ç¬¬112æ¡"ï¼‰ï¼Œä¼˜å…ˆä½¿ç”¨ç²¾å‡†æœç´¢
            article_result = await self._search_by_law_article(query, article_num, page, page_size, sub_index)
            if article_result is not None:
                # æ‰¾åˆ°æ³•è§„ï¼Œè¿”å›ç»“æœï¼ˆå³ä½¿ä¸ºç©ºï¼Œä¹Ÿæ„å‘³ç€è¯¥æ¡å·ä¸å­˜åœ¨ï¼‰
                return article_result
            # å¦‚æœ _search_by_law_article è¿”å› Noneï¼ˆæ³•è§„æœªæ‰¾åˆ°ï¼‰ï¼Œ
            # ä»ç„¶ä¸åº”è¯¥å›é€€åˆ°å…¨æ–‡æœç´¢ï¼Œå› ä¸ºé‚£å¯èƒ½è¿”å›è¯¯å¯¼æ€§ç»“æœ
            # ä¾‹å¦‚ï¼šæœ"åˆ‘æ³•ç¬¬112æ¡"å´è¿”å›å¼•ç”¨äº†112æ¡çš„ç¬¬110æ¡
            # è¿”å›ç©ºç»“æœï¼Œè®©ç”¨æˆ·çŸ¥é“æœªæ‰¾åˆ°
            return {
                "data": [],
                "pagination": {
                    "total": 0,
                    "page": page,
                    "page_size": page_size,
                    "total_pages": 0,
                }
            }

        search_engine = get_search_engine()
        if search_engine.enabled:
            try:
                engine_result = await search_engine.search_articles(query, page, page_size)
                if engine_result is not None:
                    return engine_result
            except Exception:
                pass

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æœç´¢ï¼ˆç¡®ä¿å‡†ç¡®æ€§ï¼Œä»»ä½•å­å­—ç¬¦ä¸²éƒ½èƒ½è¢«æ‰¾åˆ°ï¼‰
        search_query = {"content": {"$regex": query, "$options": "i"}}
        total = await self.articles_collection.count_documents(search_query)
        
        # è·å–æ‰€æœ‰åŒ¹é…ç»“æœï¼ˆç”¨äºæƒé‡æ’åºï¼‰
        # æ³¨æ„ï¼šå¯¹äºå¤§é‡æ•°æ®ï¼Œè¿™ç§æ–¹å¼å¯èƒ½æ¯”è¾ƒæ…¢ï¼Œä½†èƒ½ä¿è¯æ’åºå‡†ç¡®
        pipeline = [
            {"$match": search_query},
            {
                "$lookup": {
                    "from": "laws",
                    "localField": "law_id",
                    "foreignField": "law_id",
                    "as": "law_info",
                }
            },
            {"$unwind": "$law_info"},
            {
                "$project": {
                    "_id": 1,
                    "law_id": 1,
                    "law_title": "$law_info.title",
                    "law_category": "$law_info.category",
                    "article_num": 1,
                    "article_display": 1,
                    "content": 1,
                }
            }
        ]

        results = await self.articles_collection.aggregate(pipeline).to_list(length=None)
        
        # æŒ‰æƒé‡æ’åºï¼ˆæƒé‡é™åºï¼ŒåŒæƒé‡æŒ‰æ¡å·å‡åºï¼‰
        results.sort(key=lambda x: (-get_law_weight(x.get("law_title", "")), x.get("article_num", 0)))
        
        # åˆ†é¡µ
        total_pages = math.ceil(total / page_size) if total > 0 else 0
        skip = (page - 1) * page_size
        paged_results = results[skip:skip + page_size]

        for result in paged_results:
            result["_id"] = str(result["_id"])
            result["highlight"] = self._generate_highlight(result["content"], query)

        elapsed_time = time.time() - start_time
        print(f"ğŸ” æœç´¢å®Œæˆ: query=\"{query}\" | results={total} | time={elapsed_time:.3f}s")

        return {
            "data": paged_results,
            "pagination": {
                "total": total,
                "page": page,
                "page_size": page_size,
                "total_pages": total_pages,
            }
        }

    def _generate_highlight(self, content: str, query: str, context_length: int = 100) -> str:
        """
        ç”Ÿæˆé«˜äº®ç‰‡æ®µ
        """
        # ç®€å•å®ç°ï¼šæŸ¥æ‰¾å…³é”®å­—ç¬¬ä¸€æ¬¡å‡ºç°çš„ä½ç½®ï¼Œè¿”å›ä¸Šä¸‹æ–‡
        index = content.find(query)
        if index == -1:
            # å¦‚æœæ²¡æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œè¿”å›å‰ context_length å­—ç¬¦
            return content[:context_length] + ("..." if len(content) > context_length else "")

        start = max(0, index - context_length // 2)
        end = min(len(content), index + len(query) + context_length // 2)

        snippet = content[start:end]

        # æ·»åŠ çœç•¥å·
        if start > 0:
            snippet = "..." + snippet
        if end < len(content):
            snippet = snippet + "..."

        return snippet


    async def vector_search_for_rag(self, query: str, top_k: int = 6) -> List[Dict[str, Any]]:
        """
        å‘é‡è¯­ä¹‰æœç´¢ï¼ˆRAG ä¸“ç”¨ï¼‰
        ä½¿ç”¨æœ¬åœ° embedding æœåŠ¡å°†æŸ¥è¯¢è½¬ä¸ºå‘é‡ï¼Œç„¶ååœ¨ MongoDB ä¸­è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        """
        from app.services import embedding_client
        import numpy as np
        
        if not query or not query.strip():
            return []
        
        # 1. æ£€æŸ¥å‘é‡æœåŠ¡æ˜¯å¦å¯ç”¨
        if not await embedding_client.check_health():
            print("[LawService] âš ï¸ å‘é‡æœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡å‘é‡æœç´¢")
            return []
        
        # 2. è·å–æŸ¥è¯¢å‘é‡
        query_embedding = await embedding_client.get_embedding(query)
        if not query_embedding:
            print("[LawService] âš ï¸ è·å–æŸ¥è¯¢å‘é‡å¤±è´¥")
            return []
        
        print(f"[LawService] ğŸ” å‘é‡æœç´¢: '{query}' (å‘é‡ç»´åº¦: {len(query_embedding)})")
        
        # 3. ä» MongoDB è·å–æ‰€æœ‰æœ‰å‘é‡çš„æ¡æ–‡
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨å†…å­˜è®¡ç®—ç›¸ä¼¼åº¦ï¼Œé€‚åˆä¸­å°è§„æ¨¡æ•°æ®ï¼ˆ<10ä¸‡æ¡ï¼‰
        # å¦‚æœæ•°æ®é‡å¾ˆå¤§ï¼Œåº”ä½¿ç”¨ä¸“é—¨çš„å‘é‡æ•°æ®åº“
        cursor = self.articles_collection.find(
            {"embedding": {"$exists": True}},
            {"_id": 1, "law_id": 1, "article_num": 1, "article_display": 1, "content": 1, "embedding": 1}
        )
        
        articles = await cursor.to_list(length=10000)  # é™åˆ¶æœ€å¤šå¤„ç† 1 ä¸‡æ¡
        
        if not articles:
            print("[LawService] âš ï¸ æ²¡æœ‰å·²å‘é‡åŒ–çš„æ¡æ–‡")
            return []
        
        # 4. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        query_vec = np.array(query_embedding)
        
        scored_articles = []
        for article in articles:
            article_vec = np.array(article.get("embedding", []))
            if len(article_vec) == 0:
                continue
            
            # ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = np.dot(query_vec, article_vec) / (np.linalg.norm(query_vec) * np.linalg.norm(article_vec))
            scored_articles.append((similarity, article))
        
        # 5. æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå– top_k
        scored_articles.sort(key=lambda x: x[0], reverse=True)
        top_articles = scored_articles[:top_k]
        
        # 6. æ‰¹é‡å…³è”æ³•å¾‹ä¿¡æ¯ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼šé¿å… N+1 æŸ¥è¯¢ï¼‰
        law_ids_set = list({article["law_id"] for _, article in top_articles})
        law_docs = await self.laws_collection.find(
            {"law_id": {"$in": law_ids_set}},
            {"law_id": 1, "title": 1, "category": 1}
        ).to_list(length=len(law_ids_set))
        law_map = {law["law_id"]: law for law in law_docs}
        
        results = []
        for similarity, article in top_articles:
            law = law_map.get(article["law_id"], {})
            
            result = {
                "_id": str(article["_id"]),
                "law_id": article["law_id"],
                "law_title": law.get("title", ""),
                "law_category": law.get("category", ""),
                "article_num": article.get("article_num"),
                "article_display": article.get("article_display", ""),
                "content": article.get("content", ""),
                "similarity": float(similarity),
            }
            results.append(result)
        
        print(f"[LawService] âœ… å‘é‡æœç´¢å®Œæˆï¼Œè¿”å› {len(results)} æ¡ç»“æœ")
        if results:
            print(f"[LawService]    æœ€é«˜ç›¸ä¼¼åº¦: {results[0]['similarity']:.4f} - {results[0]['law_title']} {results[0]['article_display']}")
            print(f"[LawService]    æœ€ä½ç›¸ä¼¼åº¦: {results[-1]['similarity']:.4f} - {results[-1]['law_title']} {results[-1]['article_display']}")
        
        return results

    async def search_for_rag(self, query: str, top_k: int = 6) -> List[Dict[str, Any]]:
        """
        çŸ¥è¯†åº“æ£€ç´¢ï¼ˆRAG ä¸“ç”¨ï¼‰ï¼šä¼˜å…ˆè§„åˆ™å¬å›ä¸æœç´¢å¼•æ“ï¼Œå¿…è¦æ—¶é™çº§ä¸ºæ–‡æœ¬ç´¢å¼•/æ­£åˆ™ã€‚
        """
        if not query or not query.strip():
            return []

        article_num, sub_index = self.parse_article_input(query)
        if article_num:
            article_result = await self._search_by_law_article(query, article_num, 1, top_k, sub_index)
            if article_result and article_result.get("data"):
                return article_result["data"]

        search_engine = get_search_engine()
        if search_engine.enabled:
            try:
                engine_result = await search_engine.search_articles(query, 1, top_k)
                if engine_result and engine_result.get("data"):
                    return engine_result["data"]
            except Exception:
                pass

        pipeline = [
            {"$match": {"$text": {"$search": query}}},
            {"$addFields": {"score": {"$meta": "textScore"}}},
            {"$sort": {"score": -1, "article_num": 1}},
            {"$limit": top_k},
            {
                "$lookup": {
                    "from": "laws",
                    "localField": "law_id",
                    "foreignField": "law_id",
                    "as": "law_info",
                }
            },
            {"$unwind": {"path": "$law_info", "preserveNullAndEmptyArrays": True}},
            {
                "$project": {
                    "_id": 1,
                    "law_id": 1,
                    "law_title": "$law_info.title",
                    "law_category": "$law_info.category",
                    "article_num": 1,
                    "article_display": 1,
                    "content": 1,
                    "score": 1,
                }
            },
        ]

        results = await self.articles_collection.aggregate(pipeline).to_list(length=top_k)
        if results:
            for result in results:
                result["_id"] = str(result["_id"])
                result["highlight"] = self._generate_highlight(result.get("content", ""), query)
            return results

        regex_query = {"content": {"$regex": query, "$options": "i"}}
        cursor = self.articles_collection.find(regex_query).sort("article_num", 1).limit(top_k)
        articles = await cursor.to_list(length=top_k)
        
        # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ‹†åˆ†å…³é”®è¯å•ç‹¬æœç´¢
        if not articles and len(query) > 2:
            # å°è¯•ç”¨æŸ¥è¯¢ä¸­çš„å…³é”®è¯ï¼ˆå»æ‰å¸¸è§åç¼€å¦‚"å¤„ç½š""è§„å®š""æ¡æ¬¾"ç­‰ï¼‰
            import re
            clean_query = re.sub(r'(å¤„ç½š|è§„å®š|æ¡æ¬¾|æ³•å¾‹|æ³•è§„|å¦‚ä½•|æ€ä¹ˆ|ä»€ä¹ˆ|ç›¸å…³)$', '', query).strip()
            if clean_query and clean_query != query:
                regex_query = {"content": {"$regex": clean_query, "$options": "i"}}
                cursor = self.articles_collection.find(regex_query).sort("article_num", 1).limit(top_k)
                articles = await cursor.to_list(length=top_k)
        
        if not articles:
            return []

        law_ids = list({article.get("law_id") for article in articles if article.get("law_id")})
        laws = await self.laws_collection.find(
            {"law_id": {"$in": law_ids}},
            {"law_id": 1, "title": 1, "category": 1},
        ).to_list(length=len(law_ids))
        law_map = {law["law_id"]: law for law in laws}

        output = []
        for article in articles:
            article["_id"] = str(article["_id"])
            law_info = law_map.get(article.get("law_id"), {})
            output.append({
                "law_id": article.get("law_id"),
                "law_title": law_info.get("title", ""),
                "law_category": law_info.get("category", ""),
                "article_num": article.get("article_num"),
                "article_display": article.get("article_display", ""),
                "content": article.get("content", ""),
                "highlight": self._generate_highlight(article.get("content", ""), query),
                "score": None,
            })

        return output

    async def get_categories(self) -> List[str]:
        """è·å–æ‰€æœ‰æ³•è§„åˆ†ç±»ï¼ˆåªè¿”å›æ•°æ®åº“ä¸­å®é™…å­˜åœ¨çš„åˆ†ç±»ï¼‰"""
        categories = await self.laws_collection.distinct("category")
        # è¿‡æ»¤ç©ºå€¼å¹¶æ’åº
        return sorted([c for c in categories if c])

    async def get_levels(self) -> List[str]:
        """
        è·å–æ‰€æœ‰æ•ˆåŠ›å±‚çº§
        """
        levels = await self.laws_collection.distinct("level")
        return sorted(levels)

    async def record_view(self, law_id: str) -> bool:
        """è®°å½•ä¸€æ¬¡æ³•è§„æµè§ˆ"""
        doc = {
            "law_id": law_id,
            "viewed_at": datetime.utcnow()
        }
        await self.view_logs_collection.insert_one(doc)
        return True

    async def get_today_views(self) -> int:
        """è·å–ä»Šæ—¥æµè§ˆæ€»æ•°"""
        # è·å–ä»Šå¤© UTC 0ç‚¹
        today = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)
        count = await self.view_logs_collection.count_documents({
            "viewed_at": {"$gte": today}
        })
        return count

    async def get_total_views(self) -> int:
        """Get total view count."""
        return await self.view_logs_collection.count_documents({})
