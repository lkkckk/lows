"""
æ³•è§„ç›¸å…³ä¸šåŠ¡é€»è¾‘
"""
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta
from app.models import LawCreate
from motor.motor_asyncio import AsyncIOMotorDatabase
from pathlib import Path
import json
from app.services.search_engine import get_search_engine
import hashlib
import re
import math


# æ³•å¾‹æƒé‡é…ç½®ï¼ˆæƒé‡è¶Šå¤§æ’åºè¶Šé å‰ï¼‰
LAW_WEIGHT_CONFIG = {
    "æ²»å®‰ç®¡ç†å¤„ç½šæ³•": 100,
    "ä¸­åäººæ°‘å…±å’Œå›½æ²»å®‰ç®¡ç†å¤„ç½šæ³•": 100,
    "åˆ‘æ³•": 95,
    "ä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•": 95,
    "åˆ‘äº‹è¯‰è®¼æ³•": 90,
    "ä¸­åäººæ°‘å…±å’Œå›½åˆ‘äº‹è¯‰è®¼æ³•": 90,
    "å…¬å®‰æœºå…³åŠç†è¡Œæ”¿æ¡ˆä»¶ç¨‹åºè§„å®š": 85,
    "å…¬å®‰æœºå…³åŠç†åˆ‘äº‹æ¡ˆä»¶ç¨‹åºè§„å®š": 80,
}
DEFAULT_WEIGHT = 50


COMMON_LAW_PREFIXES = [
    "ä¸­åäººæ°‘å…±å’Œå›½",
    "ä¸­åäººæ°‘",
    "å…¨å›½äººæ°‘ä»£è¡¨å¤§ä¼š",
    "æœ€é«˜äººæ°‘æ³•é™¢",
    "æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢",
]

_LAW_ALIAS_MAP: Optional[Dict[str, str]] = None


def _normalize_law_name(keyword: str) -> str:
    if not keyword:
        return ""

    cleaned = re.sub(r"[^\u4e00-\u9fa5A-Za-z0-9]+", "", keyword)
    if not cleaned:
        return ""

    for prefix in COMMON_LAW_PREFIXES:
        if cleaned.startswith(prefix):
            cleaned = cleaned[len(prefix):]
            break

    return cleaned


def _load_law_alias_map() -> Dict[str, str]:
    global _LAW_ALIAS_MAP
    if _LAW_ALIAS_MAP is not None:
        return _LAW_ALIAS_MAP

    alias_map: Dict[str, str] = {}
    alias_path = Path(__file__).resolve().parents[1] / "data" / "law_aliases.json"
    if alias_path.exists():
        try:
            data = json.loads(alias_path.read_text(encoding="utf-8"))
            if isinstance(data, dict):
                for canonical, aliases in data.items():
                    norm_key = _normalize_law_name(canonical)
                    if norm_key:
                        alias_map[norm_key] = canonical
                    if isinstance(aliases, list):
                        for alias in aliases:
                            norm_alias = _normalize_law_name(alias)
                            if norm_alias:
                                alias_map[norm_alias] = canonical
        except Exception:
            alias_map = {}

    _LAW_ALIAS_MAP = alias_map
    return alias_map


def _resolve_law_alias(keyword: str) -> str:
    normalized = _normalize_law_name(keyword)
    if not normalized:
        return ""
    return _load_law_alias_map().get(normalized, normalized)


def get_law_weight(title: str) -> int:
    """æ ¹æ®æ³•å¾‹æ ‡é¢˜è·å–æƒé‡"""
    # ç²¾ç¡®åŒ¹é…
    if title in LAW_WEIGHT_CONFIG:
        return LAW_WEIGHT_CONFIG[title]
    # æ¨¡ç³ŠåŒ¹é…ï¼ˆåŒ…å«å…³é”®è¯ï¼‰
    for key, weight in LAW_WEIGHT_CONFIG.items():
        if key in title:
            return weight
    return DEFAULT_WEIGHT


class LawService:
    """æ³•è§„æœåŠ¡ç±»"""

    def __init__(self, db: AsyncIOMotorDatabase):
        self.db = db
        self.laws_collection = db.laws
        self.articles_collection = db.law_articles
        self.view_logs_collection = db.view_logs

    async def create_law(self, law_in: LawCreate) -> Dict[str, Any]:
        """åˆ›å»ºæ³•è§„ï¼ˆåŒ…å«æ¡æ–‡ï¼‰"""
        # 1. ç”Ÿæˆ ID
        law_id = hashlib.md5(law_in.title.encode()).hexdigest()[:16]
        
        # 2. å‡†å¤‡æ³•è§„ä¸»æ¡£æ•°æ®
        law_data = law_in.dict(exclude={"articles"})
        law_data["law_id"] = law_id
        # ä¸ºäº†é¿å…å…¨æ–‡è¿‡å¤§ï¼Œè¿™é‡Œå¯ä»¥ç®€å•æ‹¼æ¥å‰å‡ æ¡ä½œä¸º full_text, æˆ–è€…å­˜ç©ºå­—ç¬¦ä¸²
        # æ›´å¥½çš„åšæ³•æ˜¯æŠŠæ‰€æœ‰ content æ‹¼èµ·æ¥
        full_text = "\n".join([a["content"] for a in law_in.articles])
        law_data["full_text"] = full_text
        
        # upsert æ³•è§„
        await self.laws_collection.replace_one(
            {"law_id": law_id}, law_data, upsert=True
        )
        
        # 3. å‡†å¤‡æ¡æ–‡æ•°æ®
        # å…ˆåˆ é™¤æ—§æ¡æ–‡
        await self.articles_collection.delete_many({"law_id": law_id})
        
        article_docs = []
        for art in law_in.articles:
            art_doc = {
                "law_id": law_id,
                "article_num": art["article_num"],
                "article_display": art["article_display"],
                "content": art["content"],
                "chapter": art.get("chapter", ""),
                "section": art.get("section", ""),
                "keywords": [] # TODO: æå–å…³é”®è¯
            }
            article_docs.append(art_doc)
            
        if article_docs:
            await self.articles_collection.insert_many(article_docs)
            
        return {"law_id": law_id, "message": f"æˆåŠŸå¯¼å…¥ {len(article_docs)} æ¡æ¡æ–‡"}

    async def get_laws_list(
        self,
        page: int = 1,
        page_size: int = 20,
        category: Optional[str] = None,
        level: Optional[str] = None,
        status: Optional[str] = None,
        tags: Optional[List[str]] = None,
        title: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        è·å–æ³•è§„åˆ—è¡¨ï¼ˆåˆ†é¡µ + ç­›é€‰ï¼‰
        """
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        query = {}
        if category:
            query["category"] = category
        if level:
            query["level"] = level
        if status:
            query["status"] = status
        if tags:
            query["tags"] = {"$in": tags}
        if title:
            query["title"] = {"$regex": title, "$options": "i"}

        # è®¡ç®—æ€»æ•°
        total = await self.laws_collection.count_documents(query)

        # è®¡ç®—åˆ†é¡µ
        skip = (page - 1) * page_size
        total_pages = math.ceil(total / page_size)

        # æŸ¥è¯¢æ•°æ®ï¼ˆæ’é™¤å…¨æ–‡å­—æ®µä»¥æé«˜æ€§èƒ½ï¼‰
        cursor = self.laws_collection.find(
            query, {"full_text": 0}
        ).sort("effect_date", -1).skip(skip).limit(page_size)

        laws = await cursor.to_list(length=page_size)

        # è½¬æ¢ ObjectId ä¸ºå­—ç¬¦ä¸²
        for law in laws:
            law["_id"] = str(law["_id"])

        return {
            "data": laws,
            "pagination": {
                "total": total,
                "page": page,
                "page_size": page_size,
                "total_pages": total_pages,
            }
        }

    async def get_law_detail(self, law_id: str) -> Optional[Dict[str, Any]]:
        """
        è·å–æ³•è§„è¯¦æƒ…ï¼ˆåŒ…å«å…¨æ–‡ï¼‰
        """
        law = await self.laws_collection.find_one({"law_id": law_id})
        if law:
            law["_id"] = str(law["_id"])
        return law

    async def update_law(self, law_id: str, update_data: Dict[str, Any]) -> bool:
        """
        æ›´æ–°æ³•è§„ä¿¡æ¯
        """
        # åªå…è®¸æ›´æ–°ç‰¹å®šå­—æ®µ
        allowed_fields = {"status", "category", "level", "issue_org", "issue_date", "effect_date", "expire_date", "summary"}
        filtered_data = {k: v for k, v in update_data.items() if k in allowed_fields}
        
        if not filtered_data:
            return False
            
        result = await self.laws_collection.update_one(
            {"law_id": law_id},
            {"$set": filtered_data}
        )
        return result.matched_count > 0

    async def delete_law(self, law_id: str) -> bool:
        """
        åˆ é™¤æ³•è§„åŠå…¶æ‰€æœ‰æ¡æ–‡
        """
        # å…ˆæ£€æŸ¥æ³•è§„æ˜¯å¦å­˜åœ¨
        law = await self.laws_collection.find_one({"law_id": law_id})
        if not law:
            return False
            
        # åˆ é™¤æ‰€æœ‰å…³è”æ¡æ–‡
        await self.articles_collection.delete_many({"law_id": law_id})
        # åˆ é™¤æ³•è§„ä¸»è®°å½•
        await self.laws_collection.delete_one({"law_id": law_id})
        return True

    async def get_law_articles(
        self, law_id: str, chapter: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        è·å–æ³•è§„çš„æ‰€æœ‰æ¡æ–‡
        """
        query = {"law_id": law_id}
        if chapter:
            query["chapter"] = chapter

        cursor = self.articles_collection.find(query).sort("article_num", 1)
        articles = await cursor.to_list(length=None)

        for article in articles:
            article["_id"] = str(article["_id"])

        return articles

    async def get_article_by_number(
        self, law_id: str, article_num: int
    ) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®æ¡å·è·å–æ¡æ–‡
        é€šè¿‡å°†é˜¿æ‹‰ä¼¯æ•°å­—è½¬æ¢ä¸ºä¸­æ–‡ï¼ŒåŒ¹é… article_display å­—æ®µ
        """
        # å°†é˜¿æ‹‰ä¼¯æ•°å­—è½¬æ¢ä¸ºä¸­æ–‡æ•°å­—
        chinese_num = self._arabic_to_chinese(article_num)
        display_pattern = f"^ç¬¬{chinese_num}æ¡"
        
        article = await self.articles_collection.find_one({
            "law_id": law_id,
            "article_display": {"$regex": display_pattern}
        })
        if article:
            article["_id"] = str(article["_id"])
        return article
    
    def _arabic_to_chinese(self, num: int, full_form: bool = False) -> str:
        """
        é˜¿æ‹‰ä¼¯æ•°å­—è½¬ä¸­æ–‡æ•°å­—ï¼ˆæ”¯æŒ1-999ï¼‰
        full_form: å½“ä¸ºTrueæ—¶ï¼Œ10-19ä¼šè½¬æ¢ä¸º"ä¸€å"è€Œä¸æ˜¯"å"
        """
        if num <= 0:
            return "é›¶"
        
        digits = ["é›¶", "ä¸€", "äºŒ", "ä¸‰", "å››", "äº”", "å…­", "ä¸ƒ", "å…«", "ä¹"]
        
        if num < 10:
            return digits[num]
        elif num < 20:
            # ååˆ°åä¹
            # å½“ full_form=True æˆ–å•ç‹¬ä½¿ç”¨æ—¶ï¼Œ10="ä¸€å"ï¼Œ11="ä¸€åä¸€"
            # å½“ full_form=Falseï¼ˆé»˜è®¤ï¼Œç”¨äºæ¡æ–‡é¦–å­—ï¼‰ï¼Œ10="å"ï¼Œ11="åä¸€"
            prefix = "ä¸€" if full_form else ""
            return prefix + "å" + (digits[num % 10] if num % 10 != 0 else "")
        elif num < 100:
            # äºŒååˆ°ä¹åä¹
            tens = num // 10
            ones = num % 10
            result = digits[tens] + "å"
            if ones > 0:
                result += digits[ones]
            return result
        elif num < 1000:
            hundreds = num // 100
            remainder = num % 100
            result = digits[hundreds] + "ç™¾"
            if remainder >= 10:
                # ç™¾ä½åçš„10-19éœ€è¦å®Œæ•´å½¢å¼ï¼Œå¦‚110="ä¸€ç™¾ä¸€å"
                result += self._arabic_to_chinese(remainder, full_form=True)
            elif remainder > 0:
                result += "é›¶" + digits[remainder]
            return result
        else:
            return str(num)  # è¶…è¿‡999ç›´æ¥è¿”å›æ•°å­—

    def parse_article_input(self, input_str: str) -> Optional[int]:
        """
        è§£ææ¡å·è¾“å…¥ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼š
        - "ç¬¬å…«åä¸‰æ¡" â†’ 83
        - "83æ¡" â†’ 83
        - "83" â†’ 83
        """
        # ä¸­æ–‡æ•°å­—æ˜ å°„
        chinese_num_map = {
            'é›¶': 0, 'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4,
            'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9,
            'å': 10, 'ç™¾': 100, 'åƒ': 1000
        }

        # åŒ¹é…æ¨¡å¼1: ç¬¬XXæ¡
        pattern1 = r'ç¬¬([é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+)æ¡'
        match = re.search(pattern1, input_str)
        if match:
            chinese_num = match.group(1)
            return self._chinese_to_arabic(chinese_num, chinese_num_map)

        # åŒ¹é…æ¨¡å¼2: XXæ¡
        pattern2 = r'(\d+)æ¡'
        match = re.search(pattern2, input_str)
        if match:
            return int(match.group(1))

        # åŒ¹é…æ¨¡å¼3: çº¯æ•°å­—
        pattern3 = r'^\d+$'
        if re.match(pattern3, input_str.strip()):
            return int(input_str.strip())

        return None

    def _extract_law_keyword(self, query: str) -> str:
        """
        Extract law name part from a query that includes an article number.
        """
        if not query:
            return ""

        cn_nums = "\u96f6\u4e00\u4e8c\u4e09\u56db\u4e94\u516d\u4e03\u516b\u4e5d\u5341\u767e\u5343"
        text = re.sub(rf"\u7b2c?[{cn_nums}]+\u6761", " ", query)
        text = re.sub(r"\u7b2c?\d+\u6761", " ", text)

        return text.strip()

    def _normalize_law_keyword(self, keyword: str) -> str:
        """
        Normalize law keyword for fuzzy title matching.
        """
        return _normalize_law_name(keyword)

    def _build_title_fuzzy_regex(self, keyword: str) -> Optional[str]:
        """
        Build a fuzzy regex that requires all tokens to appear in the title.
        """
        if not keyword or len(keyword) <= 1:
            return None

        if len(keyword) <= 4:
            tokens = list(keyword)
        else:
            tokens = [keyword[i:i + 2] for i in range(len(keyword) - 1)]

        tokens = [t for t in tokens if t.strip()]
        if not tokens:
            return None

        lookaheads = "".join(f"(?=.*{re.escape(t)})" for t in tokens)
        return f"{lookaheads}.*"

    async def _find_laws_by_keyword(self, keyword: str) -> List[Dict[str, Any]]:
        """
        Find candidate laws by keyword using exact-substring, then fuzzy match.
        """
        if not keyword:
            return []

        exact = await self.laws_collection.find(
            {"title": {"$regex": re.escape(keyword), "$options": "i"}},
            {"full_text": 0},
        ).to_list(length=20)
        if exact:
            return exact

        fuzzy_regex = self._build_title_fuzzy_regex(keyword)
        if not fuzzy_regex:
            return []

        return await self.laws_collection.find(
            {"title": {"$regex": fuzzy_regex, "$options": "i"}},
            {"full_text": 0},
        ).to_list(length=20)

    async def _search_by_law_article(
        self, query: str, article_num: int, page: int, page_size: int
    ) -> Optional[Dict[str, Any]]:
        """
        Search by law name + article number pattern.
        """
        raw_keyword = self._extract_law_keyword(query)
        if not raw_keyword:
            return None

        keywords = []
        resolved = _resolve_law_alias(raw_keyword)
        if resolved:
            keywords.append(resolved)
        normalized = _normalize_law_name(raw_keyword)
        if normalized and normalized not in keywords:
            keywords.append(normalized)

        laws = []
        for keyword in keywords:
            laws = await self._find_laws_by_keyword(keyword)
            if laws:
                break
        if not laws:
            return None

        law_map = {law["law_id"]: law for law in laws}
        law_ids = list(law_map.keys())

        search_query = {"law_id": {"$in": law_ids}, "article_num": article_num}
        total = await self.articles_collection.count_documents(search_query)
        total_pages = math.ceil(total / page_size) if total > 0 else 0
        skip = (page - 1) * page_size

        cursor = self.articles_collection.find(search_query).skip(skip).limit(page_size)
        articles = await cursor.to_list(length=page_size)

        results = []
        for article in articles:
            article["_id"] = str(article["_id"])
            law_info = law_map.get(article["law_id"], {})
            results.append({
                "law_id": article["law_id"],
                "law_title": law_info.get("title", ""),
                "law_category": law_info.get("category", ""),
                "article_num": article.get("article_num"),
                "article_display": article.get("article_display", ""),
                "content": article.get("content", ""),
                "highlight": self._generate_highlight(
                    article.get("content", ""), normalized
                ),
            })

        return {
            "data": results,
            "pagination": {
                "total": total,
                "page": page,
                "page_size": page_size,
                "total_pages": total_pages,
            }
        }

    def _chinese_to_arabic(self, chinese_num: str, num_map: Dict[str, int]) -> int:
        """
        ä¸­æ–‡æ•°å­—è½¬é˜¿æ‹‰ä¼¯æ•°å­—
        æ”¯æŒï¼šä¸€ã€åã€åä¸€ã€äºŒåã€äºŒåä¸‰ã€ä¸€ç™¾ã€ä¸€ç™¾é›¶ä¸‰ã€ä¸€åƒç­‰
        """
        if not chinese_num:
            return 0

        result = 0
        current = 0

        for char in chinese_num:
            num = num_map.get(char, 0)
            if num >= 10:  # å•ä½ï¼ˆåã€ç™¾ã€åƒï¼‰
                if current == 0:
                    current = 1
                result += current * num
                current = 0
            else:
                current = num

        return result + current

    async def search_in_law(
        self, law_id: str, query: str, page: int = 1, page_size: int = 20
    ) -> Dict[str, Any]:
        """
        åœ¨å•ä¸ªæ³•è§„å†…æœç´¢ - æ”¯æŒæ¡å·å’Œå…³é”®å­—
        
        æœç´¢ç­–ç•¥ï¼š
        1. é¦–å…ˆå°è¯•è§£æä¸ºæ¡å·ï¼ˆå¦‚"ç¬¬å…«åä¸‰æ¡"ã€"83"ï¼‰
        2. å¦‚æœä¸æ˜¯æ¡å·ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æœç´¢
        """
        # å…ˆå°è¯•è§£æä¸ºæ¡å·
        article_num = self.parse_article_input(query)

        if article_num:
            # æŒ‰æ¡å·ç²¾å‡†æŸ¥è¯¢
            article = await self.get_article_by_number(law_id, article_num)
            if article:
                return {
                    "data": [article],
                    "pagination": {
                        "total": 1,
                        "page": 1,
                        "page_size": page_size,
                        "total_pages": 1,
                    }
                }

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æœç´¢ï¼ˆç¡®ä¿å‡†ç¡®æ€§ï¼‰
        search_query = {
            "law_id": law_id,
            "content": {"$regex": query, "$options": "i"}
        }
        total = await self.articles_collection.count_documents(search_query)
        
        # åˆ†é¡µå‚æ•°
        skip = (page - 1) * page_size
        total_pages = math.ceil(total / page_size) if total > 0 else 0
        
        # æŒ‰æ¡å·æ’åº
        cursor = self.articles_collection.find(search_query).sort("article_num", 1).skip(skip).limit(page_size)
        results = await cursor.to_list(length=page_size)

        for result in results:
            result["_id"] = str(result["_id"])
            result["highlight"] = self._generate_highlight(result["content"], query)

        return {
            "data": results,
            "pagination": {
                "total": total,
                "page": page,
                "page_size": page_size,
                "total_pages": total_pages,
            }
        }

    async def search_global(
        self, query: str, page: int = 1, page_size: int = 20
    ) -> Dict[str, Any]:
        """
        å…¨åº“æœç´¢ï¼ˆè·¨æ³•è§„ï¼‰- ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç¡®ä¿å‡†ç¡®æ€§
        
        æ³¨æ„ï¼šMongoDB æ–‡æœ¬ç´¢å¼•å¯¹ä¸­æ–‡æ”¯æŒä¸å®Œå–„ï¼Œä¼šæ¼æ‰å¤§é‡ç»“æœã€‚
        ä¸ºç¡®ä¿æœç´¢å‡†ç¡®æ€§ï¼ˆ100%ä¸æ¼ï¼‰ï¼Œç›´æ¥ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æœç´¢ã€‚
        
        æ’åºç­–ç•¥ï¼šæŒ‰æ³•å¾‹æƒé‡é™åº + æ¡å·å‡åº
        """
        import time
        start_time = time.time()

        article_num = self.parse_article_input(query)
        if article_num:
            article_result = await self._search_by_law_article(query, article_num, page, page_size)
            if article_result is not None:
                return article_result

        search_engine = get_search_engine()
        if search_engine.enabled:
            try:
                engine_result = await search_engine.search_articles(query, page, page_size)
                if engine_result is not None:
                    return engine_result
            except Exception:
                pass

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æœç´¢ï¼ˆç¡®ä¿å‡†ç¡®æ€§ï¼Œä»»ä½•å­å­—ç¬¦ä¸²éƒ½èƒ½è¢«æ‰¾åˆ°ï¼‰
        search_query = {"content": {"$regex": query, "$options": "i"}}
        total = await self.articles_collection.count_documents(search_query)
        
        # è·å–æ‰€æœ‰åŒ¹é…ç»“æœï¼ˆç”¨äºæƒé‡æ’åºï¼‰
        # æ³¨æ„ï¼šå¯¹äºå¤§é‡æ•°æ®ï¼Œè¿™ç§æ–¹å¼å¯èƒ½æ¯”è¾ƒæ…¢ï¼Œä½†èƒ½ä¿è¯æ’åºå‡†ç¡®
        pipeline = [
            {"$match": search_query},
            {
                "$lookup": {
                    "from": "laws",
                    "localField": "law_id",
                    "foreignField": "law_id",
                    "as": "law_info",
                }
            },
            {"$unwind": "$law_info"},
            {
                "$project": {
                    "_id": 1,
                    "law_id": 1,
                    "law_title": "$law_info.title",
                    "law_category": "$law_info.category",
                    "article_num": 1,
                    "article_display": 1,
                    "content": 1,
                }
            }
        ]

        results = await self.articles_collection.aggregate(pipeline).to_list(length=None)
        
        # æŒ‰æƒé‡æ’åºï¼ˆæƒé‡é™åºï¼ŒåŒæƒé‡æŒ‰æ¡å·å‡åºï¼‰
        results.sort(key=lambda x: (-get_law_weight(x.get("law_title", "")), x.get("article_num", 0)))
        
        # åˆ†é¡µ
        total_pages = math.ceil(total / page_size) if total > 0 else 0
        skip = (page - 1) * page_size
        paged_results = results[skip:skip + page_size]

        for result in paged_results:
            result["_id"] = str(result["_id"])
            result["highlight"] = self._generate_highlight(result["content"], query)

        elapsed_time = time.time() - start_time
        print(f"ğŸ” æœç´¢å®Œæˆ: query=\"{query}\" | results={total} | time={elapsed_time:.3f}s")

        return {
            "data": paged_results,
            "pagination": {
                "total": total,
                "page": page,
                "page_size": page_size,
                "total_pages": total_pages,
            }
        }

    def _generate_highlight(self, content: str, query: str, context_length: int = 100) -> str:
        """
        ç”Ÿæˆé«˜äº®ç‰‡æ®µ
        """
        # ç®€å•å®ç°ï¼šæŸ¥æ‰¾å…³é”®å­—ç¬¬ä¸€æ¬¡å‡ºç°çš„ä½ç½®ï¼Œè¿”å›ä¸Šä¸‹æ–‡
        index = content.find(query)
        if index == -1:
            # å¦‚æœæ²¡æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œè¿”å›å‰ context_length å­—ç¬¦
            return content[:context_length] + ("..." if len(content) > context_length else "")

        start = max(0, index - context_length // 2)
        end = min(len(content), index + len(query) + context_length // 2)

        snippet = content[start:end]

        # æ·»åŠ çœç•¥å·
        if start > 0:
            snippet = "..." + snippet
        if end < len(content):
            snippet = snippet + "..."

        return snippet

    async def get_categories(self) -> List[str]:
        """è·å–æ‰€æœ‰æ³•è§„åˆ†ç±»ï¼ˆåªè¿”å›æ•°æ®åº“ä¸­å®é™…å­˜åœ¨çš„åˆ†ç±»ï¼‰"""
        categories = await self.laws_collection.distinct("category")
        # è¿‡æ»¤ç©ºå€¼å¹¶æ’åº
        return sorted([c for c in categories if c])

    async def get_levels(self) -> List[str]:
        """
        è·å–æ‰€æœ‰æ•ˆåŠ›å±‚çº§
        """
        levels = await self.laws_collection.distinct("level")
        return sorted(levels)

    async def record_view(self, law_id: str) -> bool:
        """è®°å½•ä¸€æ¬¡æ³•è§„æµè§ˆ"""
        doc = {
            "law_id": law_id,
            "viewed_at": datetime.utcnow()
        }
        await self.view_logs_collection.insert_one(doc)
        return True

    async def get_today_views(self) -> int:
        """è·å–ä»Šæ—¥æµè§ˆæ€»æ•°"""
        # è·å–ä»Šå¤© UTC 0ç‚¹
        today = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)
        count = await self.view_logs_collection.count_documents({
            "viewed_at": {"$gte": today}
        })
        return count

    async def get_total_views(self) -> int:
        """Get total view count."""
        return await self.view_logs_collection.count_documents({})
