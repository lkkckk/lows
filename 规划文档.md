# 法律 AI 知识库建设规划（内网离线 + 多 LLM）

## 1. 背景与目标

本项目运行在内网离线环境，LLM 为内网模型（如 DeepSeek、Qwen 等）。目前 AI 问法仅做模型对话，未接入法规检索，导致回答质量不稳定。本规划旨在引入“专门知识库 + RAG”，以法规条文为依据提升答案准确度，并支持多模型可切换。

目标：
- 在离线环境内构建“法规知识库 + 检索增强（RAG）”能力。
- 保持现有法规数据与检索路径可复用，增强中文检索效果。
- 支持多种内网 LLM 动态切换与降级策略。
- 可控延迟，检索毫秒级，整体响应主要由 LLM 生成决定。

非目标：
- 不进行模型训练与微调。
- 不依赖公网服务或外部 API。

## 2. 当前基础与可复用能力

- 数据来源：爬虫输出 JSONL → MongoDB（`crawler/*`，`crawler/import_data.py`）。
- 语料结构：`laws` 与 `law_articles` 集合（`README.md`）。
- 搜索能力：全库搜索 + OpenSearch/ES（可选）（`backend/app/services/law_service.py`，`backend/app/services/search_engine.py`）。
- AI 对话：仅调用内网 LLM，无检索增强（`backend/app/services/ai_service.py`，`backend/app/api/ai.py`）。

现有缺口：
- 无向量索引、无 embedding 生成、无 RAG 拼接。
- 关键词搜索存在正则全库扫描的性能风险。

## 3. 总体方案（关键词检索 + RAG）

采用“关键词检索 + 规则召回”的混合召回（内网无 embedding 服务），生成回答时附带法名与条号引用。

### 3.1 数据流

1) 采集与导入：爬虫 → JSONL → MongoDB（既有流程）。
2) 索引构建：
   - 关键词索引：复用 OpenSearch/ES 或 Mongo 索引（已有）。
   - 规则索引：维护法条别名、法名同义词、条号解析规则（已有部分能力）。
3) 在线检索：
   - 先解析条号/法名（规则召回），再做全文关键词检索。
   - 合并去重并按权重/相关性排序。
4) RAG 生成：
   - 选 TopK 条文拼接上下文，明确引用（法名 + 条号）。
   - LLM 回答优先基于检索证据，无法确定时提示“需核对原文”。

### 3.2 检索策略

- 关键词检索：优先走 OpenSearch/ES（已有 `search_engine`），避免 Mongo 正则全库扫描。
- 规则召回：条号解析 + 法名别名匹配（`law_aliases.json`）。
- 排序策略：
  - 关键词相关性 + `get_law_weight`（权重优先常用法） + 条号排序。

## 4. 组件改造规划

### 4.1 后端服务层

新增：`backend/app/services/knowledge_base_service.py`
- 负责统一检索（规则 + 关键词）并生成上下文。
- 提供 `retrieve(query, top_k)` 方法。

改造：`backend/app/services/ai_service.py`
- 接入 RAG：调用 knowledge_base_service 获取上下文。
- 支持多模型配置与切换逻辑。
- 支持“无证据不乱答”的提示策略。

改造：`backend/app/api/ai.py`
- 请求可带 `use_rag`/`provider`/`model_name`。
- 返回可包含引用来源（法名 + 条号）。

### 4.2 搜索引擎与索引构建

改造：`backend/scripts/search_engine_reindex.py`
- 保持纯文本索引，加入字段权重与分析器配置优化（如 IK 分词）。
- 可选加入同义词词典与拼音搜索配置（完全离线）。

### 4.3 配置与运维

配置来源：MongoDB `settings`（已有基础配置逻辑）
- 新增 `providers`（多模型配置列表）。
- 新增 `provider_order`（优先级与降级链路）。
- 新增 `rag_enabled`/`rag_top_k`（检索开关与 TopK）。

离线运维流程：
- 数据导入后执行索引重建脚本。
- 保留全量重建与增量更新两种方式。

## 5. 多 LLM 切换设计

需求：在内网环境中可切换 DeepSeek、Qwen 等模型，并支持故障降级。

建议策略：
- `settings` 中维护 provider 列表：
  - provider_id、api_url、api_key、model_name、timeout、priority
- 请求允许指定 `provider_id` 或 `model_name`。
- 默认按优先级自动选择，失败时降级到备选。

## 6. 性能与稳定性

- 优先使用 OpenSearch/ES，避免 Mongo 正则全库扫描。
- TopK 控制与上下文截断，减少 LLM 生成时间。
- 可选：查询缓存（同一问题短时重复调用）。

## 7. 安全与合规

- 内网闭环：所有服务只对内网开放，无外部依赖。
- 访问限制：复用 IP 白名单（已存在）。
- 审计日志：记录检索命中文档与模型使用量（可扩展）。

## 8. 交付与验收标准

- AI 问法返回包含引用（法名 + 条号）。
- 内网多模型可切换，故障可自动降级。
- 检索延迟可控（毫秒级），总体响应明显优于纯 LLM。
- 不依赖外网服务，部署与运维符合内网规范。

## 9. 实施阶段划分

阶段 1：检索增强最小可用（关键词 + 规则召回 + RAG）
- 新增知识库服务层，接入现有检索能力。
- AI 问法返回引用。

阶段 2：检索优化（OpenSearch 分词与同义词）
- 改造索引与重建脚本，提升中文检索效果。
- 调整相关性权重与排序。

阶段 3：多模型切换与降级
- provider 配置与策略完善。

## 10. 需要你确认的前提（不影响规划）

- 是否已部署 OpenSearch/ES（以及是否启用 IK 分词与同义词词典）。
- 需要支持的模型清单与优先级策略。

## 11. 未来可选升级（有 embedding 能力时）

若未来内网允许部署 embedding 模型，可追加“向量检索 + 混合召回”阶段：
- 构建 embedding 与 kNN 索引。
- 混合排序（关键词 + 向量相似度）。
