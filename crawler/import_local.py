import os
import re
import hashlib
import json
import logging
from pymongo import MongoClient, ASCENDING, TEXT
from dotenv import load_dotenv

# Word æ–‡æ¡£æ”¯æŒ
try:
    from docx import Document
    DOCX_SUPPORTED = True
except ImportError:
    DOCX_SUPPORTED = False
    logging.warning("python-docx æœªå®‰è£…ï¼Œ.docx æ–‡ä»¶å°†ä¸è¢«æ”¯æŒã€‚è¿è¡Œ: pip install python-docx")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(os.path.join(os.path.dirname(os.path.dirname(__file__)), 'backend', '.env'))

class LocalImporter:
    def __init__(self, input_dir="manual_data"):
        self.input_dir = input_dir
        # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå¦åˆ™ä½¿ç”¨ Docker æ˜ å°„çš„ç«¯å£
        self.mongo_uri = os.getenv("MONGODB_URL", "mongodb://localhost:27019")
        logging.info(f"ğŸ”— è¿æ¥ MongoDB: {self.mongo_uri}")
        self.client = MongoClient(self.mongo_uri, serverSelectionTimeoutMS=5000)
        self.db = self.client[os.getenv("MONGODB_DB", "law_system")]
        # æµ‹è¯•è¿æ¥
        try:
            self.client.admin.command('ping')
            logging.info(f"âœ… MongoDB è¿æ¥æˆåŠŸ")
        except Exception as e:
            logging.error(f"âŒ MongoDB è¿æ¥å¤±è´¥: {e}")
            raise
        self.setup_indexes()

    def setup_indexes(self):
        """ç¡®ä¿ç´¢å¼•å­˜åœ¨ï¼ˆå¿½ç•¥å·²å­˜åœ¨çš„ç´¢å¼•å†²çªï¼‰"""
        try:
            self.db.laws.create_index([("title", ASCENDING)], unique=True)
        except Exception:
            pass
        try:
            self.db.laws.create_index([("title", TEXT), ("summary", TEXT)], name="law_text_search")
        except Exception:
            pass
        try:
            self.db.law_articles.create_index([("law_id", ASCENDING), ("article_num", ASCENDING)], unique=True)
        except Exception:
            pass
        try:
            self.db.law_articles.create_index([("content", TEXT)], name="article_content_search")
        except Exception:
            pass

    def parse_metadata(self, content):
        """è§£æå…ƒæ•°æ®å’Œä¿®è®¢è¯´æ˜"""
        metadata = {
            "issue_date": "",
            "effect_date": "",
            "issue_org": "",  # å…ˆç•™ç©ºï¼Œç¨åå…œåº•
            "status": "ç°è¡Œæœ‰æ•ˆ",
            "category": "",
            "level": "",  # å…ˆç•™ç©ºï¼Œç¨åå…œåº•
            "summary": ""
        }
        
        # ===== æ–°å¢ï¼šæ’é™¤å…¬å‘Šé¡µå¹²æ‰° =====
        # å¦‚æœå†…å®¹å¼€å¤´åŒ…å«"å…¬å‘Š"å­—æ ·ï¼Œè·³è¿‡åˆ°æ­£å¼æ ‡é¢˜
        lines = [line.strip() for line in content.split('\n') if line.strip()]
        
        # æŸ¥æ‰¾å¹¶è·³è¿‡å…¬å‘Šéƒ¨åˆ†
        skip_until = 0
        for i, line in enumerate(lines[:30]):  # åªæ£€æŸ¥å‰30è¡Œ
            # åŒ¹é…"å…¬å‘Š"æˆ–"å…¬  å‘Š"æˆ–"å…¬    å‘Š"ç­‰ï¼ˆä»»æ„ç©ºæ ¼ï¼‰
            if re.match(r'^å…¬\s*å‘Š$', line):
                # æ‰¾åˆ°å…¬å‘Šï¼Œç»§ç»­å‘ä¸‹æ‰¾åˆ°æ—¥æœŸè¡Œï¼ˆå¦‚"2013å¹´4æœˆ2æ—¥"æˆ–"2012 å¹´ 12 æœˆ 12 æ—¥"ï¼‰ä½œä¸ºå…¬å¸ƒæ—¥æœŸ
                for j in range(i + 1, min(i + 15, len(lines))):
                    # æ”¯æŒå¸¦ç©ºæ ¼å’Œä¸å¸¦ç©ºæ ¼çš„æ—¥æœŸæ ¼å¼
                    date_match = re.match(r'^(\d{4})\s*å¹´\s*(\d{1,2})\s*æœˆ\s*(\d{1,2})\s*æ—¥$', lines[j])
                    if date_match:
                        # æå–å…¬å‘Šè½æ¬¾æ—¥æœŸä½œä¸ºå…¬å¸ƒæ—¥æœŸ
                        year, month, day = date_match.groups()
                        metadata["issue_date"] = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                        logging.info(f"   ğŸ“… ä»å…¬å‘Šæå–å…¬å¸ƒæ—¥æœŸ: {metadata['issue_date']}")
                        skip_until = j + 1
                        break
                break
        
        if skip_until > 0:
            lines = lines[skip_until:]
            logging.info(f"   ğŸ“‹ å·²è·³è¿‡å…¬å‘Šéƒ¨åˆ† ({skip_until} è¡Œ)")
        
        # ===== æ–°å¢ï¼šæ™ºèƒ½åˆå¹¶å¤šè¡Œæ ‡é¢˜ =====
        # å¸æ³•è§£é‡Šçš„æ ‡é¢˜å¯èƒ½åˆ†æˆå¤šè¡Œï¼Œéœ€è¦åˆå¹¶
        # åˆå¹¶è§„åˆ™ï¼šä»ç¬¬ä¸€è¡Œå¼€å§‹ï¼Œç›´åˆ°é‡åˆ°"æ³•é‡Š"/"æ³•å‘"æˆ–æ‹¬å·å¼€å¤´çš„è¡Œ
        title_lines = []
        for i, line in enumerate(lines[:15]):
            # è·³è¿‡æ¡ä»¶ï¼šæœºå…³åç§°è¡Œã€å…¬å‘Šè¡Œã€æ—¥æœŸè¡Œ
            if 'å…¬å‘Š' in line or 'å…¬  å‘Š' in line:
                continue
            if line.startswith('ä¸­åäººæ°‘å…±å’Œå›½'):
                continue
            if re.match(r'^\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥$', line):
                continue
            # åœæ­¢æ¡ä»¶ï¼šé‡åˆ°æ³•é‡Š/æ³•å‘ç¼–å·è¡Œï¼Œæˆ–è€…æ‹¬å·å¼€å¤´çš„ä¿®è®¢è¯´æ˜
            if re.match(r'^æ³•[é‡Šå‘][\[ã€”ã€ï¼ˆ(]', line) or \
               line.startswith('(') or line.startswith('ï¼ˆ'):
                break
            # åªä¿ç•™åŒ…å«"è§£é‡Š"/"è§„å®š"/"æ„è§"ç­‰å…³é”®è¯çš„è¡Œï¼Œæˆ–"å…³äº"å¼€å¤´çš„è¡Œ
            if 'è§£é‡Š' in line or 'è§„å®š' in line or 'æ„è§' in line or \
               'å…³äº' in line or 'è‹¥å¹²é—®é¢˜' in line:
                # å¦‚æœè¿™è¡ŒåŒ…å«"å·²äº"/"å·²ç»"ï¼Œè¯´æ˜è¿›å…¥äº†ä¼šè®®é€šè¿‡ä¿¡æ¯ï¼Œæˆªæ–­
                if 'å·²äº' in line or 'å·²ç»' in line:
                    # åªå–"å·²äº"ä¹‹å‰çš„éƒ¨åˆ†
                    idx = line.find('å·²äº')
                    if idx == -1:
                        idx = line.find('å·²ç»')
                    if idx > 0:
                        title_lines.append(line[:idx])
                    break
                title_lines.append(line)
        
        if title_lines:
            # åˆå¹¶æ ‡é¢˜å¹¶æ¸…ç†æ ¼å¼
            merged_title = ''.join(title_lines)
            # å°†å¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸ºä¸­æ–‡é¡¿å·
            merged_title = re.sub(r'\s{2,}', 'ã€', merged_title)
            # ç§»é™¤ã€Šã€‹ç¬¦å·
            merged_title = merged_title.replace('ã€Š', '').replace('ã€‹', '')
            
            # å¦‚æœæ ‡é¢˜ä»¥"å…³äº"å¼€å¤´ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ å‘å¸ƒæœºå…³å‰ç¼€
            if merged_title.startswith('å…³äº'):
                # ä»å‰é¢çš„è¡Œä¸­æŸ¥æ‰¾å‘å¸ƒæœºå…³
                issuer_prefix = ""
                for prev_line in lines[:10]:
                    if 'æœ€é«˜äººæ°‘æ³•é™¢' in prev_line and 'æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢' in prev_line:
                        issuer_prefix = "æœ€é«˜äººæ°‘æ³•é™¢ã€æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢"
                        break
                    elif 'æœ€é«˜äººæ°‘æ³•é™¢' in prev_line:
                        issuer_prefix = "æœ€é«˜äººæ°‘æ³•é™¢"
                    elif 'æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢' in prev_line:
                        issuer_prefix = "æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢"
                if issuer_prefix:
                    merged_title = issuer_prefix + merged_title
            
            # å­˜å‚¨åˆå¹¶åçš„æ ‡é¢˜
            metadata["merged_title"] = merged_title
            logging.info(f"   ğŸ“‹ åˆå¹¶æ ‡é¢˜: {merged_title}")

        
        # ===== å¸æ³•è§£é‡Šè¯†åˆ« =====
        full_text = '\n'.join(lines)
        is_judicial_interpretation = False
        
        # ç‰¹å¾1ï¼šæ ‡é¢˜æˆ–å†…å®¹åŒ…å«"è§£é‡Š"
        # ç‰¹å¾2ï¼šå‘å¸ƒæœºå…³åŒ…å«"æœ€é«˜äººæ°‘æ³•é™¢"æˆ–"æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢"
        for line in lines[:10]:
            if 'è§£é‡Š' in line or 'æœ€é«˜äººæ°‘æ³•é™¢' in line or 'æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢' in line:
                is_judicial_interpretation = True
                break
        
        if is_judicial_interpretation:
            metadata["level"] = "å¸æ³•è§£é‡Š"
            metadata["category"] = "å¸æ³•è§£é‡Š"
            # æ™ºèƒ½è¯†åˆ«å‘å¸ƒæœºå…³
            has_fayuan = 'æœ€é«˜äººæ°‘æ³•é™¢' in full_text[:500]
            has_jianchayuan = 'æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢' in full_text[:500]
            if has_fayuan and has_jianchayuan:
                metadata["issue_org"] = "æœ€é«˜äººæ°‘æ³•é™¢ã€æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢"
            elif has_fayuan:
                metadata["issue_org"] = "æœ€é«˜äººæ°‘æ³•é™¢"
            elif has_jianchayuan:
                metadata["issue_org"] = "æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢"
            logging.info(f"   âš–ï¸ è¯†åˆ«ä¸ºå¸æ³•è§£é‡Šï¼Œå‘å¸ƒæœºå…³: {metadata['issue_org']}")
        
        # ===== æ–°å¢ï¼šä»æ­£æ–‡æå–å®æ–½æ—¥æœŸ =====
        # åŒ¹é… "è‡ªXXXXå¹´XæœˆXæ—¥èµ·æ–½è¡Œ" æˆ– "è‡ªXXXXå¹´XæœˆXæ—¥èµ·å®æ–½"
        effect_date_pattern = re.compile(r'è‡ª(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥èµ·(?:æ–½è¡Œ|å®æ–½)')
        match = effect_date_pattern.search(full_text)
        if match:
            year, month, day = match.groups()
            metadata["effect_date"] = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
            logging.info(f"   ğŸ“… ä»æ­£æ–‡æå–å®æ–½æ—¥æœŸ: {metadata['effect_date']}")
        
        # 1. å°è¯•æå–ä¿®è®¢è¯´æ˜ï¼ˆåœ†æ‹¬å·åŒ…è£¹çš„é•¿æ®µè½ï¼‰
        # é€šå¸¸åœ¨æ ‡é¢˜ï¼ˆç¬¬ä¸€è¡Œï¼‰ä¹‹åï¼Œæ­£æ–‡ä¹‹å‰
        for i in range(1, min(10, len(lines))):
            line = lines[i]
            # ç‰¹å¾ï¼šä»¥(æˆ–ï¼ˆå¼€å¤´ï¼ŒåŒ…å«"é€šè¿‡"ã€"ä¿®æ­£"ã€"ä¿®è®¢"ç­‰å­—çœ¼
            if (line.startswith('(') or line.startswith('ï¼ˆ')) and \
               ('é€šè¿‡' in line or 'ä¿®æ­£' in line or 'ä¿®è®¢' in line):
                metadata['summary'] = line
                break
        
        # 2. ä»å‰20è¡Œæå–KVå…ƒæ•°æ®
        header_lines = lines[:20]
        for line in header_lines:
            if "å‘å¸ƒæ—¥æœŸ" in line or "å…¬å¸ƒæ—¥æœŸ" in line:
                metadata["issue_date"] = self._extract_value(line)
            if ("å®æ–½æ—¥æœŸ" in line or "æ–½è¡Œæ—¥æœŸ" in line) and not metadata["effect_date"]:
                metadata["effect_date"] = self._extract_value(line)
            if "å‘å¸ƒéƒ¨é—¨" in line or "å‘æ–‡æœºå…³" in line:
                if not metadata["issue_org"]:  # ä¸è¦†ç›–å·²è¯†åˆ«çš„å¸æ³•è§£é‡Šæœºå…³
                    metadata["issue_org"] = self._extract_value(line)
            if "æ•ˆåŠ›" in line and ("çº§åˆ«" in line or "ç­‰çº§" in line):
                if not metadata["level"]:  # ä¸è¦†ç›–å·²è¯†åˆ«çš„å¸æ³•è§£é‡Š
                    metadata["level"] = self._extract_value(line)
            if "ç±»åˆ«" in line:
                if not metadata["category"]:
                    metadata["category"] = self._extract_value(line)
        
        # 3. ä»æœ«å°¾è§£æç»“æ„åŒ–å…ƒæ•°æ®ï¼ˆWord æ–‡æ¡£å¸¸è§æ ¼å¼ï¼‰
        tail_lines = lines[-30:] if len(lines) > 30 else lines
        for line in tail_lines:
            # æ’é™¤æ— å…³å†…å®¹
            if "èµ„æ–™æä¾›" in line or "æ³•å¾‹ä¹‹æ˜Ÿ" in line or "å¼•ç”¨æ—¶è¯·å¯¹ç…§æ­£å¼æ–‡æœ¬" in line:
                continue
            if "æ‰«ç " in line or "æŸ¥æ³•è§„" in line:
                continue
            
            # è§£æé”®å€¼å¯¹
            pairs = re.split(r'\t+', line)
            for pair in pairs:
                pair = pair.strip()
                if not pair:
                    continue
                    
                if "æ³•è§„æ ‡é¢˜" in pair:
                    val = self._extract_value(pair)
                    if val and not metadata.get("title_from_tail"):
                        metadata["title_from_tail"] = val
                elif "å‘å¸ƒæ—¥æœŸ" in pair or "å…¬å¸ƒæ—¥æœŸ" in pair:
                    val = self._extract_value(pair)
                    if val and not metadata["issue_date"]:
                        metadata["issue_date"] = val
                elif ("å®æ–½æ—¥æœŸ" in pair or "æ–½è¡Œæ—¥æœŸ" in pair) and not metadata["effect_date"]:
                    val = self._extract_value(pair)
                    if val:
                        metadata["effect_date"] = val
                elif ("å‘å¸ƒéƒ¨é—¨" in pair or "å‘æ–‡æœºå…³" in pair) and not metadata["issue_org"]:
                    val = self._extract_value(pair)
                    if val:
                        metadata["issue_org"] = val
                elif ("æ•ˆåŠ›å±‚çº§" in pair or "æ•ˆåŠ›çº§åˆ«" in pair) and not metadata["level"]:
                    val = self._extract_value(pair)
                    if val:
                        metadata["level"] = val

        # 4. æ™ºèƒ½åˆ†ç±»å…œåº•
        title_for_category = metadata.get("title_from_tail") or lines[0] if lines else ""
        if not metadata["category"]:
            if "åˆ‘" in title_for_category or "ç½ª" in title_for_category:
                metadata["category"] = "åˆ‘äº‹æ³•å¾‹"
            elif "æ²»å®‰" in title_for_category or "è¡Œæ”¿" in title_for_category:
                metadata["category"] = "è¡Œæ”¿æ³•å¾‹"
            elif "ç¨‹" in title_for_category and "å®š" in title_for_category:
                metadata["category"] = "ç¨‹åºè§„å®š"
            elif "è§„å®š" in title_for_category:
                metadata["category"] = "éƒ¨é—¨è§„ç« "
        
        # 5. å­—æ®µé»˜è®¤å€¼å…œåº•
        if not metadata["issue_org"]:
            if "å…¬å®‰" in title_for_category and "è§„å®š" in title_for_category:
                metadata["issue_org"] = "å…¬å®‰éƒ¨"
            else:
                metadata["issue_org"] = "å…¨å›½äººæ°‘ä»£è¡¨å¤§ä¼šåŠå…¶å¸¸åŠ¡å§”å‘˜ä¼š"
        
        if not metadata["level"]:
            if "è§„å®š" in title_for_category or "åŠæ³•" in title_for_category:
                metadata["level"] = "éƒ¨é—¨è§„ç« "
            else:
                metadata["level"] = "æ³•å¾‹"
        
        return metadata

    def _extract_value(self, line):
        """æå–å†’å·åçš„å€¼"""
        # æ”¯æŒä¸­æ–‡å†’å·å’Œè‹±æ–‡å†’å·
        val = line.split('ï¼š')[-1].strip().split(':')[-1].strip()
        # æ¸…ç†å¯èƒ½çš„åˆ¶è¡¨ç¬¦å’Œå¤šä½™ç©ºæ ¼
        val = re.sub(r'[\t\s]+$', '', val)
        return val

    def split_articles(self, full_text):
        """
        é«˜çº§æ‹†åˆ†é€»è¾‘ï¼š
        1. è¯†åˆ«å¹¶å‰¥ç¦» æ¡æ–‡å†…å®¹ ä¸­çš„ ç« èŠ‚æ ‡é¢˜
        2. ç»´æŠ¤å‡†ç¡®çš„ç« èŠ‚å±‚çº§
        """
        articles = []
        
        # é¢„å¤„ç†ï¼šç»Ÿä¸€å…¨è§’ç©ºæ ¼
        full_text = re.sub(r'\u3000', ' ', full_text)
        
        # é¢„å¤„ç†ï¼šç§»é™¤æ— å…³å†…å®¹
        lines_to_remove = [
            r'èµ„æ–™æä¾›.*æ³•å¾‹ä¹‹æ˜Ÿ.*',
            r'å¼•ç”¨æ—¶è¯·å¯¹ç…§æ­£å¼æ–‡æœ¬',
            r'æ‰«ç éšæ—¶æŸ¥æ³•è§„',
            r'æ³•è§„æ ‡é¢˜ï¼š.*',
            r'æ³•è§„æ–‡å·ï¼š.*',
            r'å‘å¸ƒæ—¥æœŸï¼š.*',
            r'å®æ–½æ—¥æœŸï¼š.*',
            r'å‘å¸ƒéƒ¨é—¨ï¼š.*',
        ]
        for pattern in lines_to_remove:
            full_text = re.sub(pattern, '', full_text)
        
        # æ ¸å¿ƒæ­£åˆ™ï¼šåŒ¹é… "ç¬¬Xæ¡"
        # ä½¿ç”¨ Lookahead ç¡®ä¿æˆ‘ä»¬ä¸æ¶ˆè€—æ‰ä¸‹ä¸€ä¸ªæ¡æ–‡çš„å¼€å§‹
        # ä½† Python re ä¸æ”¯æŒå˜é•¿ lookbehindï¼Œæ‰€ä»¥æˆ‘ä»¬è¿˜æ˜¯ç”¨è¿­ä»£æŸ¥æ‰¾
        article_pattern = re.compile(r'(^|\n)\s*(ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+æ¡)\s+')
        
        matches = list(article_pattern.finditer(full_text))
        
        if not matches:
             # å°è¯•åŒ¹é…æ— ç©ºæ ¼ç‰ˆ
             article_pattern = re.compile(r'(^|\n)\s*(ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+æ¡)')
             matches = list(article_pattern.finditer(full_text))

        if not matches:
            logging.warning("  âš ï¸ æœªè¯†åˆ«åˆ°æ ‡å‡†æ¡æ–‡æ ¼å¼")
            return [{
                "article_num": 1,
                "article_display": "å…¨æ–‡",
                "content": full_text.strip(),
                "section": "",
                "chapter": ""
            }]

        # åˆ†åˆ«åŒ¹é…ï¼šç¼–ã€ç« ã€èŠ‚ã€ä»¥åŠç‰¹æ®Šç« èŠ‚åï¼ˆæ”¯æŒå…¨è§’ç©ºæ ¼ \u3000 å’Œæ™®é€šç©ºæ ¼ï¼‰
        part_pattern = re.compile(r'^\s*(ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+ç¼–[\s\u3000]+.*)$')  # ç¬¬Xç¼–
        chapter_pattern = re.compile(r'^\s*(ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+ç« [\s\u3000]+.*)$')  # ç¬¬Xç« 
        section_pattern = re.compile(r'^\s*(ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+èŠ‚[\s\u3000]+.*)$')  # ç¬¬XèŠ‚
        special_pattern = re.compile(r'^\s*(é™„[\s\u3000]*åˆ™|æ€»[\s\u3000]*åˆ™|åˆ†[\s\u3000]*åˆ™)$')  # é™„åˆ™ã€æ€»åˆ™ã€åˆ†åˆ™
        
        current_part = ""    # å½“å‰ç¼–
        current_chapter = "" # å½“å‰ç« 
        current_section = "" # å½“å‰èŠ‚
        
        def get_full_chapter():
            """æ„å»ºå®Œæ•´çš„ç« èŠ‚è·¯å¾„"""
            parts = []
            if current_part:
                parts.append(current_part)
            if current_chapter:
                parts.append(current_chapter)
            if current_section:
                parts.append(current_section)
            return " / ".join(parts) if parts else ""
        
        def is_structure_line(line_strip):
            """æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æ„è¡Œï¼ˆç¼–/ç« /èŠ‚/ç‰¹æ®Šç« èŠ‚ï¼‰"""
            return (part_pattern.match(line_strip) or 
                    chapter_pattern.match(line_strip) or 
                    section_pattern.match(line_strip) or 
                    special_pattern.match(line_strip))
        
        def update_structure(line_strip):
            """æ›´æ–°å½“å‰ç»“æ„å±‚çº§"""
            nonlocal current_part, current_chapter, current_section
            
            if part_pattern.match(line_strip):
                current_part = line_strip
                current_chapter = ""  # è¿›å…¥æ–°ç¼–æ—¶æ¸…ç©ºç« 
                current_section = ""  # æ¸…ç©ºèŠ‚
            elif chapter_pattern.match(line_strip):
                current_chapter = line_strip
                current_section = ""  # è¿›å…¥æ–°ç« æ—¶æ¸…ç©ºèŠ‚
            elif section_pattern.match(line_strip):
                current_section = line_strip
            elif special_pattern.match(line_strip):
                # é™„åˆ™ç­‰ç‰¹æ®Šç« èŠ‚ï¼Œä½œä¸ºç‹¬ç«‹çš„"ç¼–"å¤„ç†
                current_part = line_strip
                current_chapter = ""
                current_section = ""
        
        # åˆå§‹å±‚çº§ï¼šæ‰«æç¬¬ä¸€æ¡ä¹‹å‰çš„å†…å®¹
        pre_text = full_text[:matches[0].start()]
        for line in pre_text.split('\n'):
            line_strip = line.strip()
            if is_structure_line(line_strip):
                update_structure(line_strip)
        
        # ===== æ–°å¢ï¼šå‰è¨€æå–ï¼ˆå¸æ³•è§£é‡Šç‰¹æœ‰ï¼‰ =====
        # æ£€æµ‹ "ä¸ºä¾æ³•...è§£é‡Šå¦‚ä¸‹ï¼š" æˆ– "...è§„å®šå¦‚ä¸‹ï¼š" ç±»å‹çš„å‰è¨€æ®µè½
        preamble_content = None
        
        # åœ¨ç¬¬ä¸€æ¡ä¹‹å‰çš„æ–‡æœ¬ä¸­æŸ¥æ‰¾å‰è¨€
        pre_lines = pre_text.split('\n')
        preamble_lines = []
        in_preamble = False
        
        for line in pre_lines:
            line_strip = line.strip()
            if not line_strip:
                continue
            # è·³è¿‡ç»“æ„è¡Œ
            if is_structure_line(line_strip):
                continue
            # æ£€æµ‹å‰è¨€å¼€å§‹ï¼šä»¥"ä¸º"æˆ–"æ ¹æ®"å¼€å¤´
            if line_strip.startswith('ä¸º') or line_strip.startswith('æ ¹æ®'):
                in_preamble = True
            if in_preamble:
                preamble_lines.append(line_strip)
            # æ£€æµ‹å‰è¨€ç»“æŸï¼šä»¥"ï¼š"ç»“å°¾
            if in_preamble and (line_strip.endswith('ï¼š') or line_strip.endswith(':')):
                break
        
        if preamble_lines:
            preamble_content = ''.join(preamble_lines)
            logging.info(f"   ğŸ“‹ è¯†åˆ«åˆ°å‰è¨€: {preamble_content[:50]}...")


        for i, match in enumerate(matches):
            start = match.start()
            article_display = match.group(2).strip()
            # ä½¿ç”¨é¡ºåºç¼–å·ï¼Œé¿å…"ç¬¬Xæ¡ä¹‹ä¸€"ç­‰ç‰¹æ®Šæ¡å·å†²çª
            article_num = i + 1
            
            # ç¡®å®šç»“æŸä½ç½®
            end = matches[i+1].start() if i+1 < len(matches) else len(full_text)
            
            # æå–åŸå§‹å†…å®¹å—
            raw_content = full_text[start:end]
            lines = raw_content.split('\n')
            
            cleaned_lines = []
            found_next_structures = []  # è®°å½•å‘ç°çš„ç»“æ„è¡Œ
            
            for line in lines:
                line_strip = line.strip()
                if not line_strip:
                    cleaned_lines.append(line)
                    continue
                    
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æ„è¡Œ
                if is_structure_line(line_strip):
                    found_next_structures.append(line_strip)
                    continue  # ä¸åŒ…å«åœ¨å½“å‰æ¡æ–‡ä¸­
                
                cleaned_lines.append(line)
            
            # è®°å½•å½“å‰æ¡æ–‡çš„ç« èŠ‚ï¼ˆä½¿ç”¨æ›´æ–°å‰çš„å±‚çº§ï¼‰
            chapter_for_article = get_full_chapter()
            
            # é‡æ–°ç»„åˆå†…å®¹
            content_str = '\n'.join(cleaned_lines).strip()
            
            # ç§»é™¤å¼€å¤´çš„æ¡å·ï¼ˆå¦‚"ç¬¬ä¸€æ¡ "æˆ–"ç¬¬ä¸€æ¡ã€€"ï¼‰
            content_str = re.sub(r'^' + re.escape(article_display) + r'[\s\u3000]*', '', content_str)
            
            articles.append({
                "article_num": article_num,
                "article_display": article_display,
                "content": content_str,
                "chapter": chapter_for_article,
                "section": ""  # section å·²åˆå¹¶åˆ° chapter è·¯å¾„ä¸­
            })
            
            # æ›´æ–°ç»“æ„å±‚çº§ï¼Œä¾›ä¸‹ä¸€æ¡ä½¿ç”¨
            for struct_line in found_next_structures:
                update_structure(struct_line)
        
        # ===== æ–°å¢ï¼šå°†å‰è¨€æ’å…¥ä¸ºç¬¬é›¶æ¡ =====
        if preamble_content:
            # é‡æ–°ç¼–å·ï¼šæ‰€æœ‰æ¡æ–‡ article_num + 1
            for art in articles:
                art["article_num"] += 1
            # æ’å…¥å‰è¨€
            articles.insert(0, {
                "article_num": 0,
                "article_display": "å‰è¨€",
                "content": preamble_content,
                "chapter": "",
                "section": ""
            })
            logging.info(f"   âœ… å·²å°†å‰è¨€ä½œä¸ºç¬¬0æ¡æ’å…¥")

        return articles

    def chinese_to_number(self, chn):
        """ä¸­æ–‡æ•°å­—è½¬é˜¿æ‹‰ä¼¯æ•°å­— (ä¿®å¤ç‰ˆ)"""
        try:
            if chn.isdigit(): return int(chn)
            
            chinese_map = {'é›¶':0, 'ä¸€':1, 'äºŒ':2, 'ä¸‰':3, 'å››':4, 'äº”':5, 'å…­':6, 'ä¸ƒ':7, 'å…«':8, 'ä¹':9, 'å':10, 'ç™¾':100, 'åƒ':1000}
            
            # ç‰¹æ®Šæƒ…å†µå¤„ç†ï¼šåã€åä¸€...åä¹ -> ä¸€åã€ä¸€åä¸€...ä¸€åä¹
            if chn.startswith('å'):
                chn = 'ä¸€' + chn
                
            result = 0
            unit_val = 0 # å½“å‰ç´¯ç§¯çš„å°èŠ‚å€¼
            
            for char in chn:
                if char not in chinese_map:
                    continue
                val = chinese_map[char]
                
                if val >= 10: # æ˜¯å•ä½
                    if unit_val == 0: unit_val = 1 # å¤„ç† "å" è¿™ç§å‰é¢æ²¡æ•°å­—çš„æƒ…å†µï¼Œä½†ä¸Šé¢å·²ç»è¡¥äº†"ä¸€"ï¼Œè¿™é‡Œæ˜¯åŒä¿é™©
                    result += unit_val * val
                    unit_val = 0 # å½’é›¶ï¼Œå‡†å¤‡æ¥ä¸ªä½æ•°
                else: # æ˜¯æ•°å­—
                    unit_val = val
                    
            result += unit_val # åŠ ä¸Šæœ€åçš„ä¸ªä½æ•°
            return result
        except:
            return 0

    def generate_id(self, title):
        return hashlib.md5(title.encode('utf-8')).hexdigest()[:16]

    def read_docx(self, file_path):
        """è¯»å– Word æ–‡æ¡£å¹¶æå–æ–‡æœ¬"""
        try:
            doc = Document(file_path)
            paragraphs = []
            for para in doc.paragraphs:
                text = para.text.strip()
                if text:
                    paragraphs.append(text)
            content = '\n'.join(paragraphs)
            logging.info(f"   ğŸ“„ Word æ–‡æ¡£è¯»å–æˆåŠŸï¼Œå…± {len(paragraphs)} æ®µ")
            return content
        except Exception as e:
            logging.error(f"âŒ è¯»å– Word æ–‡æ¡£å¤±è´¥: {e}")
            return None

    def run(self):
        if not os.path.exists(self.input_dir):
            os.makedirs(self.input_dir)
            logging.info(f"ğŸ“ å·²åˆ›å»ºç›®å½• {self.input_dir}ï¼Œè¯·å°† txt/md æ–‡ä»¶æ”¾å…¥å…¶ä¸­ã€‚")
            return

        # æ”¯æŒ txt, md, docx æ ¼å¼
        supported_ext = ['.txt', '.md']
        if DOCX_SUPPORTED:
            supported_ext.append('.docx')
        
        files = [f for f in os.listdir(self.input_dir) if os.path.splitext(f)[1].lower() in supported_ext]
        if not files:
            logging.warning(f"âš ï¸ {self.input_dir} ç›®å½•ä¸ºç©ºï¼Œè¯·æ”¾å…¥æ³•è§„æ–‡ä»¶ï¼ˆæ”¯æŒ: {', '.join(supported_ext)}ï¼‰ï¼")
            return

        logging.info(f"ğŸš€ å‘ç° {len(files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
        
        for file in files:
            file_path = os.path.join(self.input_dir, file)
            title = os.path.splitext(file)[0]  # æ–‡ä»¶åä½œä¸ºæ ‡é¢˜
             # å»é™¤æ–‡ä»¶åä¸­çš„ (2024) ç­‰ä¿®é¥°ä»¥ä¿æŒå¹²å‡€ï¼Œæˆ–è€…ä¿ç•™
            
            logging.info(f"ğŸ“„ å¤„ç†: {title}")
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–å†…å®¹
            ext = os.path.splitext(file)[1].lower()
            
            if ext == '.docx':
                content = self.read_docx(file_path)
                if content is None:
                    continue
            else:
                # txt æˆ– md æ–‡ä»¶
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                except UnicodeDecodeError:
                    try:
                        with open(file_path, 'r', encoding='gbk') as f:
                            content = f.read()
                    except:
                        logging.error(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {file} (ç¼–ç é”™è¯¯)")
                        continue

            # 1. è§£æå…ƒæ•°æ®
            metadata = self.parse_metadata(content)
            
            # ä¼˜å…ˆä½¿ç”¨åˆå¹¶åçš„æ ‡é¢˜ > æ–‡æ¡£æœ«å°¾çš„æ ‡é¢˜ > æ–‡ä»¶å
            if metadata.get("merged_title"):
                title = metadata["merged_title"]
                logging.info(f"   ğŸ“‹ ä½¿ç”¨åˆå¹¶æ ‡é¢˜: {title}")
            elif metadata.get("title_from_tail"):
                title = metadata["title_from_tail"]
                logging.info(f"   ğŸ“‹ ä½¿ç”¨æ–‡æ¡£ä¸­çš„æ ‡é¢˜: {title}")
            
            law_id = self.generate_id(title)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥æ³•è§„ï¼Œä¿ç•™å·²æœ‰çš„æœ‰æ•ˆæ•°æ®
            existing_law = self.db.laws.find_one({"law_id": law_id})
            
            # æ™ºèƒ½åˆå¹¶ï¼šå¦‚æœæ–°è§£æçš„æ˜¯é»˜è®¤å€¼ï¼Œä½†æ•°æ®åº“ä¸­æœ‰æ›´å¥½çš„å€¼ï¼Œåˆ™ä¿ç•™æ•°æ®åº“çš„å€¼
            final_issue_org = metadata["issue_org"]
            final_issue_date = metadata["issue_date"] or "2000-01-01"
            final_effect_date = metadata["effect_date"] or "2000-01-01"
            
            if existing_law:
                # å¦‚æœæ–°å€¼æ˜¯é»˜è®¤å€¼ï¼Œä½†æ—§å€¼æ›´æœ‰æ„ä¹‰ï¼Œåˆ™ä¿ç•™æ—§å€¼
                if final_issue_date == "2000-01-01" and existing_law.get("issue_date") and existing_law["issue_date"] != "2000-01-01":
                    final_issue_date = existing_law["issue_date"]
                    logging.info(f"   ğŸ“… ä¿ç•™å·²æœ‰çš„å‘å¸ƒæ—¥æœŸ: {final_issue_date}")
                    
                if final_effect_date == "2000-01-01" and existing_law.get("effect_date") and existing_law["effect_date"] != "2000-01-01":
                    final_effect_date = existing_law["effect_date"]
                    logging.info(f"   ğŸ“… ä¿ç•™å·²æœ‰çš„å®æ–½æ—¥æœŸ: {final_effect_date}")
                
                # å¦‚æœæ–°çš„å‘å¸ƒæœºå…³æ˜¯é€šç”¨é»˜è®¤å€¼ï¼Œä½†æ—§å€¼æ›´å…·ä½“ï¼Œåˆ™ä¿ç•™æ—§å€¼
                generic_orgs = ["å…¬å®‰éƒ¨", "å…¨å›½äººæ°‘ä»£è¡¨å¤§ä¼šåŠå…¶å¸¸åŠ¡å§”å‘˜ä¼š"]
                if final_issue_org in generic_orgs and existing_law.get("issue_org") and existing_law["issue_org"] not in generic_orgs:
                    final_issue_org = existing_law["issue_org"]
                    logging.info(f"   ğŸ›ï¸ ä¿ç•™å·²æœ‰çš„å‘å¸ƒæœºå…³: {final_issue_org}")
            
            law_doc = {
                "law_id": law_id,
                "title": title,
                "status": metadata["status"],
                "issue_org": final_issue_org,
                "issue_date": final_issue_date,
                "effect_date": final_effect_date,
                "category": metadata["category"],
                "level": metadata["level"],
                "content_type": "text",
                "summary": metadata.get("summary", "")
            }
            
            # 2. æ’å…¥æ³•è§„ä¸»è¡¨
            try:
                self.db.laws.replace_one({"law_id": law_id}, law_doc, upsert=True)
            except Exception as e:
                logging.error(f"å…¥åº“æ³•è§„å¤±è´¥: {e}")
                continue

            # 3. æ‹†åˆ†æ¡æ–‡
            articles = self.split_articles(content)
            
            # 4. æ’å…¥æ¡æ–‡è¡¨ (å…ˆåˆ åæ’)
            self.db.law_articles.delete_many({"law_id": law_id})
            
            article_docs = []
            for art in articles:
                art["law_id"] = law_id
                article_docs.append(art)
            
            if article_docs:
                try:
                    # ordered=False å…è®¸è·³è¿‡é‡å¤é¡¹ç»§ç»­æ’å…¥
                    self.db.law_articles.insert_many(article_docs, ordered=False)
                except Exception as e:
                    # BulkWriteError å¯èƒ½å› é‡å¤é”®å¼•å‘ï¼Œä½†éƒ¨åˆ†æ•°æ®å·²æˆåŠŸæ’å…¥
                    logging.warning(f"   âš ï¸ éƒ¨åˆ†æ¡æ–‡æ’å…¥æ—¶é‡åˆ°é—®é¢˜: {str(e)[:100]}")
                
                inserted_count = self.db.law_articles.count_documents({"law_id": law_id})
                logging.info(f"   âœ… æˆåŠŸå¯¼å…¥ {inserted_count} æ¡æ¡æ–‡")
            else:
                logging.warning(f"   âš ï¸ æœªæå–åˆ°æ¡æ–‡ï¼Œè¯·æ£€æŸ¥æ ¼å¼")

        logging.info("ğŸ‰ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    importer = LocalImporter()
    importer.run()
