import os
import re
import hashlib
import json
import logging
from pymongo import MongoClient, ASCENDING, TEXT
from dotenv import load_dotenv

# Word æ–‡æ¡£æ”¯æŒ
try:
    from docx import Document
    DOCX_SUPPORTED = True
except ImportError:
    DOCX_SUPPORTED = False
    logging.warning("python-docx æœªå®‰è£…ï¼Œ.docx æ–‡ä»¶å°†ä¸è¢«æ”¯æŒã€‚è¿è¡Œ: pip install python-docx")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(os.path.join(os.path.dirname(os.path.dirname(__file__)), 'backend', '.env'))

class LocalImporter:
    def __init__(self, input_dir="manual_data"):
        self.input_dir = input_dir
        self.mongo_uri = os.getenv("MONGODB_URL", "mongodb://localhost:27017")
        self.client = MongoClient(self.mongo_uri)
        self.db = self.client[os.getenv("MONGODB_DB", "law_system")]  # ç¡®ä¿æ•°æ®åº“åä¸åç«¯ä¸€è‡´
        self.setup_indexes()

    def setup_indexes(self):
        """ç¡®ä¿ç´¢å¼•å­˜åœ¨ï¼ˆå¿½ç•¥å·²å­˜åœ¨çš„ç´¢å¼•å†²çªï¼‰"""
        try:
            self.db.laws.create_index([("title", ASCENDING)], unique=True)
        except Exception:
            pass
        try:
            self.db.laws.create_index([("title", TEXT), ("summary", TEXT)], name="law_text_search")
        except Exception:
            pass
        try:
            self.db.law_articles.create_index([("law_id", ASCENDING), ("article_num", ASCENDING)], unique=True)
        except Exception:
            pass
        try:
            self.db.law_articles.create_index([("content", TEXT)], name="article_content_search")
        except Exception:
            pass

    def parse_metadata(self, content):
        """è§£æå…ƒæ•°æ®å’Œä¿®è®¢è¯´æ˜"""
        metadata = {
            "issue_date": "",
            "effect_date": "",
            "issue_org": "",  # å…ˆç•™ç©ºï¼Œç¨åå…œåº•
            "status": "æœ‰æ•ˆ",
            "category": "",
            "level": "",  # å…ˆç•™ç©ºï¼Œç¨åå…œåº•
            "summary": ""
        }
        
        lines = [line.strip() for line in content.split('\n') if line.strip()]
        
        # 1. å°è¯•æå–ä¿®è®¢è¯´æ˜ï¼ˆåœ†æ‹¬å·åŒ…è£¹çš„é•¿æ®µè½ï¼‰
        # é€šå¸¸åœ¨æ ‡é¢˜ï¼ˆç¬¬ä¸€è¡Œï¼‰ä¹‹åï¼Œæ­£æ–‡ä¹‹å‰
        for i in range(1, min(10, len(lines))):
            line = lines[i]
            # ç‰¹å¾ï¼šä»¥(æˆ–ï¼ˆå¼€å¤´ï¼ŒåŒ…å«"é€šè¿‡"ã€"ä¿®æ­£"ã€"ä¿®è®¢"ç­‰å­—çœ¼
            if (line.startswith('(') or line.startswith('ï¼ˆ')) and \
               ('é€šè¿‡' in line or 'ä¿®æ­£' in line or 'ä¿®è®¢' in line):
                metadata['summary'] = line
                break
        
        # 2. ä»å‰20è¡Œæå–KVå…ƒæ•°æ®
        header_lines = lines[:20]
        for line in header_lines:
            if "å‘å¸ƒæ—¥æœŸ" in line or "å…¬å¸ƒæ—¥æœŸ" in line:
                metadata["issue_date"] = self._extract_value(line)
            if "å®æ–½æ—¥æœŸ" in line or "æ–½è¡Œæ—¥æœŸ" in line:
                metadata["effect_date"] = self._extract_value(line)
            if "å‘å¸ƒéƒ¨é—¨" in line or "å‘æ–‡æœºå…³" in line:
                metadata["issue_org"] = self._extract_value(line)
            if "æ•ˆåŠ›" in line and ("çº§åˆ«" in line or "ç­‰çº§" in line):
                metadata["level"] = self._extract_value(line)
            if "ç±»åˆ«" in line:
                metadata["category"] = self._extract_value(line)
        
        # 3. ä»æœ«å°¾è§£æç»“æ„åŒ–å…ƒæ•°æ®ï¼ˆWord æ–‡æ¡£å¸¸è§æ ¼å¼ï¼‰
        # æ ¼å¼å¦‚ï¼šæ³•è§„æ ‡é¢˜ï¼šxxx	æ³•è§„æ–‡å·ï¼šxxx
        #        å‘å¸ƒæ—¥æœŸï¼šxxx	å®æ–½æ—¥æœŸï¼šxxx
        #        å‘å¸ƒéƒ¨é—¨ï¼šxxx
        tail_lines = lines[-30:] if len(lines) > 30 else lines
        for line in tail_lines:
            # æ’é™¤æ— å…³å†…å®¹
            if "èµ„æ–™æä¾›" in line or "æ³•å¾‹ä¹‹æ˜Ÿ" in line or "å¼•ç”¨æ—¶è¯·å¯¹ç…§æ­£å¼æ–‡æœ¬" in line:
                continue
            if "æ‰«ç " in line or "æŸ¥æ³•è§„" in line:
                continue
            
            # è§£æé”®å€¼å¯¹ï¼ˆæ”¯æŒåˆ¶è¡¨ç¬¦åˆ†éš”çš„å¤šä¸ªé”®å€¼å¯¹ï¼‰
            # ä¾‹å¦‚ï¼šå‘å¸ƒæ—¥æœŸï¼š2020å¹´07æœˆ20æ—¥	å®æ–½æ—¥æœŸï¼š2020å¹´09æœˆ01æ—¥
            pairs = re.split(r'\t+', line)
            for pair in pairs:
                pair = pair.strip()
                if not pair:
                    continue
                    
                if "æ³•è§„æ ‡é¢˜" in pair:
                    val = self._extract_value(pair)
                    if val and not metadata.get("title_from_tail"):
                        metadata["title_from_tail"] = val
                elif "å‘å¸ƒæ—¥æœŸ" in pair or "å…¬å¸ƒæ—¥æœŸ" in pair:
                    val = self._extract_value(pair)
                    if val and not metadata["issue_date"]:
                        metadata["issue_date"] = val
                elif "å®æ–½æ—¥æœŸ" in pair or "æ–½è¡Œæ—¥æœŸ" in pair:
                    val = self._extract_value(pair)
                    if val and not metadata["effect_date"]:
                        metadata["effect_date"] = val
                elif "å‘å¸ƒéƒ¨é—¨" in pair or "å‘æ–‡æœºå…³" in pair:
                    val = self._extract_value(pair)
                    if val and not metadata["issue_org"]:
                        metadata["issue_org"] = val
                elif "æ•ˆåŠ›å±‚çº§" in pair or "æ•ˆåŠ›çº§åˆ«" in pair:
                    val = self._extract_value(pair)
                    if val and not metadata["level"]:
                        metadata["level"] = val

        # 4. æ™ºèƒ½åˆ†ç±»å…œåº•
        title_for_category = metadata.get("title_from_tail") or lines[0] if lines else ""
        if not metadata["category"]:
            if "åˆ‘" in title_for_category or "ç½ª" in title_for_category:
                metadata["category"] = "åˆ‘äº‹æ³•å¾‹"
            elif "æ²»å®‰" in title_for_category or "è¡Œæ”¿" in title_for_category:
                metadata["category"] = "è¡Œæ”¿æ³•å¾‹"
            elif "ç¨‹" in title_for_category and "å®š" in title_for_category:
                metadata["category"] = "ç¨‹åºè§„å®š"
            elif "è§„å®š" in title_for_category:
                metadata["category"] = "éƒ¨é—¨è§„ç« "
        
        # 5. å­—æ®µé»˜è®¤å€¼å…œåº•
        if not metadata["issue_org"]:
            # æ ¹æ®æ ‡é¢˜æ™ºèƒ½åˆ¤æ–­å‘å¸ƒæœºå…³
            if "å…¬å®‰" in title_for_category and "è§„å®š" in title_for_category:
                metadata["issue_org"] = "å…¬å®‰éƒ¨"
            else:
                metadata["issue_org"] = "å…¨å›½äººæ°‘ä»£è¡¨å¤§ä¼šåŠå…¶å¸¸åŠ¡å§”å‘˜ä¼š"
        
        if not metadata["level"]:
            # æ ¹æ®æ ‡é¢˜æ™ºèƒ½åˆ¤æ–­æ•ˆåŠ›å±‚çº§
            if "è§„å®š" in title_for_category or "åŠæ³•" in title_for_category:
                metadata["level"] = "éƒ¨é—¨è§„ç« "
            else:
                metadata["level"] = "æ³•å¾‹"
        
        return metadata

    def _extract_value(self, line):
        """æå–å†’å·åçš„å€¼"""
        # æ”¯æŒä¸­æ–‡å†’å·å’Œè‹±æ–‡å†’å·
        val = line.split('ï¼š')[-1].strip().split(':')[-1].strip()
        # æ¸…ç†å¯èƒ½çš„åˆ¶è¡¨ç¬¦å’Œå¤šä½™ç©ºæ ¼
        val = re.sub(r'[\t\s]+$', '', val)
        return val

    def split_articles(self, full_text):
        """
        é«˜çº§æ‹†åˆ†é€»è¾‘ï¼š
        1. è¯†åˆ«å¹¶å‰¥ç¦» æ¡æ–‡å†…å®¹ ä¸­çš„ ç« èŠ‚æ ‡é¢˜
        2. ç»´æŠ¤å‡†ç¡®çš„ç« èŠ‚å±‚çº§
        """
        articles = []
        
        # é¢„å¤„ç†ï¼šç»Ÿä¸€å…¨è§’ç©ºæ ¼
        full_text = re.sub(r'\u3000', ' ', full_text)
        
        # é¢„å¤„ç†ï¼šç§»é™¤æ— å…³å†…å®¹
        lines_to_remove = [
            r'èµ„æ–™æä¾›.*æ³•å¾‹ä¹‹æ˜Ÿ.*',
            r'å¼•ç”¨æ—¶è¯·å¯¹ç…§æ­£å¼æ–‡æœ¬',
            r'æ‰«ç éšæ—¶æŸ¥æ³•è§„',
            r'æ³•è§„æ ‡é¢˜ï¼š.*',
            r'æ³•è§„æ–‡å·ï¼š.*',
            r'å‘å¸ƒæ—¥æœŸï¼š.*',
            r'å®æ–½æ—¥æœŸï¼š.*',
            r'å‘å¸ƒéƒ¨é—¨ï¼š.*',
        ]
        for pattern in lines_to_remove:
            full_text = re.sub(pattern, '', full_text)
        
        # æ ¸å¿ƒæ­£åˆ™ï¼šåŒ¹é… "ç¬¬Xæ¡"
        # ä½¿ç”¨ Lookahead ç¡®ä¿æˆ‘ä»¬ä¸æ¶ˆè€—æ‰ä¸‹ä¸€ä¸ªæ¡æ–‡çš„å¼€å§‹
        # ä½† Python re ä¸æ”¯æŒå˜é•¿ lookbehindï¼Œæ‰€ä»¥æˆ‘ä»¬è¿˜æ˜¯ç”¨è¿­ä»£æŸ¥æ‰¾
        article_pattern = re.compile(r'(^|\n)\s*(ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+æ¡)\s+')
        
        matches = list(article_pattern.finditer(full_text))
        
        if not matches:
             # å°è¯•åŒ¹é…æ— ç©ºæ ¼ç‰ˆ
             article_pattern = re.compile(r'(^|\n)\s*(ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+æ¡)')
             matches = list(article_pattern.finditer(full_text))

        if not matches:
            logging.warning("  âš ï¸ æœªè¯†åˆ«åˆ°æ ‡å‡†æ¡æ–‡æ ¼å¼")
            return [{
                "article_num": 1,
                "article_display": "å…¨æ–‡",
                "content": full_text.strip(),
                "section": "",
                "chapter": ""
            }]

        # åˆ†åˆ«åŒ¹é…ï¼šç¼–ã€ç« ã€èŠ‚ã€ä»¥åŠç‰¹æ®Šç« èŠ‚åï¼ˆæ”¯æŒå…¨è§’ç©ºæ ¼ \u3000 å’Œæ™®é€šç©ºæ ¼ï¼‰
        part_pattern = re.compile(r'^\s*(ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+ç¼–[\s\u3000]+.*)$')  # ç¬¬Xç¼–
        chapter_pattern = re.compile(r'^\s*(ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+ç« [\s\u3000]+.*)$')  # ç¬¬Xç« 
        section_pattern = re.compile(r'^\s*(ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+èŠ‚[\s\u3000]+.*)$')  # ç¬¬XèŠ‚
        special_pattern = re.compile(r'^\s*(é™„[\s\u3000]*åˆ™|æ€»[\s\u3000]*åˆ™|åˆ†[\s\u3000]*åˆ™)$')  # é™„åˆ™ã€æ€»åˆ™ã€åˆ†åˆ™
        
        current_part = ""    # å½“å‰ç¼–
        current_chapter = "" # å½“å‰ç« 
        current_section = "" # å½“å‰èŠ‚
        
        def get_full_chapter():
            """æ„å»ºå®Œæ•´çš„ç« èŠ‚è·¯å¾„"""
            parts = []
            if current_part:
                parts.append(current_part)
            if current_chapter:
                parts.append(current_chapter)
            if current_section:
                parts.append(current_section)
            return " / ".join(parts) if parts else ""
        
        def is_structure_line(line_strip):
            """æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æ„è¡Œï¼ˆç¼–/ç« /èŠ‚/ç‰¹æ®Šç« èŠ‚ï¼‰"""
            return (part_pattern.match(line_strip) or 
                    chapter_pattern.match(line_strip) or 
                    section_pattern.match(line_strip) or 
                    special_pattern.match(line_strip))
        
        def update_structure(line_strip):
            """æ›´æ–°å½“å‰ç»“æ„å±‚çº§"""
            nonlocal current_part, current_chapter, current_section
            
            if part_pattern.match(line_strip):
                current_part = line_strip
                current_chapter = ""  # è¿›å…¥æ–°ç¼–æ—¶æ¸…ç©ºç« 
                current_section = ""  # æ¸…ç©ºèŠ‚
            elif chapter_pattern.match(line_strip):
                current_chapter = line_strip
                current_section = ""  # è¿›å…¥æ–°ç« æ—¶æ¸…ç©ºèŠ‚
            elif section_pattern.match(line_strip):
                current_section = line_strip
            elif special_pattern.match(line_strip):
                # é™„åˆ™ç­‰ç‰¹æ®Šç« èŠ‚ï¼Œä½œä¸ºç‹¬ç«‹çš„"ç¼–"å¤„ç†
                current_part = line_strip
                current_chapter = ""
                current_section = ""
        
        # åˆå§‹å±‚çº§ï¼šæ‰«æç¬¬ä¸€æ¡ä¹‹å‰çš„å†…å®¹
        pre_text = full_text[:matches[0].start()]
        for line in pre_text.split('\n'):
            line_strip = line.strip()
            if is_structure_line(line_strip):
                update_structure(line_strip)

        for i, match in enumerate(matches):
            start = match.start()
            article_display = match.group(2).strip()
            # ä½¿ç”¨é¡ºåºç¼–å·ï¼Œé¿å…"ç¬¬Xæ¡ä¹‹ä¸€"ç­‰ç‰¹æ®Šæ¡å·å†²çª
            article_num = i + 1
            
            # ç¡®å®šç»“æŸä½ç½®
            end = matches[i+1].start() if i+1 < len(matches) else len(full_text)
            
            # æå–åŸå§‹å†…å®¹å—
            raw_content = full_text[start:end]
            lines = raw_content.split('\n')
            
            cleaned_lines = []
            found_next_structures = []  # è®°å½•å‘ç°çš„ç»“æ„è¡Œ
            
            for line in lines:
                line_strip = line.strip()
                if not line_strip:
                    cleaned_lines.append(line)
                    continue
                    
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æ„è¡Œ
                if is_structure_line(line_strip):
                    found_next_structures.append(line_strip)
                    continue  # ä¸åŒ…å«åœ¨å½“å‰æ¡æ–‡ä¸­
                
                cleaned_lines.append(line)
            
            # è®°å½•å½“å‰æ¡æ–‡çš„ç« èŠ‚ï¼ˆä½¿ç”¨æ›´æ–°å‰çš„å±‚çº§ï¼‰
            chapter_for_article = get_full_chapter()
            
            # é‡æ–°ç»„åˆå†…å®¹
            content_str = '\n'.join(cleaned_lines).strip()
            
            articles.append({
                "article_num": article_num,
                "article_display": article_display,
                "content": content_str,
                "chapter": chapter_for_article,
                "section": ""  # section å·²åˆå¹¶åˆ° chapter è·¯å¾„ä¸­
            })
            
            # æ›´æ–°ç»“æ„å±‚çº§ï¼Œä¾›ä¸‹ä¸€æ¡ä½¿ç”¨
            for struct_line in found_next_structures:
                update_structure(struct_line)

        return articles

    def chinese_to_number(self, chn):
        """ä¸­æ–‡æ•°å­—è½¬é˜¿æ‹‰ä¼¯æ•°å­— (ä¿®å¤ç‰ˆ)"""
        try:
            if chn.isdigit(): return int(chn)
            
            chinese_map = {'é›¶':0, 'ä¸€':1, 'äºŒ':2, 'ä¸‰':3, 'å››':4, 'äº”':5, 'å…­':6, 'ä¸ƒ':7, 'å…«':8, 'ä¹':9, 'å':10, 'ç™¾':100, 'åƒ':1000}
            
            # ç‰¹æ®Šæƒ…å†µå¤„ç†ï¼šåã€åä¸€...åä¹ -> ä¸€åã€ä¸€åä¸€...ä¸€åä¹
            if chn.startswith('å'):
                chn = 'ä¸€' + chn
                
            result = 0
            unit_val = 0 # å½“å‰ç´¯ç§¯çš„å°èŠ‚å€¼
            
            for char in chn:
                if char not in chinese_map:
                    continue
                val = chinese_map[char]
                
                if val >= 10: # æ˜¯å•ä½
                    if unit_val == 0: unit_val = 1 # å¤„ç† "å" è¿™ç§å‰é¢æ²¡æ•°å­—çš„æƒ…å†µï¼Œä½†ä¸Šé¢å·²ç»è¡¥äº†"ä¸€"ï¼Œè¿™é‡Œæ˜¯åŒä¿é™©
                    result += unit_val * val
                    unit_val = 0 # å½’é›¶ï¼Œå‡†å¤‡æ¥ä¸ªä½æ•°
                else: # æ˜¯æ•°å­—
                    unit_val = val
                    
            result += unit_val # åŠ ä¸Šæœ€åçš„ä¸ªä½æ•°
            return result
        except:
            return 0

    def generate_id(self, title):
        return hashlib.md5(title.encode('utf-8')).hexdigest()[:16]

    def read_docx(self, file_path):
        """è¯»å– Word æ–‡æ¡£å¹¶æå–æ–‡æœ¬"""
        try:
            doc = Document(file_path)
            paragraphs = []
            for para in doc.paragraphs:
                text = para.text.strip()
                if text:
                    paragraphs.append(text)
            content = '\n'.join(paragraphs)
            logging.info(f"   ğŸ“„ Word æ–‡æ¡£è¯»å–æˆåŠŸï¼Œå…± {len(paragraphs)} æ®µ")
            return content
        except Exception as e:
            logging.error(f"âŒ è¯»å– Word æ–‡æ¡£å¤±è´¥: {e}")
            return None

    def run(self):
        if not os.path.exists(self.input_dir):
            os.makedirs(self.input_dir)
            logging.info(f"ğŸ“ å·²åˆ›å»ºç›®å½• {self.input_dir}ï¼Œè¯·å°† txt/md æ–‡ä»¶æ”¾å…¥å…¶ä¸­ã€‚")
            return

        # æ”¯æŒ txt, md, docx æ ¼å¼
        supported_ext = ['.txt', '.md']
        if DOCX_SUPPORTED:
            supported_ext.append('.docx')
        
        files = [f for f in os.listdir(self.input_dir) if os.path.splitext(f)[1].lower() in supported_ext]
        if not files:
            logging.warning(f"âš ï¸ {self.input_dir} ç›®å½•ä¸ºç©ºï¼Œè¯·æ”¾å…¥æ³•è§„æ–‡ä»¶ï¼ˆæ”¯æŒ: {', '.join(supported_ext)}ï¼‰ï¼")
            return

        logging.info(f"ğŸš€ å‘ç° {len(files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
        
        for file in files:
            file_path = os.path.join(self.input_dir, file)
            title = os.path.splitext(file)[0]  # æ–‡ä»¶åä½œä¸ºæ ‡é¢˜
             # å»é™¤æ–‡ä»¶åä¸­çš„ (2024) ç­‰ä¿®é¥°ä»¥ä¿æŒå¹²å‡€ï¼Œæˆ–è€…ä¿ç•™
            
            logging.info(f"ğŸ“„ å¤„ç†: {title}")
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–å†…å®¹
            ext = os.path.splitext(file)[1].lower()
            
            if ext == '.docx':
                content = self.read_docx(file_path)
                if content is None:
                    continue
            else:
                # txt æˆ– md æ–‡ä»¶
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                except UnicodeDecodeError:
                    try:
                        with open(file_path, 'r', encoding='gbk') as f:
                            content = f.read()
                    except:
                        logging.error(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {file} (ç¼–ç é”™è¯¯)")
                        continue

            # 1. è§£æå…ƒæ•°æ®
            metadata = self.parse_metadata(content)
            
            # ä¼˜å…ˆä½¿ç”¨ä»æ–‡æ¡£æœ«å°¾è§£æåˆ°çš„æ ‡é¢˜
            if metadata.get("title_from_tail"):
                title = metadata["title_from_tail"]
                logging.info(f"   ğŸ“‹ ä½¿ç”¨æ–‡æ¡£ä¸­çš„æ ‡é¢˜: {title}")
            
            law_id = self.generate_id(title)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥æ³•è§„ï¼Œä¿ç•™å·²æœ‰çš„æœ‰æ•ˆæ•°æ®
            existing_law = self.db.laws.find_one({"law_id": law_id})
            
            # æ™ºèƒ½åˆå¹¶ï¼šå¦‚æœæ–°è§£æçš„æ˜¯é»˜è®¤å€¼ï¼Œä½†æ•°æ®åº“ä¸­æœ‰æ›´å¥½çš„å€¼ï¼Œåˆ™ä¿ç•™æ•°æ®åº“çš„å€¼
            final_issue_org = metadata["issue_org"]
            final_issue_date = metadata["issue_date"] or "2000-01-01"
            final_effect_date = metadata["effect_date"] or "2000-01-01"
            
            if existing_law:
                # å¦‚æœæ–°å€¼æ˜¯é»˜è®¤å€¼ï¼Œä½†æ—§å€¼æ›´æœ‰æ„ä¹‰ï¼Œåˆ™ä¿ç•™æ—§å€¼
                if final_issue_date == "2000-01-01" and existing_law.get("issue_date") and existing_law["issue_date"] != "2000-01-01":
                    final_issue_date = existing_law["issue_date"]
                    logging.info(f"   ğŸ“… ä¿ç•™å·²æœ‰çš„å‘å¸ƒæ—¥æœŸ: {final_issue_date}")
                    
                if final_effect_date == "2000-01-01" and existing_law.get("effect_date") and existing_law["effect_date"] != "2000-01-01":
                    final_effect_date = existing_law["effect_date"]
                    logging.info(f"   ğŸ“… ä¿ç•™å·²æœ‰çš„å®æ–½æ—¥æœŸ: {final_effect_date}")
                
                # å¦‚æœæ–°çš„å‘å¸ƒæœºå…³æ˜¯é€šç”¨é»˜è®¤å€¼ï¼Œä½†æ—§å€¼æ›´å…·ä½“ï¼Œåˆ™ä¿ç•™æ—§å€¼
                generic_orgs = ["å…¬å®‰éƒ¨", "å…¨å›½äººæ°‘ä»£è¡¨å¤§ä¼šåŠå…¶å¸¸åŠ¡å§”å‘˜ä¼š"]
                if final_issue_org in generic_orgs and existing_law.get("issue_org") and existing_law["issue_org"] not in generic_orgs:
                    final_issue_org = existing_law["issue_org"]
                    logging.info(f"   ğŸ›ï¸ ä¿ç•™å·²æœ‰çš„å‘å¸ƒæœºå…³: {final_issue_org}")
            
            law_doc = {
                "law_id": law_id,
                "title": title,
                "status": metadata["status"],
                "issue_org": final_issue_org,
                "issue_date": final_issue_date,
                "effect_date": final_effect_date,
                "category": metadata["category"],
                "level": metadata["level"],
                "content_type": "text",
                "summary": metadata.get("summary", "")
            }
            
            # 2. æ’å…¥æ³•è§„ä¸»è¡¨
            try:
                self.db.laws.replace_one({"law_id": law_id}, law_doc, upsert=True)
            except Exception as e:
                logging.error(f"å…¥åº“æ³•è§„å¤±è´¥: {e}")
                continue

            # 3. æ‹†åˆ†æ¡æ–‡
            articles = self.split_articles(content)
            
            # 4. æ’å…¥æ¡æ–‡è¡¨ (å…ˆåˆ åæ’)
            self.db.law_articles.delete_many({"law_id": law_id})
            
            article_docs = []
            for art in articles:
                art["law_id"] = law_id
                article_docs.append(art)
            
            if article_docs:
                try:
                    # ordered=False å…è®¸è·³è¿‡é‡å¤é¡¹ç»§ç»­æ’å…¥
                    self.db.law_articles.insert_many(article_docs, ordered=False)
                except Exception as e:
                    # BulkWriteError å¯èƒ½å› é‡å¤é”®å¼•å‘ï¼Œä½†éƒ¨åˆ†æ•°æ®å·²æˆåŠŸæ’å…¥
                    logging.warning(f"   âš ï¸ éƒ¨åˆ†æ¡æ–‡æ’å…¥æ—¶é‡åˆ°é—®é¢˜: {str(e)[:100]}")
                
                inserted_count = self.db.law_articles.count_documents({"law_id": law_id})
                logging.info(f"   âœ… æˆåŠŸå¯¼å…¥ {inserted_count} æ¡æ¡æ–‡")
            else:
                logging.warning(f"   âš ï¸ æœªæå–åˆ°æ¡æ–‡ï¼Œè¯·æ£€æŸ¥æ ¼å¼")

        logging.info("ğŸ‰ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    importer = LocalImporter()
    importer.run()
