"""
æ³•è§„çˆ¬è™«åŸºç±»ï¼ˆé€‚é…å™¨æ¨¡å¼ï¼‰

ä½¿ç”¨è¯´æ˜ï¼š
1. ç»§æ‰¿ BaseLawSpider ç±»
2. å®ç° extract_law_links() å’Œ parse_law_page() æ–¹æ³•
3. è¿è¡Œ run() æ–¹æ³•å¼€å§‹çˆ¬å–
"""
import httpx
import json
import hashlib
import time
from pathlib import Path
from typing import List, Dict, Optional, Set
from bs4 import BeautifulSoup
from tenacity import retry, stop_after_attempt, wait_exponential
import re


class BaseLawSpider:
    """æ³•è§„çˆ¬è™«åŸºç±»"""

    def __init__(
        self,
        name: str,
        base_url: str,
        output_dir: str = "output",
        delay: float = 1.0,
    ):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            name: çˆ¬è™«åç§°
            base_url: ç›®æ ‡ç½‘ç«™åŸºç¡€ URL
            output_dir: è¾“å‡ºç›®å½•
            delay: è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
        """
        self.name = name
        self.base_url = base_url
        self.output_dir = Path(output_dir)
        self.delay = delay

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # å·²çˆ¬å– URL é›†åˆï¼ˆå»é‡ï¼‰
        self.seen_urls: Set[str] = self._load_seen_urls()

        # HTTP å®¢æˆ·ç«¯
        self.client = httpx.Client(
            timeout=30.0,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            },
            follow_redirects=True,
        )

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total": 0,
            "success": 0,
            "failed": 0,
            "skipped": 0,
        }

    def _load_seen_urls(self) -> Set[str]:
        """åŠ è½½å·²çˆ¬å–çš„ URL é›†åˆ"""
        seen_file = self.output_dir / f"{self.name}_seen_urls.txt"
        if seen_file.exists():
            with open(seen_file, "r", encoding="utf-8") as f:
                return set(line.strip() for line in f)
        return set()

    def _save_seen_url(self, url: str):
        """ä¿å­˜å·²çˆ¬å–çš„ URL"""
        seen_file = self.output_dir / f"{self.name}_seen_urls.txt"
        with open(seen_file, "a", encoding="utf-8") as f:
            f.write(url + "\n")
        self.seen_urls.add(url)

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=10))
    def _fetch_page(self, url: str) -> str:
        """
        è·å–é¡µé¢ HTMLï¼ˆå¸¦é‡è¯•ï¼‰

        Args:
            url: ç›®æ ‡ URL

        Returns:
            HTML å†…å®¹
        """
        print(f"ğŸ“¥ æ­£åœ¨è·å–: {url}")
        response = self.client.get(url)
        response.raise_for_status()
        time.sleep(self.delay)  # é™é€Ÿ
        return response.text

    def extract_law_links(self, list_page_url: str) -> List[str]:
        """
        ä»åˆ—è¡¨é¡µæå–æ³•è§„é“¾æ¥ï¼ˆéœ€å­ç±»å®ç°ï¼‰

        Args:
            list_page_url: åˆ—è¡¨é¡µ URL

        Returns:
            æ³•è§„è¯¦æƒ…é¡µ URL åˆ—è¡¨
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° extract_law_links() æ–¹æ³•")

    def parse_law_page(self, url: str, html: str) -> Optional[Dict]:
        """
        è§£ææ³•è§„è¯¦æƒ…é¡µï¼ˆéœ€å­ç±»å®ç°ï¼‰

        Args:
            url: æ³•è§„è¯¦æƒ…é¡µ URL
            html: é¡µé¢ HTML

        Returns:
            æ³•è§„æ•°æ®å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹é”®ï¼š
            - law_id: æ³•è§„å”¯ä¸€æ ‡è¯†
            - title: æ³•è§„æ ‡é¢˜
            - category: æ³•è§„åˆ†ç±»
            - level: æ•ˆåŠ›å±‚çº§
            - issue_org: åˆ¶å®šæœºå…³
            - issue_date: å‘å¸ƒæ—¥æœŸ
            - effect_date: ç”Ÿæ•ˆæ—¥æœŸ
            - status: æ•ˆåŠ›çŠ¶æ€
            - summary: æ‘˜è¦
            - tags: æ ‡ç­¾åˆ—è¡¨
            - full_text: æ³•è§„å…¨æ–‡
            - articles: æ¡æ–‡åˆ—è¡¨ï¼ˆæ¯ä¸ªæ¡æ–‡åŒ…å« article_num, article_display, content, chapter, section, keywordsï¼‰
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° parse_law_page() æ–¹æ³•")

    def _clean_text(self, text: str) -> str:
        """
        æ¸…æ´—æ–‡æœ¬
        Args:
            text: åŸå§‹æ–‡æœ¬
        Returns:
            æ¸…æ´—åçš„æ–‡æœ¬
        """
        # 1. æ›¿æ¢æ°´å¹³ç©ºç™½ç¬¦ï¼ˆç©ºæ ¼ã€tabï¼‰ä¸ºå•ä¸ªç©ºæ ¼ï¼Œä½†ä¿ç•™æ¢è¡Œç¬¦
        text = re.sub(r'[ \t\r\f\v]+', ' ', text)
        
        # 2. è§„èŒƒåŒ–æ¢è¡Œç¬¦
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text.strip()

    def _split_articles(self, full_text: str) -> List[Dict]:
        """
        æ‹†åˆ†æ¡æ–‡ï¼ˆé€šç”¨å®ç°ï¼Œå­ç±»å¯è¦†ç›–ï¼‰

        Args:
            full_text: æ³•è§„å…¨æ–‡

        Returns:
            æ¡æ–‡åˆ—è¡¨
        """
        articles = []

        # åŒ¹é…æ¡æ–‡ï¼ˆç¤ºä¾‹ï¼šç¬¬ä¸€æ¡ã€ç¬¬äºŒæ¡ã€ç¬¬123æ¡ï¼‰
        pattern = r'ç¬¬([é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+|[0-9]+)æ¡'
        matches = list(re.finditer(pattern, full_text))

        for i, match in enumerate(matches):
            article_display = match.group(0)
            article_num = self._parse_article_num(match.group(1))

            # æå–æ¡æ–‡å†…å®¹ï¼ˆä»å½“å‰æ¡å·åˆ°ä¸‹ä¸€ä¸ªæ¡å·ä¹‹é—´çš„æ–‡æœ¬ï¼‰
            start = match.end()
            end = matches[i + 1].start() if i + 1 < len(matches) else len(full_text)
            content = self._clean_text(full_text[start:end])

            articles.append({
                "article_num": article_num,
                "article_display": article_display,
                "content": content,
                "keywords": self._extract_keywords(content),
            })

        return articles

    def _parse_article_num(self, num_str: str) -> int:
        """
        è§£ææ¡å·ï¼ˆä¸­æ–‡æ•°å­—è½¬é˜¿æ‹‰ä¼¯æ•°å­—ï¼‰

        Args:
            num_str: æ•°å­—å­—ç¬¦ä¸²ï¼ˆå¯èƒ½æ˜¯ä¸­æ–‡æˆ–é˜¿æ‹‰ä¼¯æ•°å­—ï¼‰

        Returns:
            é˜¿æ‹‰ä¼¯æ•°å­—
        """
        # å¦‚æœå·²ç»æ˜¯é˜¿æ‹‰ä¼¯æ•°å­—ï¼Œç›´æ¥è¿”å›
        if num_str.isdigit():
            return int(num_str)

        # ä¸­æ–‡æ•°å­—è½¬æ¢
        chinese_num_map = {
            'é›¶': 0, 'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4,
            'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9,
            'å': 10, 'ç™¾': 100, 'åƒ': 1000
        }

        # ç‰¹æ®Šå¤„ç†"å"å¼€å¤´çš„æƒ…å†µ
        if num_str.startswith('å'):
            num_str = 'ä¸€' + num_str

        result = 0
        temp = 0
        unit = 1

        for char in reversed(num_str):
            num = chinese_num_map.get(char, 0)

            if num >= 10:
                if num > unit:
                    unit = num
                else:
                    unit *= num

                if temp == 0:
                    temp = unit
            else:
                temp = num * unit

            result += temp
            temp = 0

        return result

    def _extract_keywords(self, content: str, top_n: int = 10) -> List[str]:
        """
        ç®€å•å…³é”®è¯æå–ï¼ˆåŸºäºé•¿åº¦å’Œé¢‘ç‡ï¼‰

        Args:
            content: æ¡æ–‡å†…å®¹
            top_n: è¿”å›å‰ N ä¸ªå…³é”®è¯

        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        # ç®€å•å®ç°ï¼šæå– 2-4 å­—çš„è¯æ±‡
        words = re.findall(r'[\u4e00-\u9fa5]{2,4}', content)

        # ç»Ÿè®¡è¯é¢‘
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1

        # æŒ‰é¢‘ç‡æ’åº
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)

        return [word for word, _ in sorted_words[:top_n]]

    def _generate_law_id(self, title: str, issue_date: str) -> str:
        """
        ç”Ÿæˆæ³•è§„å”¯ä¸€æ ‡è¯†

        Args:
            title: æ³•è§„æ ‡é¢˜
            issue_date: å‘å¸ƒæ—¥æœŸ

        Returns:
            æ³•è§„ ID
        """
        # ä½¿ç”¨æ ‡é¢˜å’Œæ—¥æœŸç”Ÿæˆå”¯ä¸€ ID
        raw = f"{title}_{issue_date}"
        return hashlib.md5(raw.encode()).hexdigest()[:16]

    def _save_to_jsonl(self, law_data: Dict):
        """
        ä¿å­˜æ³•è§„æ•°æ®åˆ° JSONL æ–‡ä»¶

        Args:
            law_data: æ³•è§„æ•°æ®
        """
        # ä¿å­˜æ³•è§„å…ƒä¿¡æ¯
        laws_file = self.output_dir / "laws.jsonl"
        law_info = {k: v for k, v in law_data.items() if k != "articles"}
        with open(laws_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(law_info, ensure_ascii=False) + "\n")

        # ä¿å­˜æ¡æ–‡
        articles_file = self.output_dir / "law_articles.jsonl"
        for article in law_data.get("articles", []):
            article_data = {
                "law_id": law_data["law_id"],
                "article_num": article["article_num"],
                "article_display": article["article_display"],
                "content": article["content"],
                "chapter": article.get("chapter"),
                "section": article.get("section"),
                "keywords": article.get("keywords", []),
            }
            with open(articles_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(article_data, ensure_ascii=False) + "\n")

    def run(self, list_page_urls: List[str]):
        """
        è¿è¡Œçˆ¬è™«

        Args:
            list_page_urls: åˆ—è¡¨é¡µ URL åˆ—è¡¨
        """
        print(f"ğŸš€ å¯åŠ¨çˆ¬è™«: {self.name}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")

        all_law_urls = []

        # æ­¥éª¤1: æå–æ‰€æœ‰æ³•è§„é“¾æ¥
        for list_url in list_page_urls:
            try:
                law_urls = self.extract_law_links(list_url)
                all_law_urls.extend(law_urls)
                print(f"âœ… ä» {list_url} æå–åˆ° {len(law_urls)} ä¸ªæ³•è§„é“¾æ¥")
            except Exception as e:
                print(f"âŒ æå–é“¾æ¥å¤±è´¥ {list_url}: {e}")

        print(f"\nğŸ“Š æ€»å…±å‘ç° {len(all_law_urls)} ä¸ªæ³•è§„é“¾æ¥")

        # æ­¥éª¤2: çˆ¬å–æ¯ä¸ªæ³•è§„
        for i, url in enumerate(all_law_urls, 1):
            self.stats["total"] += 1

            # å»é‡æ£€æŸ¥
            if url in self.seen_urls:
                print(f"â­ï¸  [{i}/{len(all_law_urls)}] å·²çˆ¬å–ï¼Œè·³è¿‡: {url}")
                self.stats["skipped"] += 1
                continue

            try:
                # è·å–é¡µé¢
                html = self._fetch_page(url)

                # è§£ææ•°æ®
                law_data = self.parse_law_page(url, html)

                if law_data:
                    # ä¿å­˜æ•°æ®
                    self._save_to_jsonl(law_data)
                    self._save_seen_url(url)

                    self.stats["success"] += 1
                    print(f"âœ… [{i}/{len(all_law_urls)}] æˆåŠŸ: {law_data['title']}")
                else:
                    self.stats["failed"] += 1
                    print(f"âŒ [{i}/{len(all_law_urls)}] è§£æå¤±è´¥: {url}")

            except Exception as e:
                self.stats["failed"] += 1
                print(f"âŒ [{i}/{len(all_law_urls)}] é”™è¯¯: {url} - {e}")

        # æ‰“å°ç»Ÿè®¡
        print("\n" + "=" * 60)
        print("ğŸ“ˆ çˆ¬å–ç»Ÿè®¡:")
        print(f"  æ€»è®¡: {self.stats['total']}")
        print(f"  æˆåŠŸ: {self.stats['success']}")
        print(f"  å¤±è´¥: {self.stats['failed']}")
        print(f"  è·³è¿‡: {self.stats['skipped']}")
        print("=" * 60)

        self.client.close()
